For all this talk, whenever you give me code in python, comment on it and always do it in English.
As for text, always write in English, unless I tell you otherwise, and then write in well-written Portuguese without Brazilianisms.
As for writing, you write in simple but academic English and follow the following rules:
Tone- Use a direct, personable, and casual tone. Be straightforward and helpful, not overly upbeat or exuberant.
Concise and Clear- Be concise and informative. Keep sentences and paragraphs short.
No AI Buzzwords- Use clear, jargon-free language. Focus on clarity.
Active voice- Write in a way that feels natural and engaging
No over explaining- Just deliver the response, no unnecessary commentary.
Fresh research- If available, pull in recent info from the web.
Criticism Allowed- ChatGPT should push back when needed, not just agree
Additional Guidelines:
Acknowledge and correct any past errors.
Use the metric system for measurements and calculations.
Research answers when necessary before responding.
Please note that I only use better formatting when I'm not improving sentences or writing parts of my report, which you should always write in simple but academic and well-written longhand.
Deep Learning Group Project
Master’s Degree in Data Science 24/25
In this project, you will develop a deep-learning model to classify rare species based on their images. The dataset used in this project consists of images of rare species sourced from the Encyclopedia of Life (EOL) and curated as part of the BioCLIP: A Vision Foundation Model for the Tree of Life [1] study. Each image is associated with metadata, including its kingdom, phylum, and family.
Your objective is to build a model that predicts the family of a given species based on its image. The dataset includes a structured CSV file containing the file path of each image alongside its corresponding labels. As part of the project, you will need to create your own data splits into training, validation, and test sets. The training and validation sets will be used to develop and fine-tune your model, while the test set should be kept separate to evaluate its performance on unseen data.
Like almost every Machine Learning task, there is no “right solution” for this problem. You can create your model however you wish, as long as you do not look at the test set. To develop your model, feel free to apply different pre-processing techniques, test distinct combinations of hyperparameters, and make use of all the concepts covered in the course. The project should be done in Python 3. Naturally, you can (and should) use available libraries, such as Pandas, Keras, etc. Just make sure to credit everything you use in the report appropriately. Also, while the approaches you try are likely to have already been tried by someone else, you should not plagiarise anyone’s code.
Report
In addition to your code, you should deliver a short scientific report (maximum 5 pages + bibliography, if applicable) with the following information:
Group identification of each member’s student ID and name.
A clear description of your best approach (why it is the best, etc..) and the steps you followed for arriving at that model, including (when applicable):
•	The applied pre-processing.
•	The model/approach implemented.
Your experimental setup:
•	The list of Python libraries (for example, "We used Keras to build the model and Keras Tuner to hyperparameter tuning") needed to run your code and any other implementation details that can be relevant to reproduce your experiment.
How you evaluated your approach:
•	A brief description of your intermediate models (what have you tested and why it has not worked compared to your best approach, i.e. "The standard scaler worked better instead of MinMax, etc..").
•	An evaluation of your best approach, which should appropriate metrics. Feel free to include results for other intermediate approaches you tried.
A summary of the results reported:
A brief error analysis: what are the most common errors made by your approach? Feel free to illustrate it with a confusion matrix and/or specific examples. (Maybe images of wrong classifications with the corresponding conclusion why)
Future work:
•	If you had more time, what would you have tried?
The report must be written in English.
Deliverables:
By the 30th of April, you must deliver your source code used and a report that discusses the previous points and where you explain your choices. You must deliver your work through Moodle (a Turnitin assignment will be available soon). The required files must be uploaded in a unique zip/rar file named with the group number (e.g., GROUP_1.rar). For this purpose, we have created an Excel file you must fill in with group information. Each group MUST consist of 5 students. There are no exceptions to this rule.
The grade of the project will be obtained by evaluating the following criteria:
•	Quality of your image/data pre-processing, such as image transformations. (3 points)
•	Ability to design and implement Deep Learning models. (3 points)
•	Ability to utilize the pre-trained models and correctly adapt them to your task. (2 points)
•	Amount and quality of innovative approaches implemented. (2 points)
•	Ability to analyse the results obtained and to use Deep Learning concepts correctly. (1 point)
•	Quality of the report and language adequacy. (5 points)
•	The performance of your model, and the metrics used. (2 points)
•	Clarity and quality of the projects code. (2 points)
•	Comparative evaluations against other groups’ solutions.
Beside the criteria mentioned above, your group will be compared and evaluated against other group’s solutions.
The collaboration between groups and the use of AI tools to generate code is strictly forbidden. Not respecting these rules will result in the immediate reprobation of the curricular unit in both epochs.
The fact that you are working in a group does not imply that all the components of the group will receive the same grade.
[1]: Stevens, S., Wu, J., Thompson, M. J., Campolongo, E. G., Song, C. H., Carlyn, D. E., ... & Su, Y. (2024). Bioclip: A vision foundation model for the tree
of life. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 19412-19424).
TENDO EM CONTA ESTE ENUNCIADO E UTILIZANDO COMO BASE DE RELATÓRIO NO QUE DIZ RESPOSTO, AO ESTILO DE ESCRITA, ESTRUTURA DAS JUSTIFICAÇÕES, E TUDO O QUE NELES CONTÉM, RESPODENDE ÀS MINHAS QUESTÕES SEMPRE TENDO POR BASE O ENUNCIADO E ESTES RELATORIOS DE EXEMPLOS

-------------------

BASEANDO NAS INTRODUÇÕES DOS MEUS DOIS OUTROS RELATÓRIOS, FAZ AGORA A INTRODUÇÃO DO MEU TRABALHO DE DL EM QUE NO 1º PARAGRAFO FAÇAS A INTRODUÇÃO DE DL NA RESOLUÇÃO DO MEU PROBLEMA (VE O ENUNCIADO) - FAZ ALGO CHAMATIVO, MAS TMB N EXAGERES. NO 2º PARAGRAFO RESUMES O MAIS POSSIVEL OS TRABALHOS DA LITERATURE REVIEW E NO 3 PARAGRAGO A ESTURA DO MEU REPORT DO TRABALHO. SÊ O MAIS SIMPLES, MAS ACADEMICO E CONCISO, MAS SEM FALTAR NADA, POSSIVEL!

TEM EM CONSIDERAÇÃO QUE O MEU RELATORIO TERÁ ESTA ESTRUTURA 


1.	INTRODUCTION
2.	DATA EXPLORATION AND IMAGE PREPROCESSING
3.	MODELLING & EVALUATION
4.	INNOVATIVE APPROACH
5.	CONCLUSION


BIBLIOGRAPHICAL REFERENCES
[1]	Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. The MIT Press.
[2]	Stevens, S., Wu, J., Thompson, M. J., Campolongo, E. G., Song, C. H., Carlyn, D. E., ... & Su, Y. (2024). Bioclip: A vision foundation model for the tree of life. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 19412-19424).
[3]	S, N. J., Kamsala Tharun, & Somu Geetha Sravya. (2024). Deep Learning Approaches to Image-Based Species Identification. 2024 International Conference on Integrated Circuits and Communication Systems (ICICACS), 1–7. https://doi.org/10.1109/icicacs60521.2024.10498423
[4]	Bhargavi, I., Pratap, A. R., & Sri, A. S. (2024). An Enhanced EfficientNet-Powered Wildlife Species Classification for Biodiversity Monitoring. 2024 4th International Conference on Intelligent Technologies (CONIT), 1–6. https://doi.org/10.1109/conit61985.2024.10627148
[5]	Mane, V., Pranjali Nikude, Patil, T., & Tambe, P. (2024). Wildlife Classification using Convolutional Neural Networks (CNN). 2024 International Conference on Inventive Computation Technologies (ICICT). https://doi.org/10.1109/icict60155.2024.10544702
[6]	Habib, S., Ahmad, M., Ul Haq, Y., Sana, R., Muneer, A., Waseem, M., … Dev, S. (2024). Advancing Taxonomic Classification Through Deep Learning: A Robust Artificial Intelligence Framework for Species Identification Using Natural Images.  IEEE Access,  12, 146718–146732. doi:10.1109/ACCESS.2024.3450016
[7]	Kimly Y, Malis Lany, Soy Vitou, & Kor, S. (2023). Animal Classification using Convolutional Neural Network. The 2nd Student Conference on Digital Technology 2023. https://www.researchgate.net/publication/376751387_Animal_Classification_using_Convolutional_Neural_Network
[8]	Sharma, S., Sisir Dhakal, & Bhavsar, M. (2024). Transfer Learning for Wildlife Classification: Evaluating YOLOv8 against DenseNet, ResNet, and VGGNet on a Custom Dataset. Journal of Artificial Intelligence and Capsule Networks, 6(4), 415–435. https://doi.org/10.36548/jaicn.2024.4.003
[9]	Supreet Parida, Mishra, A., Sahoo, B. P., Nayak, S., Mishra, N., & Panda, B. S. (2024). Recognizing Wild Animals from Camera Trap Images Using Deep Learning. 2024 International Conference on Intelligent Computing and Emerging Communication Technologies (ICEC), 1–6. https://doi.org/10.1109/icec59683.2024.10837421
[10]	aa Gagandeep M D, Jagath S K, Kartik Tomar, Senthil Kumar R. (2024). R-CNN Based Deep Learning Approach for Counting Animals in the Forest: A Survey. International Journal of Networks and Systems, 13(1), 1–4. https://doi.org/10.30534/ijns/2024/011312024
[11]	Pruthvi Darshan S S, L, J. M., & Sangeetha V. (2024). Multiclass Bird Species Identification using Deep Learning Techniques. 2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT), 1–6. https://doi.org/10.1109/conecct62155.2024.10677184
[12]	Gill, K. S., Gupta, R., Malhotra, S., Swati Devliyal, & G Sunil. (2024, April 5). Classification of Reptiles and Amphibians Using Transfer Learning and Deep Convolutional Neural Networks. 2022 IEEE 7th International Conference for Convergence in Technology (I2CT). https://doi.org/10.1109/i2ct61223.2024.10544030
[13]	P Kanaga Priya, T Vaishnavi, N Selvakumar, G Ramesh Kalyan, & A Reethika. (2023, July 19). An Enhanced Animal Species Classification and Prediction Engine using CNN. 2023 2nd International Conference on Edge Computing and Applications (ICECAA). https://doi.org/10.1109/icecaa58104.2023.10212299
[14]	Sharma, S., Neupane, S., Gautam, B., & Sato, K. (December 2023). Automated Multi-Species Classification Using Wildlife Datasets Based on Deep Learning Algorithms. Materials, Methods & Technologies, 17, 2023. doi:10.62991/MMT1996359772
[15]	Oion, M. S. R., Islam, M., Amir, F., Ali, M. E., Habib, M., Hossain, M. S., & Wadud, M. A. H. (2023). Marine Animal Classification Using Deep Learning and Convolutional Neural Networks (CNN). 2023 26th International Conference on Computer and Information Technology (ICCIT), 1–6. doi:10.1109/ICCIT60459.2023.10441585
[16]	Binta Islam, S., Valles, D., Hibbitts, T. J., Ryberg, W. A., Walkup, D. K., & Forstner, M. R. J. (2023). Animal Species Recognition with Deep Convolutional Neural Networks from Ecological Camera Trap Images. Animals, 13(9), 1526. https://doi.org/10.3390/ani13091526
[17]	Cai, R. (2023). Automating bird species classification: A deep learning approach with CNNs. Journal of Physics: Conference Series, 2664(1), 012007. https://doi.org/10.1088/1742-6596/2664/1/012007
[18]	Priya, P. K., Vinu, M. S., PrasannaBlessy, M., Kirupa, P., Gayathri, R., & Selvakumar, N. (2023). An Eagle-Eye Vision: Advancements in Avian Species Classification. 2023 2nd International Conference on Automation, Computing and Renewable Systems (ICACRS), 758–764. doi:10.1109/ICACRS58579.2023.10404897
[19]	Larson, J. (2021). Assessing Convolutional Neural Network Animal Classification Models for Practical Applications in Wildlife Conservation [MSc Thesis]. https://doi.org/10.31979/etd.ysr5-th9v
[20]	Sanghvi, K., Aralkar, A., Sanghvi, S., & Saha, I. (May 2020). Fauna Image Classification using Convolutional Neural Network. 13, 8–16.
[21]	Rajasekaran, T., Kaliappan, V., Surendran, R., Sellamuthu, K., & Palanisamy, J. (October 2019). Recognition Of Animal Species On Camera Trap Images Using Machine Learning And Deep Learning Models. International Journal of Scientific & Technology Research, 8.
[22]	Albuquerque, C. (2019). Convolutional neural networks for cell detection and counting : a case study of human cell quantification in zebrafish xenografts using deep learning object detection techniques [MSc Thesis]. https://run.unl.pt/handle/10362/62425
[23]	



 
 
APPENDIX A. LITERATURE REVIEW
Table A1 – Literature Review
Paper Title	Abstract Summary	Methodology	Main Findings	Image Preprocessing Techniques Used	Algorithms Used	Reference
Deep Learning Approaches To Image-Based Species Identification	Highlights deep learning and transfer learning for automating image-based species identification, aiding biodiversity conservation.	- Used TensorFlow, Keras, and OpenCV; 
- Trained various CNN models; 
- Employed data augmentation; 
- Used optimization techniques (EarlyStopping and learning rate reduction strategies);
- Compared transfer learning models;
- Fine-tuned Xception.	Transfer learning with Xception was most effective, showing potential for classifying rare species.	Data augmentation, Image processing with OpenCV	ResNet50V2 
ResNet152V2
InceptionV3 
Xception and 
DenseNet121	[3]

An Enhanced EfficientNet-Powered Wildlife Species Classification for Biodiversity Monitoring	Focuses on an enhanced EfficientNet model for efficient and accurate wildlife species classification, supporting biodiversity monitoring.	- Used EfficientNet architecture; 
- Employed transfer learning with pre-trained models; 
- Utilized advanced preprocessing methods (Lanczos interpolation); 
Leveraged data augmentation and attention mechanisms.	The model demonstrated efficiency and accuracy, particularly useful for biodiversity monitoring.	Feature Normalization
Transfer Learning
Data Augmentation
Attention Mechanisms	EfficientNet	[4]

Wildlife Classification using Convolutional Neural Networks (CNN)	Develops a CNN-based model for accurate identification and classification of diverse wildlife species, contributing to wildlife conservation efforts.	-Developed a CNN model; 
- Used high-resolution wildlife images; 
- Optimized hyperparameters; 
- Utilized data augmentation; 
- Fine-tuned the model; 
- Evaluated accuracy and validation loss on a separate test dataset.	Achieved precise classification, supporting wildlife conservation applications.	Data augmentation	CNNs	[5]

Advancing Taxonomic Classification Through Deep Learning: A Robust Artificial Intelligence Framework for Species Identification Using Natural Images	A deep learning framework using ResNet-50 CNN achieves high accuracy in classifying species, including rare animals, from natural images.	-Utilized ResNet-50; 
- Pre-processed and augmented the dataset; 
- Modified ResNet-50 for 4 million trainable parameters;
Compared with models like GoogleNet, VGG, SegNet, and DeepLab v3+.	ResNet-50 modified showed superior computational efficiency and accuracy compared to existing models.	Grayscale Conversion 
Data Augmentation (Rotation)	ResNet-50 CNN
Modified ResNet-50	[6]

Animal Classification Using Cnn Architectures	Explores the use of CNN architectures for classifying wild animal species, contributing to conservation efforts.	- Used CNNs like LeNet-5, AlexNet, VGG16, ResNet50, Inception V3, and InceptionResnetV2; 
- Employed a dataset of 90 animal images from Kaggle; 
- Implemented data augmentation; 
- Used dropout to prevent overfitting.	Compared performance of different CNNs, with InceptionResnetV2 performing best.	Data augmentation	LeNet-5, 
AlexNet, 
VGG16 
ResNet50 
Inception V3 
InceptionResnetV2 
Data Augmentation
Dropout	[7]

Transfer Learning for Wildlife Classification: Evaluating YOLOv8 against DenseNet, ResNet, and VGGNet on a Custom Dataset	Evaluates the performance of deep learning models, including YOLOv8, for classifying rare wildlife species using a custom dataset.	- Image Preprocessing: resizing to 400x400 pixels, normalization, 80/20 train-validation split; 
- Data Augmentation: rotation, flipping, translation; 
- Transfer Learning: fine-tuned pre-trained models by freezing initial layers and adding custom fully connected layers; 
- Training: used weighted Adam optimizer and categorical cross-entropy loss function for 100 epochs with a batch size of 32.	Compared YOLOv8, DenseNet, ResNet, and VGGNet, with YOLOv8 showing the best performance.	Resizing, normalization, data augmentation	YOLOv8
DenseNet
ResNet
VGGNet	[8]

Recognizing Wild Animals from Camera Trap Images Using Deep Learning	Demonstrates the effectiveness of deep learning, specifically VGG16, for detecting and classifying animals in wildlife conservation.	- Used VGG16 model; 
- Leveraged transfer learning with a pre-trained VGG16 model on ImageNet; 
- Loaded and preprocessed images to match VGG16 input size; 
- Fed preprocessed images into the model for prediction; 
- Decoded outputs for classification results, specifically for buffalo detection.	The VGG16 model was effective for detection and classification in conservation efforts.	Resizing, normalization	VGG16
Transfer Learning	[9]

R-CNN Based Deep Learning Approach for Counting Animals in the Forest: A Survey	Presents a deep learning approach using R-CNN for classifying animals in forest camera trap images, relevant to rare species identification.	- Captured high-resolution images of forest environments; 
- Divided images into region proposals using a selective search algorithm; 
- Processed region proposals through a CNN to extract high-level features; 
- Fed features into SVMs for species classification; 
- Applied bounding box regression to refine localization.	The R-CNN approach was effective for classification in camera trap images.	Not specified	R-CNN
CNN
SVM	[10]

Multiclass Bird Species Identification using Deep Learning Techniques	Deep learning models like EfficientNet-B0 can accurately identify bird species for ecosystem conservation.	- Employed four deep learning models (ResNet50, MobileNetV3, EfficientNet-B0, Wide-ResNet50V2) to identify 525 bird species;
- Utilized transfer learning and fine-tuning to adapt pre-trained models;
- Evaluated performance using precision, recall, and F1-score metrics.	EfficientNet-B0 performed best among the models for bird species identification, but all models demonstrated high overall performance, with accuracy above 97%	Resizing images to 224x224 pixels
Normalization	ResNet50
MobileNetV3
EfficientNet-B0
Wide-ResNet50V2	[11]

						
Classification of Reptiles and Amphibians Using Transfer Learning and Deep Convolutional Neural Networks	A deep learning model using transfer learning and data augmentation classifies reptiles and amphibians with high accuracy, demonstrating potential for AI in biodiversity conservation.	- Used deep CNNs and Transfer Learning for automatic classification; 
- Optimized a pre-trained MobileNetV2 model on a large dataset of reptile and amphibian images;
- Explored image augmentation techniques to improve model performance.	The MobileNetV2 model achieved high accuracy (82%) in classifying reptiles and amphibians;
Data augmentation improved the model's ability to generalize across diverse conditions.	contrast optimization, noise reduction, and image sizing (Image augmentation).	MobileNetV2	[12]

An Enhanced Animal Species Classification and Prediction Engine using CNN	A CNN-based approach achieves high accuracy for automated animal species classification, enabling more efficient wildlife conservation efforts.	- Used CNNs for animal species classification, leveraging transfer learning by fine-tuning pre-trained CNN models;
- Employed a prediction engine and web application for user interaction.	The CNN approach achieved a high accuracy of 98% for species classification.
Developed a web application for image input and species prediction.	Image Resizing (to a uniform size, unspecified dimensions)
Normalization (zero mean and unit variance)
Image Augmentation (rotation, flipping, scaling)	CNNs, transfer learning	[13]

Automated Multi-Species Classification Using Wildlife Datasets Based on Deep Learning Algorithms	A deep learning model for automated multi-species classification of wildlife images achieves high accuracy and could be valuable for conservation applications.	- Used two CNN models, EfficientNetB0 and VGG16, for multi-species wildlife classification; 
- Developed a deep learning model to classify 37 distinct wildlife species; 
- Trained the model on a dataset of 185,111 images and tested it on 3,131 images; 
- Achieved over 80% accuracy and over 90% top-5 accuracy.	Achieved over 80% accuracy and 90% top-5 accuracy in multi-species classification.
EfficientNetB0 model outperformed the VGG16 model overall	Data cleaning to eliminate bias and irrelevant information
Normalization (between 0 and 1)
Data augmentation	EfficientNetB0, VGG16	[14]

Marine Animal Classification Using Deep Learning and Convolutional Neural Networks (CNN)	Demonstrates the effectiveness of CNNs for classifying marine animal species.	- Data Collection: gathered various photos of marine animals; 
- Data Preprocessing: compressed and flattened the pixel values of the collected photos; 
- CNN Model Architecture: convolutional, pooling, and fully connected layers; 
- Model Training: used the dataset to train the CNN model and tuned the hyperparameters for optimal performance; 
- Model Deployment and Evaluation: deployed the trained model for performance evaluation and real-time forecasting.	The CNN model was effective for classifying marine animals.	Compressing and flattening pixel values	CNNs (Pooling layers, fully connected layers), EfficientNet B0/B3/B5, Grad-CAM (Gradient-weighted Class Activation Mapping)	[15]

Animal Species Recognition with Deep Convolutional Neural Networks from Ecological Camera Trap Images	Deep learning models can classify rare animal species like snakes, lizards, and toads from camera trap images.	- Balanced the imbalanced dataset; 
- Investigated various image preprocessing techniques; 
- Applied data augmentation; 
- Trained and tested ML models to classify three animal groups (snakes, lizards, and toads) from camera trap images; 
- Experimented with two pre-trained models (VGG16 and ResNet50) and a self-trained CNN with varying layers and augmentation parameters.	The custom CNN with specific parameters performed best.	Image preprocessing, data augmentation	VGG16, ResNet50, custom CNN (CNN-1)	[16]

Automating bird species classification: A deep learning approach with CNNs	A CNN model using transfer learning and data augmentation achieved high accuracy in classifying 525 bird species.	- Dataset of 84,635 training images, 2,625 test images, and 2,625 validation images for 525 bird species from Kaggle; 
- Divided dataset into training and validation subsets;
- Applied data augmentation; 
- Used callbacks like Model Checkpoint, EarlyStopping, Reduce LROnPlateau; 
- Constructed model by adding layers to pre-trained EfficientNetB0; 
- Trained for 150 epochs using Adam optimizer and categorical cross-entropy loss, with accuracy as the metric.	Achieved high accuracy in classifying 525 bird species.	Data augmentation	EfficientNetB0, transfer learning	[17]

An Eagle-Eye Vision: Advancements in Avian Species Classification	CNNs are effective for automated bird species classification, with potential applications in avian conservation.	- Image Preprocessing, including scaling, normalization, and data augmentation; 
- Feature Extraction using CNN architecture; 
- Transfer learning to fine-tune a pre-trained CNN model for bird species classification.	The CNN approach was effective for automated bird species classification.	Scaling, normalization, augmentation	CNNs, transfer learning	[18]

Assessing Convolutional Neural Network Animal Classification Models for Practical Applications in Wildlife Conservation	Assesses the performance of CNN models for identifying animal species in camera-trap images for wildlife conservation applications.	 -Used the Wellington Camera Traps dataset to simulate a wildlife conservation project; 
- Developed and analyzed 10 different CNN models; 
- Tested the CNN models on 7 different datasets, simulating 13 possible project scenarios; 
- Evaluated model performance using standard metrics (top-1 and top-5 accuracy) and two novel metrics (false alarm rate and missed invasive rate) to reflect wildlife conservation goals.	Different CNN models were evaluated, with performance reflecting conservation goals.	Not specified	CNNs	[19]

Fauna Image Classification using Convolutional Neural Network	A CNN model developed for classifying fauna images with high accuracy, aiding in wildlife conservation research.	- Used a CNN for image classification of different animal species; 
- Trained and developed the CNN model to achieve high classification accuracy (91.84% training accuracy, 99.77% test accuracy); 
- Classified images of animals captured in dense forest environments.	The model achieved high accuracy in classifying fauna images.	Not specified	CNNs (VGG16)	[20]

Recognition Of Animal Species on Camera Trap Images Using Machine Learning and Deep Learning Models	Deep learning models like InceptionV3 outperform machine learning algorithms in classifying animal species from camera trap images.	
- Compared the performance of several ML algorithms (SVM, Random Forest) and Deep Learning models (AlexNet, Inception V3); 
- Used the KTH dataset with 19 different animal categories, selected 12 for evaluation; 
- Found deep learning models outperformed machine learning algorithms in terms of classification accuracy.

	Deep learning models (AlexNet, Inception V3) outperformed machine learning (SVM, Random Forest).	Not specified	SVM, Random Forest, AlexNet, Inception V3	[21]

Convolutional neural networks for cell detection and counting: a case study of human cell quantification in zebrafish xenografts using deep learning object detection techniques	The proposed solution is a fine-tuned architecture based on the Faster R-CNN object detection algorithm using an Inception ResNetV2 feature extractor. This automated approach is applied to a specific, innovative research area at Fundação Champalimaud: quantifying cells in zebrafish xenografts used for studying cancer, metastasis, and drug discovery. The goal is to provide researchers with an accurate automatic tool for cell counting, contributing a practical application of object detection techniques to overcome common challenges like cell overlap, high cell density, and heterogeneity in medical imaging analysis.	- Preparing a labelled dataset of zebrafish cell images;
- Selecting a powerful object detection model (Faster R-CNN with Inception ResNetV2);
- Leveraging transfer learning from the COCO dataset;
- Implementing and training the model using the TensorFlow Object Detection API;
- Evaluating its cell counting performance using standard detection metrics like mAP.	The proposed Faster R-CNN model accurately detected and counted cells in challenging zebrafish xenograft images, achieving 89.5% mAP@0.50.	Data Augmentation (HorizontalFlip, VerticalFlip, PixelValueScale, ImageScale, RGBtoGray, Adjustbrightness, Adjustcontrast, Adjusthue, Adjustsaturation, Distortcolor, Jitterboxes, Cropimage, Padimage, Croppadimage, Croptoaspectratio, Blackpatches, Rotation90)
Additionally, combinations were tested, such as applying only the top four performing augmentations (HorizontalFlip, Adjustbrightness, Jitterboxes, Croptoaspectratio), applying all listed techniques except Adjusthue, Padimage, and Croppadimage, and finally, applying all listed individual augmentations together.	Faster R-CNN, Inception ResNet V2, Transfer Learning	[22]



-------------

TENDO POR BASE TODO ESTE CÓDIGO:

============================================ utilities.py

# ==================================================================
# Functions DL Project 
#
# This file contains utility auxiliary functions for the DL project.
#
# Group: 37
# Members:
#   - André Silvestre, 20240502
#   - Diogo Duarte, 20240525
#   - Filipa Pereira, 20240509
#   - Maria Cruz, 20230760
#   - Umeima Adam Mahomed, 20240543
# ==================================================================

# System Libraries  
import os
import random
from pathlib import Path
import importlib
import utilities
import datetime
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
from typing import Union, List
    
# Data Manipulation & Visualization Libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from matplotlib.image import imread
from PIL import Image

# Setting seaborn style
sns.set_theme(style="white")

# Machine Learning Libraries
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# TensorFlow Libraries
import tensorflow as tf
from tensorflow.keras.preprocessing import image as keras_image             # Use alias to avoid conflict
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image_dataset_from_directory

# ------------------------------------------------------------------------
# Function to reload utilities.py in Jupyter Notebook
def reload_utilities():
    """
    Reload the utilities module in Jupyter Notebook.
    
    This function is useful for reloading the utilities module after making changes to it.
    """

    importlib.reload(utilities)
    print(f"Utilities module reloaded successfully ({datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S')})")
    
# ------------------------------------------------------------------------
# Function to load images from a directory
def load_images_from_directory(# Directory paths 
                               train_dir: str,
                               val_dir: str,
                               test_dir: str,
                               
                               # Optional parameters for image loading
                               labels = 'inferred',
                               label_mode = 'categorical',
                               class_names = None,
                               color_mode = 'rgb',
                               batch_size: int = 32,
                               image_size: tuple = (256, 256),
                               seed: int = 2025,
                               interpolation: str = 'bilinear',
                               crop_to_aspect_ratio: bool = False,
                               pad_to_aspect_ratio: bool = False) -> tf.data.Dataset:
    """
    Load images from a specified directory using TensorFlow's image_dataset_from_directory.
    This function creates a data generator for loading images, which can be used for training, validation, or testing.
    The images are resized to the specified size and can be loaded in either RGB or grayscale color mode.
    The function also includes data augmentation options, such as rotation, zoom, and horizontal flipping.

    Args:
        train_dir (str): Path to the training directory.
        val_dir (str): Path to the validation directory.
        test_dir (str): Path to the test directory.
        
        # Optional parameters for image loading 
        # Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory
        labels (str): Type of labels to generate. Default is 'inferred'.
        label_mode (str): Type of labels to generate. Default is 'categorical'.
        class_names (list): List of class names. Default is None.
        color_mode (str): Color mode to read images. Default is 'rgb'.
        batch_size (int): Size of the batches of data. Default is 32.
        image_size (tuple): Size of the images to read (height, width). Default is (256, 256) == default size for tf.keras.preprocessing.image_dataset_from_directory
        seed (int): Random seed for shuffling and transformations. Default is 2025.
        interpolation (str): Interpolation method to resample the image. Default is 'bilinear'.
        crop_to_aspect_ratio (bool): Whether to crop the image to the aspect ratio. Default is False.
        pad_to_aspect_ratio (bool): Whether to pad the image to the aspect ratio. Default is False.
        
    Returns:
        train_datagen (tf.data.Dataset): Data generator for training data.
        val_datagen (tf.data.Dataset): Data generator for validation data.
        test_datagen (tf.data.Dataset): Data generator for test data.
    Raises:
        FileNotFoundError: If the specified directories do not exist.
        
    ----
    Assumptions:
        - The directory structure is assumed to be organized like this:
        data/RareSpecies_Split/
                ├── train
                │   ├── class1
                |   |     ├── image1.1.jpg        
                |   |     ├── image1.2.jpg        
                |   |     └── image1.3.jpg        
                │   ├── class2
                |   |     ├── image2.1.jpg
                ├── val
                ├── test
    """
    # Check if the directories exist
    if not os.path.exists(train_dir):
        raise FileNotFoundError(f"Training directory '{train_dir}' does not exist.")
    if not os.path.exists(val_dir):
        raise FileNotFoundError(f"Validation directory '{val_dir}' does not exist.")
    if not os.path.exists(test_dir):
        raise FileNotFoundError(f"Test directory '{test_dir}' does not exist.")
    
    # Data generators with built-in rescaling (no augmentation yet)
    # Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory
    #         https://stackoverflow.com/questions/59228816/what-do-the-tensorflow-datasets-functions-cache-and-prefetch-do

    # Training data generator
    train_datagen = image_dataset_from_directory(
        train_dir,                                      # Path to the directory
        labels=labels,                                  # Type of labels to generate (inferred = from the directory structure)
        label_mode=label_mode,                          # Type of labels to generate (categorical = 'float32' tensor of shape (batch_size, num_classes), representing a one-hot encoding of the class index.)
        class_names=class_names,                        # List of class names (if None, the class names are inferred from the directory structure)
        color_mode=color_mode,                          # Color mode to read images
        batch_size=batch_size,                          # Size of the batches of data
        image_size=image_size,                          # Size of the images to read (256x256)
        shuffle=True,                                   # Whether to shuffle the data - True for training data 
        seed=seed,                                      # Random seed for shuffling and transformations
        interpolation=interpolation,                    # Interpolation method to resample the image
        crop_to_aspect_ratio=crop_to_aspect_ratio,      # Whether to crop the image to the aspect ratio
        pad_to_aspect_ratio=pad_to_aspect_ratio         # Whether to pad the image to the aspect ratio
    )

    # Validation data generator
    val_datagen = image_dataset_from_directory(val_dir, labels=labels, label_mode=label_mode, image_size=image_size, batch_size=batch_size,
                                               class_names=class_names, color_mode=color_mode, interpolation=interpolation, 
                                               seed=seed, shuffle=False,   # No shuffling for validation data
                                               crop_to_aspect_ratio=crop_to_aspect_ratio, pad_to_aspect_ratio=pad_to_aspect_ratio)

    # Test data generator
    test_datagen = image_dataset_from_directory(test_dir, labels=labels, label_mode=label_mode, image_size=image_size, batch_size=batch_size,
                                                class_names=class_names, color_mode=color_mode, interpolation=interpolation,
                                                seed=seed, shuffle=False,  # No shuffling for test data
                                                crop_to_aspect_ratio=crop_to_aspect_ratio, pad_to_aspect_ratio=pad_to_aspect_ratio)
    
    # # --- Optimize Data Pipelines with Prefetch ---
    # # Apply prefetch to all datasets for performance optimization
    # # Source: https://www.tensorflow.org/guide/data_performance#prefetching
    # train_datagen = train_datagen.prefetch(buffer_size=tf.data.AUTOTUNE)
    # val_datagen = val_datagen.prefetch(buffer_size=tf.data.AUTOTUNE)
    # test_datagen = test_datagen.prefetch(buffer_size=tf.data.AUTOTUNE)
    
    # Return the data generators
    return train_datagen, val_datagen, test_datagen

# ------------------------------------------------------------------------
# Function to show an image from a given path
def show_image(image_path: str) -> None:
    """
    Displays an image using Matplotlib.
    
    Args:
        image_path (str): Path to the image file.
        
    Returns:
        None. Displays the image.
    """
    image = Image.open(image_path)      # Open the image
    plt.figure(figsize=(10, 10))        # Set the figure size   
    plt.imshow(image)                   # Display the image
    plt.axis('off')                     # Remove axis for cleaner visualization
    plt.show()                          # Show the image
    


# ------------------------------------------------------------------------
# Auxiliary Function to display multiple dataframes side by side
# Source: https://python.plainenglish.io/displaying-multiple-dataframes-side-by-side-in-jupyter-lab-notebook-9a4649a4940
from IPython.display import display_html
from itertools import chain,cycle
def display_side_by_side(*args, super_title: str, titles=cycle([''])):
    """
    :param args: Variable number of DataFrame objects to be displayed side by side.
    :param super_title: The main title to be displayed at the top of the combined view.
    :param titles: An iterable containing titles for each DataFrame to be displayed. Defaults to an infinite cycle of empty strings.
    
    :return: None. The function generates and displays HTML content side by side for given DataFrames.
    """
    html_str = ''
    html_str += f'<h1 style="text-align: left; margin-bottom: -15px;">{super_title}</h1><br>'
    html_str += '<div style="display: flex;">'
    for df, title in zip(args, chain(titles, cycle(['</br>']))):
        html_str += f'<div style="margin-right: 20px;"><h3 style="text-align: center;color:#555555;">{title}</h3>'
        html_str += df.to_html().replace('table', 'table style="display:inline; margin-right: 20px;"')
        html_str += '</div>'
    html_str += '</div>'
    display_html(html_str, raw=True)

# ------------------------------------------------------------------------
# Function to create a DataFrame for model evaluation metrics
def create_evaluation_dataframe(model_name: str, variation: str, 
                                train_metrics: dict, val_metrics: dict, test_metrics: dict,
                                train_time: Union[float, str] = None, round_decimals: int = 4, csv_save_path: str = None) -> pd.DataFrame:
    """
    Create a MultiIndex DataFrame for model evaluation metrics with row and column levels.

    Args:
        model_name(str) : Name of the model (e.g., "Baseline Model")
        variation(str) : Variation or experiment label (e.g., "W/ SMOTE", "Tuned").
        train_metrics(dict) : Dictionary with keys: 'accuracy', 'precision', 'recall', 'f1_score', 'auc'.
        val_metrics(dict) : Same structure as train_metrics, for the validation set.
        test_metrics(dict) : Same structure as train_metrics, for the test set.
        train_time(Union[float, str]) : Time of execution in seconds. If None, it will be '' in the DataFrame.
        round_decimals(int, optional) : Number of decimal places to round the metrics (default is 4).
        csv_save_path(str, optional) : Path to save the DataFrame as a CSV file (default is None, no saving).

    Returns:
        df(pd.DataFrame) : A styled MultiIndex DataFrame with metrics across Train/Validation/Test sets.
    """
    
    # Define column MultiIndex: (Set, Metric)
    column_index = pd.MultiIndex.from_product(
        [["Train", "Validation", "Test"],
         ["Accuracy", "Precision", "Recall", "F1 Score", "AUROC"]],
        names=["Set", "Metric"]
    )
    
    # Collect metrics in the correct order
    metrics_data = [
        train_metrics['accuracy'], train_metrics['precision'], train_metrics['recall'], train_metrics['f1_score'], train_metrics['auc'],
        val_metrics['accuracy'], val_metrics['precision'], val_metrics['recall'], val_metrics['f1_score'], val_metrics['auc'],
        test_metrics['accuracy'], test_metrics['precision'], test_metrics['recall'], test_metrics['f1_score'], test_metrics['auc'],
    ]
    
    # Round values and create the DataFrame
    data = [np.round(metrics_data, round_decimals)]
    df = pd.DataFrame(data, columns=column_index)
    
    # Add time of execution column
    if train_time is None:
        # If train_time is None, set it to an empty string
        train_time = ''
    else:
        # Round time to 2 decimal places
        train_time = round(train_time, 2)
    # Insert time of execution at the beginning of the DataFrame
    df.insert(0, ("", "Time of Execution"), train_time)
    
    # Set row MultiIndex: (Model, Variation)
    df.index = pd.MultiIndex.from_tuples([(model_name, variation)], names=["Model", "Variation"])
    
    # Clean column index names for better readability
    df.columns.names = ["", ""]
    
    # Save the DataFrame to a CSV file
    if csv_save_path:
        df.to_csv(csv_save_path, index=True)
    
    return df



# ------------------------------------------------------------------------
# Function to plot metrics
def plot_metrics(history: Union[tf.keras.callbacks.History, pd.DataFrame], file_path=None, model_name=None):
    """Plots training and validation metrics.
    Args:
        history (Union[History, pd.DataFrame]): Keras History object or DataFrame containing training metrics.
        file_path (str): Path to save the plot. If None, the plot is displayed only.
        model_name (str): Name of the model for the title.
    Returns:
        Show the plot.
    """
    # Create a figure with 5 subplots (1 row, 5 columns)
    fig, ax = plt.subplots(1, 5, figsize=(20, 4))
    
    # Plot each metric (loss, accuracy, precision, recall, f1_score)
    metrics = [('loss', 'Loss'), ('accuracy', 'Accuracy'), ('precision', 'Precision'),  ('recall', 'Recall'), ('f1_score', 'F1 Score')]
    
    # Iterate through the metrics and plot them
    for i, (metric, title) in enumerate(metrics):
        if type(history) == tf.keras.callbacks.History:
            ax[i].plot(history.history[metric], label='Train', color='#22c1c3')
            ax[i].plot(history.history[f'val_{metric}'], label='Val', color='#090979')
        else:
            # Assuming history is a DataFrame
            ax[i].plot(history[metric], label='Train', color='#22c1c3')
            ax[i].plot(history[f'val_{metric}'], label='Val', color='#090979')
        ax[i].set_title(title, fontsize=14, fontweight='bold')
        ax[i].set_xlabel('Epoch', fontsize=10, fontweight='bold')
        ax[i].set_ylabel(title, fontsize=10, fontweight='bold')
        ax[i].legend()
        ax[i].set_yticklabels([f'{int(t*100)}%' if metric != 'loss' else round(t,2) for t in ax[i].get_yticks()])
        
        # Remove the top and right spines
        sns.despine(top=True, right=True)
        
    # Adjust layout
    plt.tight_layout()
    
    # Save the figure if a file path is provided
    if file_path:
        fig.savefig(file_path, dpi=300, bbox_inches='tight')
        
    # Add a title if model_name is provided
    if model_name:
        plt.suptitle(f'Training and Validation Metrics\n {model_name}', fontsize=16, fontweight='bold')
        
    # Plot the metrics
    plt.show()
    
    
# ------------------------------------------------------------------------
# Function to plot confusion matrix
def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, title: str = None, 
                          cmap: plt.cm = plt.cm.Blues, file_path: str = None):
    """
    This function plots a confusion matrix using Matplotlib and Seaborn.
    
    Args:
        y_true (array-like): True labels.
        y_pred (array-like): Predicted labels.
        title (str): Title for the plot. Default is None.
        cmap: Colormap to use. Default is plt.cm.Blues.
        file_path (str): Path to save the plot. If None, the plot is displayed only.

    Returns:
        None: Displays the confusion matrix plot.
    """
    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # Transform the confusion matrix into a DataFrame
    cm_df = pd.DataFrame(cm)
    
    # Plot the confusion matrix using Seaborn heatmap
    fig = plt.figure(figsize=(30, 30))
    
    # Show only the lower triangle of the matrix and values more than 0
    mask_annot = cm_df.values > 0
    annot = np.where(mask_annot, cm_df.values, np.full(cm_df.shape,""))

    # Create a heatmap
    sns.heatmap(cm_df, annot=annot, fmt='s', cmap=cmap, cbar=False, annot_kws={"size": 6})   
    
    # Set plot title and labels
    plt.title(title or 'Confusion Matrix', fontsize=16, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=14, fontweight='bold')
    
    # Set tick labels
    plt.xticks(ticks=np.arange(len(np.unique(y_true))), labels=np.unique(y_true), rotation=90, fontsize=8)
    plt.yticks(ticks=np.arange(len(np.unique(y_true))), labels=np.unique(y_true), rotation=0, fontsize=8)
    plt.gca().set_aspect('equal')  # Set aspect ratio to be equal
    
    if file_path:
        # Save the figure if a file path is provided
        fig.savefig(file_path, dpi=300, bbox_inches='tight')
        print(f"Confusion matrix saved to {file_path}")

    # Show the plot
    plt.show()

# ------------------------------------------------------------------------
# Function to plot 5 right predictions and 5 wrong predictions 
# (image pred, sample image of the true class, predicted class)
def plot_predictions(model: tf.keras.Model, test_data: tf.data.Dataset, class_names: list, 
                     train_dir: str, num_images: int = 5, file_path: str = None):
    """
    Plot 5 right predictions and 5 wrong predictions from the test data.
    
    Args:
        model (tf.keras.Model): Trained model for making predictions.
        test_data (tf.data.Dataset): Test dataset containing images and labels.
        class_names (list): List of class names corresponding to the labels.
        train_dir (str): Directory where training images are stored.
        num_images (int): Number of images to plot. Default is 5.
        file_path (str): Path to save the plot. If None, the plot is displayed only.
        
    Returns:
        None: Displays or saves the plot.
    """
    # --- 1. Data Collection and Prediction ---
    images_list = []
    labels_list = []

    # Iterate through the dataset to collect all samples
    for images, labels in test_data:
        images_list.append(images.numpy()) # Convert tensor to numpy array
        labels_list.append(labels.numpy())  

    # Check if data was loaded
    if not images_list or not labels_list:
        print("Error: No data collected from test_data.")
        return

    # Concatenate numpy arrays
    try:
        images_all = np.concatenate(images_list, axis=0)
        labels_all = np.concatenate(labels_list, axis=0)
    except ValueError as e:
        print(f"Error concatenating data: {e}")
        print("Please ensure all batches have compatible shapes.")
        return
    
    # Get true class indices (assuming one-hot encoding)
    if labels_all.ndim > 1 and labels_all.shape[1] > 1:
        y_true_indices = np.argmax(labels_all, axis=1)

    # Handle integer labels directly
    elif labels_all.ndim == 1:
         y_true_indices = labels_all
    else:
        print("Error: Unexpected label format.")
        return

    # Predict on the dataset
    y_pred_probs = model.predict(test_data, verbose=1)
    if y_pred_probs.shape[0] != y_true_indices.shape[0]:
         print(f"Warning: Number of predictions ({y_pred_probs.shape[0]}) does not match number of labels ({y_true_indices.shape[0]}). Check dataset integrity.")
         # Fallback to predicting on the gathered numpy array if counts mismatch
         if images_all.shape[0] == y_true_indices.shape[0]:
              print("Retrying prediction on gathered numpy array...")
              y_pred_probs = model.predict(images_all, batch_size=test_data.batch_size, verbose=1) # Use original batch size
         else:
              print("Error: Cannot align predictions with labels.")
              return

    y_pred_indices = np.argmax(y_pred_probs, axis=1)

    # Ensure number of predictions matches labels
    if y_pred_indices.shape[0] != y_true_indices.shape[0]:
         print("Error: Mismatch between number of predictions and true labels after prediction.")
         return

    # --- 2. Identify Correct/Incorrect Predictions ---
    correct_indices = np.where(y_pred_indices == y_true_indices)[0]
    incorrect_indices = np.where(y_pred_indices != y_true_indices)[0]

    print(f"Found \033[1m{len(correct_indices)} correct\033[0m and \033[1m{len(incorrect_indices)} incorrect\033[0m predictions.\n")

    # --- Print Top 5 Most/Least Accurate Classes ---
    # Count correct predictions per class
    correct_per_class = np.zeros(len(class_names), dtype=int)
    total_per_class = np.zeros(len(class_names), dtype=int)
    for true_idx, pred_idx in zip(y_true_indices, y_pred_indices):
        total_per_class[true_idx] += 1
        if true_idx == pred_idx:
            correct_per_class[true_idx] += 1
    # Avoid division by zero
    # Source: https://numpy.org/doc/stable/reference/generated/numpy.errstate.html
    with np.errstate(divide='ignore', invalid='ignore'):
        acc_per_class = np.where(total_per_class > 0, correct_per_class / total_per_class, 0)
    # Get top 5 most and least accurate classes
    top5_idx = np.argsort(-acc_per_class)[:5]
    bottom5_idx = np.argsort(acc_per_class)[:5]
    print("\033[1mTop 5 classes with most correct predictions:\033[0m")
    for idx in top5_idx:
        print(f"  \033[1m{class_names[idx]}:\033[0m {correct_per_class[idx]}/{total_per_class[idx]} ({acc_per_class[idx]*100:.1f}%)")
    print("\033[1mTop 5 classes with least correct predictions:\033[0m")
    for idx in bottom5_idx:
        print(f"  \033[1m{class_names[idx]}:\033[0m {correct_per_class[idx]}/{total_per_class[idx]} ({acc_per_class[idx]*100:.1f}%)")

    # Check if enough samples are available
    if len(correct_indices) < num_images or len(incorrect_indices) < num_images:
        print(f"Warning: Not enough correct ({len(correct_indices)}) or incorrect ({len(incorrect_indices)}) examples to sample {num_images}. Adjusting num_images.")
        num_images = min(len(correct_indices), len(incorrect_indices), num_images)
        if num_images == 0:
            print("Error: No examples to plot.")
            return

    # Randomly sample indices
    correct_sample_indices = np.random.choice(correct_indices, num_images, replace=False)
    incorrect_sample_indices = np.random.choice(incorrect_indices, num_images, replace=False)

    # --- 3. Plotting ---
    # Create figure: 4 rows (Correct, Incorrect, True Example, Pred Example) x num_images columns
    fig, ax = plt.subplots(4, num_images, figsize=(num_images * 4, 16))
    fig.suptitle('Model Predictions Analysis\n', fontsize=18, fontweight='bold')          

    # Plot Correct Predictions (Row 0)
    ax[0, 0].set_ylabel('Correct\nPredictions', fontsize=12, fontweight='bold')
    for i, idx in enumerate(correct_sample_indices):
        true_class_name = class_names[y_true_indices[idx]]
        pred_class_name = class_names[y_pred_indices[idx]] # Should be same as true
        img_display = images_all[idx]
        if img_display.max() <= 1.0:
            img_display = (img_display * 255)
        ax[0, i].imshow(img_display.astype(np.uint8))
        ax[0, i].set_title(f"Pred: {pred_class_name}\n(True: {true_class_name})", fontsize=10, color='green')
        ax[0, i].axis('off')

    # Plot Incorrect Predictions (Row 1), True Class Examples (Row 2), Pred Class Examples (Row 3)
    ax[1, 0].set_ylabel('Incorrect\nPredictions', fontsize=12, fontweight='bold')
    ax[2, 0].set_ylabel('Example of\nTrue Class', fontsize=12, fontweight='bold')
    ax[3, 0].set_ylabel('Example of\nPred Class', fontsize=12, fontweight='bold')
    train_dir_path = Path(train_dir)

    for i, idx in enumerate(incorrect_sample_indices):
        true_class_idx = y_true_indices[idx]
        pred_class_idx = y_pred_indices[idx]
        true_class_name = class_names[true_class_idx]
        pred_class_name = class_names[pred_class_idx]

        # Row 1: Plot the incorrectly predicted image
        img_display = images_all[idx]
        if img_display.max() <= 1.0:
            img_display = (img_display * 255)
        ax[1, i].imshow(img_display.astype(np.uint8))
        ax[1, i].set_title(f"Pred: {pred_class_name}\n(True: {true_class_name})", fontsize=10, color='red')
        ax[1, i].axis('off')

        # Row 2: Example of True Class
        true_class_dir = train_dir_path / true_class_name
        example_img = None
        if true_class_dir.is_dir():
            try:
                possible_images = list(true_class_dir.glob('*[.jpg][.jpeg][.png]'))
                if possible_images:
                    example_img_path = random.choice(possible_images)
                    img = keras_image.load_img(example_img_path, target_size=(images_all.shape[1], images_all.shape[2]))
                    example_img = keras_image.img_to_array(img)
            except Exception as e:
                print(f"Error loading example image from {true_class_dir}: {e}")
        if example_img is not None:
            ax[2, i].imshow(example_img.astype(np.uint8))
            ax[2, i].set_title(f"Example of True:\n{true_class_name}", fontsize=10, color='blue')
        else:
            ax[2, i].text(0.5, 0.5, 'No Example Found', horizontalalignment='center', verticalalignment='center')
            ax[2, i].set_title(f"Example of True:\n{true_class_name}", fontsize=10, color='gray')
        ax[2, i].axis('off')

        # Row 3: Example of Predicted Class
        pred_class_dir = train_dir_path / pred_class_name
        pred_example_img = None
        if pred_class_dir.is_dir():
            try:
                possible_pred_images = list(pred_class_dir.glob('*[.jpg][.jpeg][.png]'))
                if possible_pred_images:
                    pred_example_img_path = random.choice(possible_pred_images)
                    pred_img = keras_image.load_img(pred_example_img_path, target_size=(images_all.shape[1], images_all.shape[2]))
                    pred_example_img = keras_image.img_to_array(pred_img)
            except Exception as e:
                print(f"Error loading example image from {pred_class_dir}: {e}")
        if pred_example_img is not None:
            ax[3, i].imshow(pred_example_img.astype(np.uint8))
            ax[3, i].set_title(f"Example of Pred:\n{pred_class_name}", fontsize=10, color='orange')
        else:
            ax[3, i].text(0.5, 0.5, 'No Example Found', horizontalalignment='center', verticalalignment='center')
            ax[3, i].set_title(f"Example of Pred:\n{pred_class_name}", fontsize=10, color='gray')
        ax[3, i].axis('off')

    plt.tight_layout()
    if file_path:
        fig.savefig(file_path, dpi=300, bbox_inches='tight')
        print(f"\nPredictions plot saved to {file_path}\n")
    plt.show()

# ------------------------------------------------------------------------
# Function to plot images in a row with titles
def plot_images_from_directory(image_paths: Union[str, List[Union[str, Path]]],
                               titles: Union[str, List[str]],
                               num_rows: int = 1):
    """
    Plots images in a grid layout with titles.

    Args:
        image_paths (Union[str, List[Union[str, Path]]]): List of image file paths 
                                                          (or Path objects) to display.
        titles (Union[str, List[str]]): List of titles corresponding to each image.
        num_rows (int): Number of rows to arrange the images. Default is 1.

    Returns:
        None: Displays the images in a grid layout.
    """
    # Ensure image_paths and titles are lists
    if isinstance(image_paths, (str, Path)):
        image_paths = [image_paths]
    if isinstance(titles, str):
        titles = [titles]

    # Convert string paths to Path objects for robustness
    image_paths = [Path(p) for p in image_paths]

    # Check if the number of images matches the number of titles
    if len(image_paths) != len(titles):
        raise ValueError("The number of image paths and titles must be the same.")

    # Calculate the number of columns needed
    num_images = len(image_paths)
    num_cols = (num_images + num_rows - 1) // num_rows

    # Create a figure with the specified number of rows and columns
    # Adjust figsize dynamically based on columns and rows
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4.5))

    # Ensure 'axes' is always a flat NumPy array, even if only one subplot is created
    # np.ravel handles single Axes object, 1D array, and 2D array correctly
    axes = np.ravel(axes)

    # Iterate through the images and plot them
    for i, (image_path, title) in enumerate(zip(image_paths, titles)):
        try:
            # Load the image using PIL
            image = Image.open(image_path)

            # Display the image on the i-th Axes object
            axes[i].imshow(image)
            # Set the title for the i-th Axes object
            axes[i].set_title(title, fontsize=10, fontweight='bold')
            # Remove axis for cleaner visualization
            axes[i].axis('off')
            
        except FileNotFoundError:
            print(f"Warning: Image file not found at {image_path}. Skipping.")
            axes[i].text(0.5, 0.5, 'Image Not Found', horizontalalignment='center', verticalalignment='center')
            axes[i].set_title(title, fontsize=10, fontweight='bold', color='red')
            axes[i].axis('off')
        
        except Exception as e:
            print(f"Warning: Could not load or plot image {image_path}. Error: {e}")
            axes[i].text(0.5, 0.5, 'Error Loading', horizontalalignment='center', verticalalignment='center')
            axes[i].set_title(title, fontsize=10, fontweight='bold', color='red')
            axes[i].axis('off')


    # Hide any unused subplots at the end
    for j in range(num_images, len(axes)):
        axes[j].axis('off')

    # Adjust layout to prevent titles/labels overlapping
    plt.tight_layout()
    plt.show()

======================================================================================== utilities_InnovativeApproaches.py

# ==================================================================
# Functions DL Project 
#
# This file contains utility auxiliary functions for the DL project.
#
# Group: 37
# Members:
#   - André Silvestre, 20240502
#   - Diogo Duarte, 20240525
#   - Filipa Pereira, 20240509
#   - Maria Cruz, 20230760
#   - Umeima Adam Mahomed, 20240543
# ==================================================================

# System Libraries  
import os
import random
from pathlib import Path
import importlib
import utilities_InnovativeApproaches
import datetime
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="matplotlib")
from typing import Union, List
    
# Data Manipulation & Visualization Libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from matplotlib.image import imread
from PIL import Image

# Setting seaborn style
sns.set_theme(style="white")


# ------------------------------------------------------------------------
# Function to reload utilities.py in Jupyter Notebook
def reload_utilities():
    """
    Reload the utilities module in Jupyter Notebook.
    
    This function is useful for reloading the utilities module after making changes to it.
    """

    importlib.reload(utilities_InnovativeApproaches)
    print(f"Utilities module reloaded successfully ({datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S')})")
    
# ------------------------------------------------------------------------
# Function to plot images in a row with titles
def plot_images_from_directory(image_paths: Union[str, List[Union[str, Path]]],
                               titles: Union[str, List[str]],
                               num_rows: int = 1):
    """
    Plots images in a grid layout with titles.

    Args:
        image_paths (Union[str, List[Union[str, Path]]]): List of image file paths 
                                                          (or Path objects) to display.
        titles (Union[str, List[str]]): List of titles corresponding to each image.
        num_rows (int): Number of rows to arrange the images. Default is 1.

    Returns:
        None: Displays the images in a grid layout.
    """
    # Ensure image_paths and titles are lists
    if isinstance(image_paths, (str, Path)):
        image_paths = [image_paths]
    if isinstance(titles, str):
        titles = [titles]

    # Convert string paths to Path objects for robustness
    image_paths = [Path(p) for p in image_paths]

    # Check if the number of images matches the number of titles
    if len(image_paths) != len(titles):
        raise ValueError("The number of image paths and titles must be the same.")

    # Calculate the number of columns needed
    num_images = len(image_paths)
    num_cols = (num_images + num_rows - 1) // num_rows

    # Create a figure with the specified number of rows and columns
    # Adjust figsize dynamically based on columns and rows
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4.5))

    # Ensure 'axes' is always a flat NumPy array, even if only one subplot is created
    # np.ravel handles single Axes object, 1D array, and 2D array correctly
    axes = np.ravel(axes)

    # Iterate through the images and plot them
    for i, (image_path, title) in enumerate(zip(image_paths, titles)):
        try:
            # Load the image using PIL
            image = Image.open(image_path)

            # Display the image on the i-th Axes object
            axes[i].imshow(image)
            # Set the title for the i-th Axes object
            axes[i].set_title(title, fontsize=10, fontweight='bold')
            # Remove axis for cleaner visualization
            axes[i].axis('off')
            
        except FileNotFoundError:
            print(f"Warning: Image file not found at {image_path}. Skipping.")
            axes[i].text(0.5, 0.5, 'Image Not Found', horizontalalignment='center', verticalalignment='center')
            axes[i].set_title(title, fontsize=10, fontweight='bold', color='red')
            axes[i].axis('off')
        
        except Exception as e:
            print(f"Warning: Could not load or plot image {image_path}. Error: {e}")
            axes[i].text(0.5, 0.5, 'Error Loading', horizontalalignment='center', verticalalignment='center')
            axes[i].set_title(title, fontsize=10, fontweight='bold', color='red')
            axes[i].axis('off')


    # Hide any unused subplots at the end
    for j in range(num_images, len(axes)):
        axes[j].axis('off')

    # Adjust layout to prevent titles/labels overlapping
    plt.tight_layout()
    plt.show()
    
# ------------------------------------------------------------------------
# 
# Source: https://medium.com/@kerry.halupka/getting-started-with-openais-clip-a3b8f5277867
#         
# Function to visualize images and their probabilities
# Updated to fix the TypeError and ensure proper layout of image and graph
# ------------------------------------------------------------------------
def visualize_images_and_probs(images: Image.Image, probs, 
                               classes: List[str], title: str = "Image Probabilities"):
    """
    Visualizes an image alongside its corresponding probabilities in a horizontal layout.

    Args:
        images (Image.Image): A PIL Image object to display.
        probs (torch.Tensor): A 1D tensor of probabilities for each class.
        classes (List[str]): A list of class labels corresponding to the probabilities.
        title (str): Title for the plot. Default is "Image Probabilities".

    Returns:
        None: Displays the image and its probabilities side by side.
    """
    # Ensure probabilities are a 1D NumPy array
    prob_values = probs.detach().numpy().flatten()

        # Check if the length of probs and classes match
    if len(prob_values) != len(classes):
        raise ValueError(f"Length mismatch: {len(prob_values)} probabilities and {len(classes)} classes.")

    # Determine the color for each bar based on the class name
    color_palette_dict = {'animal': '#22c1c3','not animal': '#090979',}
    bar_colors = [color_palette_dict.get(cls, '#808080') for cls in classes] # Default to gray if class not in dict

    # Create a figure with two subplots
    fig, axes = plt.subplots(1, 2, figsize=(18, 6))

    # --- Plot Image ---
    axes[0].imshow(images)
    axes[0].axis('off')
    axes[0].set_title("Image", fontsize=14, fontweight='bold')

    # --- Plot Probabilities Bar Chart ---
    # Create horizontal bars, assigning colors individually
    y_pos = np.arange(len(classes)) # Positions for the bars
    axes[1].barh(y_pos, prob_values, color=bar_colors, alpha=0.8) # Use the 'color' argument

    # Set y-axis ticks and labels
    axes[1].set_yticks(y_pos)
    axes[1].set_yticklabels(classes, fontsize=10)
    axes[1].invert_yaxis()  # Labels read top-to-bottom

    # Set x-axis label and limits
    axes[1].set_xlabel("Probability", fontsize=12)
    axes[1].set_xlim(0, 1.0)

    # Set title for the bar chart
    axes[1].set_title(title, fontsize=14, fontweight='bold')

    # Add probability values as text labels on the bars
    for index, value in enumerate(prob_values):
        # Position text slightly to the right of the bar end
        axes[1].text(x=value + 0.01,                # x position
                     y=index,                       # y position    
                     s=f"{value*100:.1f}%",         # Text to display
                     va='center', fontsize=10,      # Vertical alignment 
                     color=color_palette_dict.get(classes[index], '#808080'))  # Adjust text color based on the class name


    axes[1].grid(axis='x', linestyle='--', alpha=0.7)

    # Remove spines for a cleaner look using Seaborn's despine
    sns.despine(ax=axes[1], left=True, bottom=True, top=True, right=True)

    # Adjust layout and display
    plt.tight_layout()
    plt.show()


======================================================================================== 1_BU&EDA&FE_DLProject_Group37.ipynb




No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
Project Structure
Data Understanding
Data Preparation
Missing Values
Duplicates
Outliers
Feature Engineering
Modeling & Evaluation
1 | Business & Data Understanding



🎲 Dataset[1])
In this project, we will develop a deep-learning model to classify rare species based on their images. The dataset used in this project consists of images of rare species sourced from the Encyclopedia of Life (EOL) and curated as part of the BioCLIP: A Vision Foundation Model for the Tree of Life [1]) study. Each image is associated with metadata, including its kingdom, phylum, and family.

No description has been provided for this image

Figure 1 | Phylogenetic tree of the phyla in the dataset.
Source: Hugging Face Datasets (2024)



👨‍💻 Project Goals
The objective is to build a model that predicts the family of a given species based on its image.

The dataset includes a structured CSV file containing the file path of each image alongside its corresponding labels.

As part of the project, we will need to create our own data splits into training, validation, and test sets. The training and validation sets will be used to develop and fine-tune your model, while the test set should be kept separate to evaluate its performance on unseen data.




Dataset Attributes: [2]
Table 1 | Dataset Attributes with Descriptions

ATTRIBUTE	DESCRIPTION	DATA TYPE
1	rare_species_id	Unique identifier for the image in the dataset.	String
2	eol_content_id	Unique identifier within EOL database for images sourced from EOL.	Int
3	eol_page_id	Identifier of page from which images from EOL are sourced.	Int
4	kingdom	Kingdom to which the subject of the image belongs (all Animalia).	String
5	phylum	Phylum to which the subject of the image belongs.	String
6	family	Family to which the subject of the image belongs.	String
7	file_path	Path to the image file.	String



📚 Libraries Import
# Print python version
import sys
print(sys.version)
3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
# System imports
import os
from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning imports
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Deep learning imports
import tensorflow as tf

# Image processing imports
from PIL import Image

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# For better resolution plots
%config InlineBackend.figure_format = 'retina'
# Setting seaborn style
sns.set_theme(style="white")

# Set random seed for reproducibility
np.random.seed(2025)
2025-04-13 16:48:42.727001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744559322.758991   30484 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744559322.769975   30484 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744559322.803640   30484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744559322.803685   30484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744559322.803689   30484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744559322.803692   30484 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-13 16:48:42.812572: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())

if tf.test.is_built_with_cuda():
    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)
TensorFlow Version: 2.19.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744559325.991517   30484 gpu_device.cc:2019] Created device /device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
🧮 Import Databases
# Load the data
rare_species_df = pd.read_csv("./data/rare_species 1/metadata.csv")

# Display the first 5 rows of the dataset
rare_species_df.head(5)
rare_species_id	eol_content_id	eol_page_id	kingdom	phylum	family	file_path
0	75fd91cb-2881-41cd-88e6-de451e8b60e2	12853737	449393	animalia	mollusca	unionidae	mollusca_unionidae/12853737_449393_eol-full-si...
1	28c508bc-63ff-4e60-9c8f-1934367e1528	20969394	793083	animalia	chordata	geoemydidae	chordata_geoemydidae/20969394_793083_eol-full-...
2	00372441-588c-4af8-9665-29bee20822c0	28895411	319982	animalia	chordata	cryptobranchidae	chordata_cryptobranchidae/28895411_319982_eol-...
3	29cc6040-6af2-49ee-86ec-ab7d89793828	29658536	45510188	animalia	chordata	turdidae	chordata_turdidae/29658536_45510188_eol-full-s...
4	94004bff-3a33-4758-8125-bf72e6e57eab	21252576	7250886	animalia	chordata	indriidae	chordata_indriidae/21252576_7250886_eol-full-s...
# Number of rows and columns
print("\033[1mDataset \033[0m")
print("  Number of rows:", rare_species_df.shape[0])
print("  Number of columns:", rare_species_df.shape[1])

# Name of the columns
print("\n\033[1mColumns \033[0m")
print("  ", rare_species_df.columns.to_list())
Dataset 
  Number of rows: 11983
  Number of columns: 7

Columns 
   ['rare_species_id', 'eol_content_id', 'eol_page_id', 'kingdom', 'phylum', 'family', 'file_path']
# Colors list for plotting
colors = ['#22c1c3', '#27b1dd', '#2d9cfd', '#090979']
📋 Evaluate Data
Before building a deep learning model, it is essential to:

Address Data Duplicates since they can significantly skew analysis and model training;
Validate Data Types to ensure accurate analysis and modeling;
Manage Missing Values to prevent errors during analysis and training;
Assess Unique Value Counts to understand data distribution and identify potential issues, such as uninformative columns with only one value.
🔄 Duplicates
# Checking for duplicates
print(f"\nNumber of duplicate rows: {rare_species_df.duplicated().sum()}")
Number of duplicate rows: 0
🔢🔠 Data Types
# Check data types
pd.DataFrame(rare_species_df.dtypes, columns=['Data Type'])
Data Type
rare_species_id	object
eol_content_id	int64
eol_page_id	int64
kingdom	object
phylum	object
family	object
file_path	object
🔎 Missing Values and Distinct Values
# Check for missing values (n = number of missing values & % = percentage of missing values)
pd.DataFrame({
    "n NAs": rare_species_df.isnull().sum(),
    "% NAs": round(rare_species_df.isnull().mean() * 100, 1)
})
n NAs	% NAs
rare_species_id	0	0.0
eol_content_id	0	0.0
eol_page_id	0	0.0
kingdom	0	0.0
phylum	0	0.0
family	0	0.0
file_path	0	0.0
# Check for unique values in each column
pd.DataFrame(rare_species_df.nunique(), columns=['Number of Unique Values'])
Number of Unique Values
rare_species_id	11983
eol_content_id	11983
eol_page_id	400
kingdom	1
phylum	5
family	202
file_path	11983

2 | Data Preparation



🔎 EDA - Exploratory Data Analysis
phylum variable
# Absolute and Relative Frequency of the 'phylum' column
pd.DataFrame({
    "n": rare_species_df['phylum'].value_counts(),
    "%": round(rare_species_df['phylum'].value_counts(normalize=True) * 100, 2)
}).sort_values(by='n', ascending=False)
n	%
phylum		
chordata	9952	83.05
arthropoda	951	7.94
cnidaria	810	6.76
mollusca	210	1.75
echinodermata	60	0.50
# Color palette associated with the 'phylum' column
phylum_palette = {
    'chordata': '#22c3be',
    'arthropoda': '#27c8dd',
    'cnidaria': '#2dc6fd',
    'mollusca': '#2d9cfd',
    'echinodermata': '#090979',
}

# Plot the distribution of the 'phylum' column
plt.figure(figsize=(10, 6))
sns.countplot(data=rare_species_df, 
              x='phylum', 
              order=rare_species_df['phylum'].value_counts().index,
              hue='phylum',
              palette=colors,
              stat='count')

# Annotate the plot with the percentage of each class
for p in plt.gca().patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / rare_species_df.shape[0])
    x = p.get_x() + p.get_width() / 2 - 0.1
    y = p.get_height() + 50
    plt.gca().annotate(percentage, (x, y), size = 12)


plt.title("Distribution of the 'phylum' column", fontsize=16, weight='bold')
plt.xlabel("")
plt.ylabel("n", fontsize=14, weight='bold')
plt.xticks(rotation=0, fontsize=12)
sns.despine(right=True, top=True)
plt.tight_layout()
plt.show()
No description has been provided for this image
The phylum variable is a categorical variable that represents the taxonomic rank of the species.
Analysis of the phylum variable reveals that:
the majority of the images belong to the Chordata (
83
%
) phylum, which includes vertebrates such as mammals, birds, reptiles, amphibians, and fish.
The second most common phylum is Arthropoda (
≈
8
%
), which includes invertebrates such as insects, arachnids, and crustaceans.
The third most common phylum is Cnidaria (
≈
7
%
), which includes invertebrates such as jellyfish, corals, and sea anemones.
The fourth most common phylum is Mollusca 
≈
2
%
, which includes invertebrates such as snails, clams, and octopuses.
The fifth most common phylum is Echinodermata (
0.5
%
), which includes invertebrates such as sea stars, sea urchins, and sea cucumbers.
Source: http://www.fossilmuseum.net/Tree_of_Life/Kingdom_animalia/animalphyla.htm

family variable (Target Variable)
# Absolute and Relative Frequency of the 'family' column
pd.DataFrame({
    "n": rare_species_df['family'].value_counts(),
    "%": round(rare_species_df['family'].value_counts(normalize=True) * 100, 2)
}).sort_values(by='n', ascending=False)
n	%
family		
dactyloidae	300	2.50
cercopithecidae	300	2.50
formicidae	291	2.43
carcharhinidae	270	2.25
salamandridae	270	2.25
...	...	...
cyprinodontidae	30	0.25
alligatoridae	30	0.25
balaenidae	30	0.25
goodeidae	30	0.25
siluridae	29	0.24
202 rows × 2 columns

# Distribution of target variable ('family')
plt.figure(figsize=(10, 6))
sns.countplot(data=rare_species_df, 
              x='family', 
              order=rare_species_df['family'].value_counts().index,
              hue='phylum',
              palette=phylum_palette,
              stat='count')
    
plt.title("Distribution of the 'family' column", fontsize=16, weight='bold')
plt.xlabel("")
plt.ylabel("n", fontsize=14, weight='bold')
plt.xticks(rotation=90, fontsize=5)
sns.despine(right=True, top=True)
plt.tight_layout()
plt.show()
No description has been provided for this image
family is the target variable for this project. It is a categorical variable with 202 distinct classes.
The distribution of the target variable is imbalanced, with some classes having significantly more samples than others. This imbalance can affect the performance of machine learning models, as they may be biased towards the majority classes.
To address this issue, we will consider using techniques such as oversampling, undersampling, or class weighting during model training to ensure that the model learns effectively from all classes.
# Plot only the top 50 common families
plt.figure(figsize=(10, 6))
sns.countplot(data=rare_species_df.sort_values(by='family', ascending=False),
              x='family', 
              order=rare_species_df['family'].value_counts().index[:50],
              hue='phylum',
              palette=phylum_palette,
              stat='count')
plt.title("Distribution of the 'family' column (Top 50)", fontsize=16, weight='bold')
plt.xlabel("")
plt.ylabel("n", fontsize=14, weight='bold')
plt.xticks(rotation=90, fontsize=10)
sns.despine(right=True, top=True)
plt.tight_layout()
plt.show()
No description has been provided for this image
# Grouping by 'family' and counting unique 'phylum'
family_phylum_counts = rare_species_df.groupby('family')['phylum'].nunique().reset_index()

# Filtering families with more than one unique 'phylum'
families_multiple_phylum = family_phylum_counts[family_phylum_counts['phylum'] > 1]

print("Families with multiple phylum:", families_multiple_phylum.shape[0])
Families with multiple phylum: 0
kingdom variable
# Absolute and Relative Frequency of the 'kingdom' column
pd.DataFrame({
    "n": rare_species_df['kingdom'].value_counts(),
    "%": round(rare_species_df['kingdom'].value_counts(normalize=True) * 100, 2)
}).sort_values(by='n', ascending=False)
n	%
kingdom		
animalia	11983	100.0
Note: The kingdom column only contains one unique value, which is Animalia. Therefore, it is not useful for our analysis and will be dropped from the dataset.

# Drop the 'kingdom' column
rare_species_df.drop(columns=['kingdom'], inplace=True)
# Path to the folder
image_folder = './data/rare_species 1/'

# List all images in the folder
image_files = []
for root, _, files in os.walk(image_folder):
    for file in files:
        if file.lower().endswith(('.jpg')):
            image_files.append(os.path.join(root, file))
# Check if the folder in 'file_path' are already the 'family' classes to predict

# Function to check folder structure
def check_folder_structure(base_dirs, expected_families):
    """Checks if folders in each split match the expected family classes.
    
    Args:
        base_dirs (dict): Dictionary of split names and their Paths.
        expected_families (set): Set of expected family names from metadata.
    
    Returns:
        None: Prints results of the check.
    """
    for split_name, dir_path in base_dirs.items():
        if not dir_path.exists():
            print(f"{split_name} directory does not exist: {dir_path}")
            continue
        
        # Get folder names (assuming format like 'chordata_familyname')
        actual_folders = {f.split('_', 1)[1] for f in os.listdir(dir_path) if os.path.isdir(dir_path / f)}
        print(f"\n\033[1mChecking {split_name} split\033[0m")
        print(f"Number of folders found: {len(actual_folders)}")
        print(f"Sample folders: {list(actual_folders)[:5]}...")
        
        # Compare with expected families
        missing_families = expected_families - actual_folders
        extra_folders = actual_folders - expected_families
        
        if not missing_families and not extra_folders:
            print(f"{split_name} folders match the expected families perfectly.")
        else:
            if missing_families:
                print(f"Missing families in {split_name}: {len(missing_families)} - {list(missing_families)[:5]}...")
            if extra_folders:
                print(f"Extra folders in {split_name}: {len(extra_folders)} - {list(extra_folders)[:5]}...")

# Run the check
check_folder_structure(base_dirs={"Full dataset": Path("data/rare_species 1"),} , expected_families=set(rare_species_df['family'].unique()))
Checking Full dataset split
Number of folders found: 202
Sample folders: ['pleuronectidae', 'percidae', 'dasyatidae', 'atelidae', 'leporidae']...
Full dataset folders match the expected families perfectly.
# Function to show an image
def show_image(image_path: str) -> None:
    """
    Displays an image given its path.
    """
    image = Image.open(image_path)      # Open the image
    plt.figure(figsize=(10, 10))        # Set the figure size   
    plt.imshow(image)                   # Display the image
    plt.axis('off')                     # Remove axis for cleaner visualization
    plt.show()                          # Show the image
# Show an example
if image_files:
    print(f"Displaying image: {image_files[1]}")
    show_image(image_files[1])
else:
    print("No images found in the folder.")
Displaying image: ./data/rare_species 1/chordata_nesospingidae/22028338_45510504_eol-full-size-copy.jpg
No description has been provided for this image
# file_path structure:
# phylum_family/eol_content_id_eol_page_id
# (no new information can be extracted by the file_path column)
# Count the number of images
print("Number of images:", len(image_files))
Number of images: 11983
# Function to display images with simplified titles
def show_images(df, split_name, n=5):
    """Displays n images from a split with phylum and family titles."""
    plt.figure(figsize=(15, 3))
    if len(df) > n:
        # Randomly sample n images if n is less than the number of images (list)
        sample_df = df.sample(n=n, random_state=42)
    else:
        # Select the first n images if n is equal than the number of images
        sample_df = df.iloc[:n]
    for i, (_, row) in enumerate(sample_df.iterrows()):
        img_path = os.path.join('./data/rare_species 1', row['file_path'])
        img = Image.open(img_path)
        plt.subplot(1, n, i+1)
        plt.imshow(img)
        plt.title(f"Phylum: {row['phylum']} \n Family: {row['family']}", fontsize=10)
        plt.axis('off')
    plt.suptitle(f"{split_name} Sample Images", fontsize=12, weight='bold')
    plt.tight_layout()
    plt.show()
show_images(rare_species_df[5:10], split_name="All Dataset", n=5)
No description has been provided for this image
Note: As we can see in the first picture, there are "outliers" that do not visually represent the species/family, but rather information about it.

We are aware that this factor could influence the model results.
🔧 Data Pre-Proccessing
# Encode the target variable ('family')
label_encoder = LabelEncoder()
rare_species_df['family_encoded'] = label_encoder.fit_transform(rare_species_df['family'])
num_classes = len(label_encoder.classes_)
print(f"\nNumber of unique families (classes): {num_classes}")
Number of unique families (classes): 202
# Encode the 'phylum' column
phylum_encoder = LabelEncoder()
rare_species_df['phylum_encoded'] = phylum_encoder.fit_transform(rare_species_df['phylum'])
num_phylum = len(phylum_encoder.classes_)
print(f"\nNumber of unique phylum: {num_phylum}")
Number of unique phylum: 5
# Check Label Encoding for 'phylum'
pd.DataFrame({
    'phylum': phylum_encoder.classes_,
    'phylum_encoded': np.arange(num_phylum)
}).sort_values(by='phylum_encoded')
phylum	phylum_encoded
0	arthropoda	0
1	chordata	1
2	cnidaria	2
3	echinodermata	3
4	mollusca	4
# Check Label Encoding for 'family'
pd.DataFrame({
    'family': label_encoder.classes_,
    'family_encoded': np.arange(num_classes)
}).sort_values(by='family_encoded')
family	family_encoded
0	accipitridae	0
1	acipenseridae	1
2	acroporidae	2
3	agamidae	3
4	agariciidae	4
...	...	...
197	vespertilionidae	197
198	viperidae	198
199	vireonidae	199
200	vombatidae	200
201	zonitidae	201
202 rows × 2 columns

Note: We chose LabelEncoder over OHE due to 202 classes, avoiding large sparse vectors that increase memory and training time.

⚖️ Training/Validation Division
# Split the dataset into train, validation, and test sets (80-10-10 split)
# Stratify on 'family' to maintain class distribution
train_df, temp_df = train_test_split(rare_species_df,
                                     test_size=0.2,
                                     stratify=rare_species_df['family'],
                                     random_state=42,
                                     shuffle=True)

val_df, test_df = train_test_split(temp_df, 
                                   test_size=0.5, 
                                   stratify=temp_df['family'], 
                                   random_state=42,
                                   shuffle=True)
print(f"\n\033[1mTrain set size:\033[0m {len(train_df)} ({round(len(train_df) / len(rare_species_df) * 100, 2)}%)")
print(f"\033[1mValidation set size:\033[0m {len(val_df)} ({round(len(val_df) / len(rare_species_df) * 100, 2)}%)")
print(f"\033[1mTest set size:\033[0m {len(test_df)} ({round(len(test_df) / len(rare_species_df) * 100, 2)}%)")
Train set size: 9586 (80.0%)
Validation set size: 1198 (10.0%)
Test set size: 1199 (10.01%)
# Data Splitting | Train, Validation, and Test Image Paths
train_image_paths = [os.path.join(image_folder, file_path) for file_path in train_df['file_path']]
val_image_paths = [os.path.join(image_folder, file_path) for file_path in val_df['file_path']]
test_image_paths = [os.path.join(image_folder, file_path) for file_path in test_df['file_path']]

print(f"\n\033[1mNumber of train images:\033[0m {len(train_image_paths)}")
print(f"\033[1mNumber of validation images:\033[0m {len(val_image_paths)}")
print(f"\033[1mNumber of test images:\033[0m {len(test_image_paths)}")
Number of train images: 9586
Number of validation images: 1198
Number of test images: 1199
# Function to move images to their respective folders (train, validation, and test)
import os
import shutil
from pathlib import Path

def copy_images(train_df, val_df, test_df, base_dir='dataset', source_dir=None):
    """
    Copies images to train, validation, and test folders, preserving the original folder structure by family.
    
    Parameters:
    -----------
    train_df : pandas.DataFrame
        DataFrame containing training set data with 'file_path' and 'family' columns.
    val_df : pandas.DataFrame
        DataFrame containing validation set data with 'file_path' and 'family' columns.
    test_df : pandas.DataFrame
        DataFrame containing test set data with 'file_path' and 'family' columns.
    base_dir : str, optional (default='dataset')
        Base directory where train, val, and test subfolders will be created.
    source_dir : str, optional (default=None)
        Source directory containing the original images (e.g., 'rare_species 1'). 
        If None, assumes file_path is absolute.
    
    Returns:
    --------
    None
    
    Raises:
    -------
    FileNotFoundError
        If an image file specified in the DataFrame is not found in the source directory.
    OSError
        If there are permission issues or other OS-related errors during file operations.
    
    Notes:
    ------
    - Creates subdirectories 'train', 'val', and 'test' under base_dir, with subfolders for each family.
    - Copies images to maintain the original directory structure (e.g., chordata_trogonidae).
    - Preserves the original files in the source directory unchanged.
    
    Sources:
    --------
    - os: Python Standard Library (https://docs.python.org/3/library/os.html)
    - shutil: Python Standard Library (https://docs.python.org/3/library/shutil.html)
    - pathlib.Path: Python Standard Library (https://docs.python.org/3/library/pathlib.html)
    """
    
    # Define destination directories
    train_dir = os.path.join(base_dir, 'train')
    val_dir = os.path.join(base_dir, 'val')
    test_dir = os.path.join(base_dir, 'test')
    
    # Create main directories if they don't exist
    for directory in [train_dir, val_dir, test_dir]:
        Path(directory).mkdir(parents=True, exist_ok=True)  # Source: pathlib.Path
    
    # Dictionary mapping DataFrames to their respective directories
    split_mapping = {
        'train': (train_df, train_dir),
        'val': (val_df, val_dir),
        'test': (test_df, test_dir)
    }

    # Process each split
    for split_name, (df, dest_dir) in split_mapping.items():
        print(f"Copying images to {split_name} directory...")
        for index, row in df.iterrows():
            # Get the source file path
            if source_dir:
                src_path = os.path.join(source_dir, row['file_path'])
            else:
                src_path = row['file_path']  # Assume absolute path if no source_dir
            
            # Extract family and construct family-specific subfolder
            phylum = row['phylum']
            family = row['family']
            family_dir = os.path.join(dest_dir, f"{phylum}_{family}")  # Match original structure
            
            # Create family subfolder if it doesn't exist
            Path(family_dir).mkdir(parents=True, exist_ok=True)  # Source: pathlib.Path
            
            # Extract filename and construct destination path
            file_name = os.path.basename(src_path)
            dest_path = os.path.join(family_dir, file_name)
            
            try:
                # Check if source file exists
                if not os.path.exists(src_path):
                    raise FileNotFoundError(f"Image not found: {src_path}")
                
                # Copy the file to the destination directory
                shutil.copy2(src_path, dest_path)                   # Source: shutil (preserves metadata)
                # print(f"Copied {file_name} to {family_dir}")      # Uncomment for verbose output
            
            except FileNotFoundError as e:
                print(f"Error: {e}")
                continue
            except OSError as e:
                print(f"OS Error: Could not copy {src_path} to {dest_path}. Reason: {e}")
                continue
    
    print("\nImage copying process completed.")

# Copy images to train, validation, and test folders
copy_images(train_df, val_df, test_df, base_dir='./data/', source_dir='./data/rare_species 1')
Copying images to train directory...
Copying images to val directory...
Copying images to test directory...

Image copying process completed.
# Show 5 sample images from each split (stratified by phylum)
show_images(train_df.groupby('phylum', group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)), split_name="Train", n=5)
show_images(val_df.groupby('phylum', group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)),  split_name="Validation", n=5)
show_images(test_df.groupby('phylum', group_keys=False).apply(lambda x: x.sample(n=5, random_state=42)),  split_name="Test", n=5)
No description has been provided for this image
No description has been provided for this image
No description has been provided for this image
💾 Save Data
# Check the distribution of the target variable in the train, validation, and test sets
train_distribution = train_df['family'].value_counts(normalize=True)
val_distribution = val_df['family'].value_counts(normalize=True)
test_distribution = test_df['family'].value_counts(normalize=True)

# Create a DataFrame to compare distributions
distribution_df = pd.DataFrame({
    'Train': train_distribution,
    'Validation': val_distribution,
    'Test': test_distribution
}).fillna(0)
distribution_df = distribution_df.sort_index()  # Sort by index (family names)
distribution_df
Train	Validation	Test
family			
accipitridae	0.009910	0.010017	0.010008
acipenseridae	0.007511	0.007513	0.007506
acroporidae	0.017526	0.017529	0.017515
agamidae	0.005007	0.005008	0.005004
agariciidae	0.010015	0.010017	0.010008
...	...	...	...
vespertilionidae	0.005007	0.005008	0.005004
viperidae	0.002504	0.002504	0.002502
vireonidae	0.002504	0.002504	0.002502
vombatidae	0.002504	0.002504	0.002502
zonitidae	0.002504	0.002504	0.002502
202 rows × 3 columns

# Save the train, validation, and test DataFrames to CSV files
train_df.to_csv('./data/train.csv', index=False)
val_df.to_csv('./data/val.csv', index=False)
test_df.to_csv('./data/test.csv', index=False)
# # Move all subfolders (train, val, test) from the dataset folder (./data/) to a new folder named 'RareSpecies_Split' (Terminal)
# mkdir -p ./data/RareSpecies_Split
# mv ./data/train ./data/RareSpecies_Split/
# mv ./data/val ./data/RareSpecies_Split/
# mv ./data/test ./data/RareSpecies_Split/

# # Zip the RareSpecies_Split folder
# cd ./data/
# zip -r ./RareSpecies_Split_withoutSMOTE.zip ./RareSpecies_Split/
🔗 Bibliography/References
[1]) Stevens, S., Wu, J., Thompson, M. J., Campolongo, E. G., Song, C. H., Carlyn, D. E., ... & Su, Y. (2024). BioCLIP: A Vision Foundation Model for the Tree of Life. arXiv [Cs.CV] http://arxiv.org/abs/2311.18803

[2] HDR Imageomics Institute. (2024, May 17). TreeOfLife-10M. Huggingface.co. https://huggingface.co/datasets/imageomics/TreeOfLife-10M



======================================================================================== 2_ImagePreprocessing&DataAugmentation_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import random
import datetime
from tqdm import tqdm

from pathlib import Path        # For file path manipulations

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from tensorflow.keras.preprocessing.image import (
    load_img, img_to_array, save_img
)

# Image processing imports (Data Augmentation layers)
from tensorflow.keras.layers import (
    Resizing, Rescaling, CenterCrop, AutoContrast, Equalization, MixUp, 
    RandAugment, RandomBrightness, RandomColorDegeneration, RandomColorJitter, 
    RandomContrast, RandomCrop, RandomFlip, RandomGrayscale, RandomHue, 
    RandomRotation, RandomSaturation, RandomSharpness, 
    RandomShear, RandomTranslation, RandomZoom
)
from tensorflow.keras.models import Sequential      # To chain augmentation layers

# Image processing imports
from PIL import Image

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# For better resolution plots
%config InlineBackend.figure_format = 'retina'
# Setting seaborn style
sns.set_theme(style="white")

# Set random seeds for reproducibility
tf.random.set_seed(2025)
np.random.seed(2025)
random.seed(2025)
2025-04-13 17:53:27.422019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744563207.597953   18505 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744563207.646910   18505 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744563208.028596   18505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744563208.028627   18505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744563208.028630   18505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744563208.028632   18505 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-13 17:53:28.067196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Check if TensorFlow is installed and CUDA is available
# Source: https://www.tensorflow.org/install/source?hl=pt-br#gpu_support_3
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
TensorFlow Version: 2.19.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744563212.910143   18505 gpu_device.cc:2019] Created device /device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
# Extra: https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth
# If you’re using a GPU, TensorFlow might pre-allocate GPU memory, leaving less for CPU operations. 
# Enabling memory growth lets the GPU allocate only what’s needed.
if tf.test.is_built_with_cuda():
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        tf.config.experimental.set_memory_growth(gpus[0], True)
🧮 Import Databases
# Define the path to the data
base_data_dir = Path("data/RareSpecies_Split")
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 32                                     # Batch size
input_shape = (img_width, img_height, 3)            # Input shape of the model
value_range = (0.0, 1.0)                            # Range of pixel values
# Import custom module for importing data, visualization, and utilities
import utilities
# Get class names from directory
class_names = sorted(os.listdir(train_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical', 
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.
I0000 00:00:1744563214.399062   18505 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
# Inspect the dataset object
print(f"\nTrain Dataset: {train_datagen}")
print(f"\nTrain File Paths:")
print(train_datagen.file_paths[:3])
print(f"\nTrain Classes Names:")
print(train_datagen.class_names[:3])
print(f"\nNumber of Classes: {len(train_datagen.class_names)}")
Train Dataset: <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 202), dtype=tf.float32, name=None))>

Train File Paths:
['data/RareSpecies_Split/train/chordata_vespertilionidae/21686384_327547_eol-full-size-copy.jpg', 'data/RareSpecies_Split/train/chordata_mimidae/22894726_1047771_eol-full-size-copy.jpg', 'data/RareSpecies_Split/train/chordata_daubentoniidae/29345771_324407_eol-full-size-copy.jpg']

Train Classes Names:
['arthropoda_apidae', 'arthropoda_attelabidae', 'arthropoda_carabidae']

Number of Classes: 202
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)
Train batch shape: (32, 224, 224, 3) (32, 202)
2025-04-13 17:53:38.032420: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Val batch shape: (32, 224, 224, 3) (32, 202)
2025-04-13 17:53:38.818517: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Test batch shape: (32, 224, 224, 3) (32, 202)
# Check 5 images from the training set

# Function to display images from a tf.data.Dataset
def show_images_from_dataset(dataset: tf.data.Dataset, num_images: int = 5, title: str = "Sample Images") -> None:
    """
    Displays a specified number of images from a tf.data.Dataset.
    
    Args:
        dataset (tf.data.Dataset): TensorFlow dataset containing images.
        num_images (int): Number of images to display. Default is 5.
        title (str): Title for the plot. Default is "Sample Images".
        
    Returns:
        None. Displays the images.
    """
    # Create a figure with subplots
    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))
    
    # Loop through the dataset and display images
    for i, (image, label) in enumerate(dataset.take(num_images).as_numpy_iterator()):
        axes[i].imshow(image[0].astype(np.uint8))             # Display the image directly
        axes[i].axis('off')                                   # Remove axis for cleaner visualization
    
    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)  # Set the title for the plot    
    plt.show()  # Show the plot
    
# Show 5 images from the training set
show_images_from_dataset(train_datagen, num_images=5, title="Sample Images from Training Set (RGB - 128x128 - Batch Size 32)")
2025-04-13 17:53:41.304290: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
No description has been provided for this image
# Import the image dataset from the directory (224x224)
from utilities import load_images_from_directory

# Define the new image size
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions

train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical', color_mode='rgb',
                                                                      class_names=class_names, batch_size=batch_size,
                                                                      image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', 
                                                                      crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")

# Show 5 images from the training set
show_images_from_dataset(train_datagen, num_images=5, title="Sample Images from Training Set (RGB - 224x224 - Batch Size 32)")
Found 9586 files belonging to 202 classes.
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
No description has been provided for this image
# Import the image dataset from the directory (224x224 - crop_to_aspect_ratio=True)
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical', color_mode='rgb', 
                                                                      class_names=class_names, batch_size=batch_size, 
                                                                      image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=True, pad_to_aspect_ratio=False)
print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")

# Show 5 images from the training set
show_images_from_dataset(train_datagen, num_images=5, title="Sample Images from Training Set (RGB - 224x224 - Batch Size 32 - crop_to_aspect_ratio=True)")
Found 9586 files belonging to 202 classes.
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
No description has been provided for this image
# Import the image dataset from the directory (224x224 - pad_to_aspect_ratio=True)
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical', color_mode='rgb', 
                                                                      class_names=class_names, batch_size=batch_size, 
                                                                      image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=True)
print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")

# Show 5 images from the training set
show_images_from_dataset(train_datagen, num_images=5, title="Sample Images from Training Set (RGB - 224x224 - Batch Size 32 - pad_to_aspect_ratio=True)")
Found 9586 files belonging to 202 classes.
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
No description has been provided for this image
Justification for Setting crop_to_aspect_ratio and pad_to_aspect_ratio to False
In our experiments, using crop_to_aspect_ratio=True resulted in resizing images by cropping parts of them to maintain the desired aspect ratio. This approach can lead to significant loss of important details — for example, in our fish images, essential features such as the head and tail were cropped out, potentially impairing model performance.

Conversely, when pad_to_aspect_ratio=True is used, the image is padded with black bars to reach the target aspect ratio. However, this introduces areas with uniform, non-informative pixel values that may bias the model's learning process, since the model considers the colors and patterns of each pixel.

Thus, we opt to set both crop_to_aspect_ratio and pad_to_aspect_ratio to False. This way, the image is simply resized to the specified dimensions (as per image_size), preserving all original content. Although this might introduce some distortion, it is preferable in our case because it avoids both the loss of crucial information and the introduction of artificial artifacts. This approach ensures that all potentially relevant features are retained for the model to learn effectively.


2 | Image Preprocessing & Data Augmentation



# Sample one image filepath from the training set
list(train_datagen.file_paths)[0]
'data/RareSpecies_Split/train/chordata_vespertilionidae/21686384_327547_eol-full-size-copy.jpg'
📷 Image Preprocessing [1]
Transform the images to Grayscale
def image_preprocessing_experiment(image_path='data/RareSpecies_Split/train/chordata_dactyloidae/29482917_453292_eol-full-size-copy.jpg'):
    """Tests key tf.image preprocessing functions on a sample image.
    
    Args:
        image_path (str): Path to the sample image.
    
    Returns:
        None: Displays a grid of transformed images.
    
    Notes:
        - Loads image, resizes to 224x224, rescales to [0, 1].
        - Applies selected tf.image transformations for classification robustness.
        - Plots in a 4xN grid for readability.
    """
    # Load and preprocess the image
    try:
        original_img = Image.open(image_path).convert('RGB')
        img = original_img.resize((224, 224))
        img_array = np.array(img).astype("float32") / 255.0
        img_array = tf.expand_dims(img_array, 0)  # Shape: (1, 224, 224, 3)
    except Exception as e:
        print(f"Error loading image: {e}")
        return

    # Define transformations from tf.image
    transformations = [
        ("Original (With Resize)", lambda x: x[0]),                                             # Unchanged image
        ("Adjust Brightness", lambda x: tf.image.adjust_brightness(x, delta=0.5)),              # Brighten by 0.5
        ("Adjust Contrast", lambda x: tf.image.adjust_contrast(x, contrast_factor=2.0)),        # Increase contrast
        ("Adjust Hue", lambda x: tf.image.adjust_hue(x, delta=0.3)),                            # Shift hue by 0.3
        ("Adjust Saturation", lambda x: tf.image.adjust_saturation(x, saturation_factor=2.0)),  # Double saturation
        ("Central Crop", lambda x: tf.image.central_crop(x, central_fraction=0.8)),             # Crop 80% central area
        ("Flip Left-Right", tf.image.flip_left_right),                                          # Horizontal flip
        ("Flip Up-Down", tf.image.flip_up_down),                                                # Vertical flip
        ("Random Brightness", lambda x: tf.image.random_brightness(x, max_delta=0.5)),          # Random ±0.5 brightness
        ("Random Contrast", lambda x: tf.image.random_contrast(x, lower=0.5, upper=2.0)),       # Random contrast 0.5-2.0
        ("Random Hue", lambda x: tf.image.random_hue(x, max_delta=0.3)),                        # Random ±0.3 hue
        ("Random Saturation", lambda x: tf.image.random_saturation(x, lower=0.5, upper=2.0)),   # Random saturation 0.5-2.0
        ("Random Crop", lambda x: tf.image.random_crop(x, size=[1, 200, 200, 3])),              # Crop to 200x200 randomly
        ("Resize", lambda x: tf.image.resize(x, size=[224, 224], method='bilinear')),           # Redundant but shown
        ("Rotate 90°", lambda x: tf.image.rot90(x, k=1)),                                       # Rotate 90° clockwise
        ("Grayscale", tf.image.rgb_to_grayscale),                                               # Convert to grayscale
        ("Transpose", tf.image.transpose)                                                       # Swap height and width
    ]

    # Apply transformations and store results
    transformed_imgs = []
    for name, transform in transformations:
        try:
            result = transform(img_array)
            # Ensure output is 224x224 (resize back if needed, e.g., after crop)
            if result.shape[1] != 224 or result.shape[2] != 224:
                result = tf.image.resize(result, [224, 224], method='bilinear')
                
            # Handle grayscale (1 channel) by converting back to 3 channels if needed
            if result.shape[-1] == 1:
                result = tf.image.grayscale_to_rgb(result)
                
            transformed_imgs.append((name, tf.squeeze(result).numpy()))
        except Exception as e:
            print(f"Error applying {name}: {e}")
            transformed_imgs.append((name, img_array[0].numpy()))  # Fallback to original

    # Plot in a CxR grid
    n_transforms = len(transformed_imgs)
    n_rows = 3 
    n_cols = (n_transforms + n_rows - 1) // n_rows  # Adjusted for R rows
    fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * 3, n_rows * 3))
    for i, (title, img_data) in enumerate(transformed_imgs):
        row, col = divmod(i, n_cols)
        ax[row, col].imshow(np.clip(img_data, 0, 1))
        ax[row, col].set_title(title, fontsize=10, fontweight='bold')
        ax[row, col].axis('off')
    
    # Hide unused subplots
    for i in range(n_transforms, n_rows * n_cols):
        row, col = divmod(i, n_cols)
        ax[row, col].axis('off')
    
    # Adjust layout and display
    sns.despine(top=True, right=True)
    plt.tight_layout()
    plt.suptitle("Preprocessing and Augmentation Effects (tf.image)", fontsize=16, fontweight='bold', y=1.05)
    plt.show()
# Run the experiment with two example images
print("\nPreprocessing Experiment (Image 1):")
image_preprocessing_experiment()
print("\nPreprocessing Experiment (Image 2):")
image_preprocessing_experiment(image_path='data/RareSpecies_Split/train/chordata_anatidae/14020527_45513542_eol-full-size-copy.jpg')
Preprocessing Experiment (Image 1):
No description has been provided for this image
Preprocessing Experiment (Image 2):
No description has been provided for this image
From the image preprocessing options tested among those available at https://www.tensorflow.org/api_docs/python/tf/image we decided to test the tf.image.rgb_to_grayscale function to convert the images to grayscale, tf.image.adjust_contrast to adjust the contrast, and tf.image.adjust_saturation to adjust the saturation.

NOTE: This decision was based on the idea of distinguishing the object (animal) from the background, which empirically seems to be the preprocessing that most enhances this goal.

🖌️ Data Augmentation [2]
def data_augmentation_experiment(image_path='data/RareSpecies_Split/train/chordata_dactyloidae/29482917_453292_eol-full-size-copy.jpg'):
    """Tests all Keras preprocessing and augmentation layers on a sample image with max effect (factor=1).
    
    Args:
        image_path (str): Path to the sample image.
    
    Returns:
        None: Displays multiple 2xN grids of transformed images.
    
    Notes:
        - Loads image, resizes to 224x224, rescales to [0, 1].
        - Applies all available Keras image preprocessing/augmentation layers.
        - Uses training=True for random layers to ensure application.
        - Splits into multiple 2-row grids for readability.
    """
    # Load and preprocess the image
    try:
        original_img = Image.open(image_path).convert('RGB')        # Ensure RGB format
        img = original_img.resize((224, 224))                       # Resize to 224x224
        img_array = np.array(img).astype("float32") / 255.0         # Rescale to [0, 1]
        img_array = tf.expand_dims(img_array, 0)                    # Add batch dimension
    except Exception as e:
        print(f"Error loading image: {e}")
        return

    # Define all preprocessing and augmentation layers with max effect
    transformations = [
        ("Original", lambda x: x[0]),                                           # Unchanged image
        ("Resized", Resizing(224, 224)),                                        # Resizes to specified dimensions (redundant here but shown)
        ("Rescaled", Rescaling(1./255)),                                        # Scales pixels to [0, 1] (redundant here)
        ("CenterCrop", CenterCrop(200, 200)),                                   # Crops center to 200x200
        ("AutoContrast", AutoContrast()),                                       # Maximizes contrast across the image
        ("Equalization", Equalization()),                                       # Equalizes histogram for contrast enhancement
        ("MixUp", MixUp(alpha=1.0)),                                            # Blends images (simplified for single image)
        ("RandAugment", RandAugment(value_range=(0, 1))),                       # Randomly applies augmentations at max magnitude
        ("Brightness", RandomBrightness(1.0)),                                  # Adjusts brightness by max factor
        ("ColorDegeneration", RandomColorDegeneration(factor=1.0)),             # Degenerates color to grayscale-like
        ("ColorJitter", RandomColorJitter(brightness_factor=1.0, 
                                          contrast_factor=1.0, 
                                          saturation_factor=1.0)),              # Max jitters brightness, contrast, saturation
        ("Contrast", RandomContrast(1.0)),                                      # Max contrast adjustment
        ("Crop", RandomCrop(200, 200)),                                         # Randomly crops to 200x200
        ("Flip", RandomFlip("horizontal_and_vertical")),                        # Flips both horizontally and vertically
        ("Grayscale", RandomGrayscale(factor=1.0)),                             # Converts to grayscale with 100% probability
        ("Hue", RandomHue(factor=0.5)),                                         # Shifts hue by max factor (0.5 is max valid range)
        ("Rotation", RandomRotation(1.0)),                                      # Rotates by up to 360 degrees (factor=1)
        ("Saturation", RandomSaturation(1.0)),                                  # Adjusts saturation by max factor
        ("Sharpness", RandomSharpness(1.0)),                                    # Enhances sharpness by max factor
        ("Shear", RandomShear(0.5)),                                            # Shears by max factor
        ("Translation", RandomTranslation(0.5, 0.5)),                           # Shifts by max horizontal/vertical factors
        ("Zoom", RandomZoom(0.5)),                                              # Zooms by max factor
    ]

    # Apply transformations and store results
    transformed_imgs = [("Original", np.array(original_img).astype("float32") / 255.0)]  # Add the original image first
    for name, layer in transformations:
        try:
            if name == "Original":
                continue                                                # Skip as it's already added
            result = layer(img_array, training=True)                    # Force application
            transformed_imgs.append((name, tf.squeeze(result).numpy()))
        except Exception as e:
            print(f"Error applying {name}: {e}")
            transformed_imgs.append((name, img_array[0].numpy()))  # Fallback to original

    # Plot in a CxR grid    
    n_transforms = len(transformed_imgs)
    n_rows = 3
    n_cols = (n_transforms + n_rows - 1) // n_rows
    fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols * 3, n_rows * 3))
    for i, (title, img_data) in enumerate(transformed_imgs):
        row, col = divmod(i, n_cols)
        ax[row, col].imshow(np.clip(img_data, 0, 1))
        ax[row, col].set_title(title, fontsize=10, fontweight='bold')
        ax[row, col].axis('off')
    
    # Hide unused subplots
    for i in range(n_transforms, n_rows * n_cols):
        row, col = divmod(i, n_cols)
        ax[row, col].axis('off')
        
    # Adjust layout and display
    sns.despine(top=True, right=True)
    plt.tight_layout()
    plt.suptitle(f"Preprocessing and Augmentation Effects", fontsize=16, fontweight='bold', y=1.05)
    plt.show()
# Run the experiment with two example images
print("\nPreprocessing Experiment (Image 1):")
data_augmentation_experiment()
print("\nPreprocessing Experiment (Image 2):")
data_augmentation_experiment(image_path='data/RareSpecies_Split/train/chordata_anatidae/14020527_45513542_eol-full-size-copy.jpg')
Preprocessing Experiment (Image 1):
No description has been provided for this image
Preprocessing Experiment (Image 2):
No description has been provided for this image
After applying Data Augmentation we decided to use this technique as a way of OverSampling our database for minority cases.

Data Imbalance
OverSampling | SMOTE Data Augmentation
Duplicate the minorities classes to balance the dataset. We will use Data Augmentation to create new images from the original ones.

# Applying Random Augmentation to handle class imbalance
# Source Code: https://medium.com/@fatimazahra.belharar/enhancing-classification-accuracy-for-imbalanced-image-data-using-smote-41737783a720
#              https://www.kaggle.com/code/souryadipstan/imbalanced-classes-image-classification-smote
# Step 1: Prepare the Augmented Training Directory

# # Copy the training set folder to a new folder ('train_DataAugmentationSMOTE') - Terminal
# mkdir -p data/RareSpecies_Split/train_DataAugmentationSMOTE
# cp -r data/RareSpecies_Split/train/* data/RareSpecies_Split/train_DataAugmentationSMOTE/
# ls -lh data/RareSpecies_Split/train_DataAugmentationSMOTE/
# Training data generator (With Data Augmentation SMOTE)
from tensorflow.keras.preprocessing import image_dataset_from_directory

train_DataAugmentationSMOTE_dir = Path("data/RareSpecies_Split/train_DataAugmentationSMOTE")

# Get class names from directory (train_DataAugmentationSMOTE)
class_names = sorted(os.listdir(train_DataAugmentationSMOTE_dir))
class_indices = {name: i for i, name in enumerate(class_names)}


train_datagen = image_dataset_from_directory(train_DataAugmentationSMOTE_dir, labels='inferred', label_mode='categorical', class_names=class_names, 
                                             color_mode='rgb', batch_size=batch_size, image_size=image_size, shuffle=False, seed=2025,  
                                             interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)
print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.

Loaded: Train (9600)
# Step 2: Calculate Class Frequencies
class_counts = {}
for class_name in os.listdir(train_DataAugmentationSMOTE_dir):      # Count in the new dir
    class_path = train_DataAugmentationSMOTE_dir / class_name
    if class_path.is_dir():
        class_counts[class_name] = len(list(class_path.glob('*')))  # Count files

# Determine the target number of images per class (We decide to use the size of the largest class)
target_count = max(class_counts.values())
print(f"Target number of images per class (max count): {target_count}")

# Identify minority classes
minority_classes = {
    cls: count for cls, count in class_counts.items() if count < target_count
}
print(f"Found {len(minority_classes)} minority classes to augment.")
Target number of images per class (max count): 240
Found 200 minority classes to augment.
# Step 3: Define Data Augmentation Pipeline
# Define a sequential model for applying augmentations
data_augmentation = Sequential([
    # Apply augmentations suitable for potentially preserving species features
    RandAugment(value_range=(0, 1))                    # Randomly apply augmentations
], name="data_augmentation")
# Step 3.1: Test the Data Augmentation Pipeline
# Test the augmentation pipeline on a sample image
def test_augmentation_pipeline(image_path='data/RareSpecies_Split/train/chordata_dactyloidae/29482917_453292_eol-full-size-copy.jpg'):
    """Tests the data augmentation pipeline on a sample image.
    
    Args:
        image_path (str): Path to the sample image.
    
    Returns:
        None: Displays the original and augmented images.
    
    Notes:
        - Loads image, resizes to 224x224, rescales to [0, 1].
        - Applies the defined data augmentation pipeline.
        - Displays original and augmented images side by side.
    """
    # Load and preprocess the image
    try:
        original_img = Image.open(image_path).convert('RGB')
        img = original_img.resize((224, 224))
        img_array = np.array(img).astype("float32") / 255.0
        img_array = tf.expand_dims(img_array, 0)  # Shape: (1, 224, 224, 3)
    except Exception as e:
        print(f"Error loading image: {e}")
        return

    # Apply the data augmentation pipeline
    augmented_img = data_augmentation(img_array, training=True)

    # Plot original and augmented images
    fig, ax = plt.subplots(1, 2, figsize=(8, 4))
    ax[0].imshow(np.clip(img_array[0], 0, 1))
    ax[0].set_title("Original Image", fontsize=12)
    ax[0].axis('off')

    ax[1].imshow(np.clip(augmented_img[0], 0, 1))
    ax[1].set_title("Augmented Image", fontsize=12)
    ax[1].axis('off')

    plt.tight_layout()
    plt.suptitle("Data Augmentation Pipeline Test", fontsize=16, fontweight='bold', y=1.05)
    plt.show()
# Run the test with two example images
print("\nData Augmentation Pipeline Test (Image 1):")
test_augmentation_pipeline()
print("\nData Augmentation Pipeline Test (Image 2):")
test_augmentation_pipeline(image_path='data/RareSpecies_Split/train/chordata_anatidae/14020527_45513542_eol-full-size-copy.jpg')
Data Augmentation Pipeline Test (Image 1):
No description has been provided for this image
Data Augmentation Pipeline Test (Image 2):
No description has been provided for this image
# Step 4: Generate and Save Augmented Images for Minority Classes
# Loop through each minority class
for class_name, current_count in tqdm(minority_classes.items(), desc="Augmenting Classes"):
    
    # Calculate the number of images to generate
    num_to_generate = target_count - current_count
    
    # Verify if the number is greater than zero
    if num_to_generate <= 0:
        continue                                        # Should not happen based on filter, but safe check
    
    # Create a directory for the augmented images of this class
    class_path = train_DataAugmentationSMOTE_dir / class_name       # Path to the class directory
    original_image_paths = list(class_path.glob('*'))               # Get list of original images for this class

    # Check if original images exist (Debugging)
    if not original_image_paths:
        print(f"Warning: No original images found for class {class_name}. Skipping.")
        continue
    
    # Generate the required number of augmented images
    for i in range(num_to_generate):
        # Randomly select an original image from this class
        original_image_path = random.choice(original_image_paths)

        try:
            # Load the original image
            img = load_img(original_image_path, target_size=image_size)
            img_array = img_to_array(img)            # Convert to numpy array
            img_array = img_array / 255.0            # Rescale to [0, 1] (important because augmentation layers expect this range)
            img_array = tf.expand_dims(img_array, 0) # Add batch dimension (1, H, W, C)

            # Apply the Augmentation Pipeline
            # Source: https://stackoverflow.com/questions/71455053/data-augmentation-layer-in-keras-sequential-model
            # Note: training=True ensures that random layers are activated
            augmented_img_array = data_augmentation(img_array, training=True)

            # Remove batch dimension and ensure correct data type/range for saving
            # Source: https://www.tensorflow.org/api_docs/python/tf/squeeze
            augmented_img_squeezed = tf.squeeze(augmented_img_array, axis=0).numpy()
            
            # Clip values to be safe, though rescaling usually handles this
            # Source: https://numpy.org/doc/stable/reference/generated/numpy.clip.html
            augmented_img_clipped = np.clip(augmented_img_squeezed * 255.0, 0, 255).astype(np.uint8)

            # Create a unique filename for the augmented image
            # Source: https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem
            #         https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes
            #         https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior
            original_stem = original_image_path.stem # Filename without extension
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S%f")          # High precision timestamp
            augmented_filename = f"{original_stem}_aug_{i+1}_{timestamp}.jpg"       # Added counter and timestamp
            save_path = class_path / augmented_filename

            # Save the augmented image
            # Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/save_img
            save_img(save_path, augmented_img_clipped)

        except Exception as e:
            print(f"\nError processing {original_image_path} for class {class_name}: {e}")
            
## Time to Execute the "SMOTE" Augmentation = 29min 7s
##                                           Augmenting Classes: 100%|██████████| 200/200 [29:07<00:00,  8.74s/it]
Augmenting Classes: 100%|██████████| 200/200 [29:07<00:00,  8.74s/it]
# Verify New Counts of Augmented Images
# Source: https://docs.python.org/3/library/os.html#os.listdir
#         https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.glob
final_class_counts = {}
for class_name in os.listdir(train_DataAugmentationSMOTE_dir):
    class_path = train_DataAugmentationSMOTE_dir / class_name
    if class_path.is_dir():
        final_class_counts[class_name] = len(list(class_path.glob('*')))
     
# Display 10 classes with their counts
print("\n\033[1mSample final counts:\033[0m")
for i, (cls, count) in enumerate(final_class_counts.items()):
    print(f"- {cls}: {count}")
    
    # Print first 10
    if i >= 9: 
        break
Sample final counts:
- chordata_nesospingidae: 240
- mollusca_cardiidae: 240
- chordata_callitrichidae: 240
- chordata_glareolidae: 240
- arthropoda_lucanidae: 240
- chordata_elapidae: 240
- chordata_mimidae: 240
- chordata_indriidae: 240
- chordata_cetorhinidae: 240
- chordata_cyprinodontidae: 240
# Check if all classes now have the target count
all_balanced = all(count == target_count for count in final_class_counts.values())
print(f"\033[1mAll classes reached target count ({target_count})?\033[0m {'Yes' if all_balanced else 'No'}")
if not all_balanced:
     print("Some classes might not have reached the target count due to errors or empty original directories.")
All classes reached target count (240)? Yes
# # Zip the RareSpecies_Split dataset (with Data Augmentation SMOTE)
# cd data
# zip -r RareSpecies_Split.zip ./RareSpecies_Split/
Weights
# Count Samples in Each Class in the Original Training Set
print(f"Counting samples in original train directory: {train_dir}")
original_class_counts = {}
total_samples = 0
for class_name in class_names:
    class_path = train_dir / class_name
    if class_path.is_dir():
        count = len(list(class_path.glob('*'))) # Count image files
        original_class_counts[class_name] = count
        total_samples += count
    else:
        print(f"Warning: Expected directory not found: {class_path}")

print(f"Total original training samples: {total_samples}")

# Randomly sample 5 items from the dictionary
sampled_class_counts = dict(random.sample(original_class_counts.items(), 5))
print(f"Sample of Original class counts: {sampled_class_counts}")
Counting samples in original train directory: data/RareSpecies_Split/train
Total original training samples: 9586
Sample of Original class counts: {'chordata_otariidae': 48, 'chordata_delphinidae': 96, 'chordata_geoemydidae': 72, 'cnidaria_acroporidae': 168, 'chordata_aotidae': 48}
# Calculate Class Weights
# Formula: weight_for_class_i = (total_samples / (num_classes * samples_in_class_i))  -> This gives higher weight to smaller classes.
class_weights = {}
for class_name, count in original_class_counts.items():
    if count == 0:
        print(f"Warning: Class '{class_name}' has 0 samples. Assigning weight 0.")
        weight = 0.0
    else:
        weight = (total_samples) / (len(class_names) * count)

    # Get the index for this class name
    class_index = class_indices[class_name]
    class_weights[class_index] = weight

# Print weights for the first few classes as an example
print("\n\033[1mClass Weights:\033[0m")
for i in range(min(10, len(class_names))):
    class_name = class_names[i]
    count = original_class_counts.get(class_name, 0)
    print(f"- Class '{class_name:23}' (Index {i}): Count={count:3}, Weight={class_weights[i]:.4f}")
Class Weights:
- Class 'arthropoda_apidae      ' (Index 0): Count=120, Weight=0.3955
- Class 'arthropoda_attelabidae ' (Index 1): Count= 24, Weight=1.9773
- Class 'arthropoda_carabidae   ' (Index 2): Count= 48, Weight=0.9887
- Class 'arthropoda_cerambycidae' (Index 3): Count= 24, Weight=1.9773
- Class 'arthropoda_coenagrionidae' (Index 4): Count= 24, Weight=1.9773
- Class 'arthropoda_formicidae  ' (Index 5): Count=233, Weight=0.2037
- Class 'arthropoda_gomphidae   ' (Index 6): Count= 48, Weight=0.9887
- Class 'arthropoda_lucanidae   ' (Index 7): Count= 24, Weight=1.9773
- Class 'arthropoda_nymphalidae ' (Index 8): Count= 24, Weight=1.9773
- Class 'arthropoda_palinuridae ' (Index 9): Count= 24, Weight=1.9773
🔗 Bibliography/References
[1] TensorFlow API Documentation - Image Module (2024). TensorFlow. Retrieved from TensorFlow

[2] TensorFlow API Documentation - Data Augmentation (2024). TensorFlow. Retrieved from TensorFlow


======================================================================================== 3_Modeling_Baseline Model_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# !pip install visualkeras
# System imports
import os
import sys
import time
import datetime
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import Model, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
from tensorflow.keras import regularizers                                                                           # For L2 regularization
import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
TensorFlow Version: 2.19.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744817229.257450    1457 gpu_device.cc:2019] Created device /device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
TensorFlow version: 2.19.0
Python version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]
CUDA version: 12.5.1
cuDNN version: 9
# Extra: https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth
# If you’re using a GPU, TensorFlow might pre-allocate GPU memory, leaving less for CPU operations. 
# Enabling memory growth lets the GPU allocate only what’s needed.
if tf.test.is_built_with_cuda():
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size
input_shape = (img_height, img_width, 3)            # Input shape of the model
value_range = (0.0, 1.0)                            # Range of pixel values
# Get class names from directory
class_names = sorted(os.listdir(train_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.
I0000 00:00:1744817230.600542    1457 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)
Train batch shape: (64, 224, 224, 3) (64, 202)
2025-04-16 16:27:12.499745: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Val batch shape: (64, 224, 224, 3) (64, 202)
2025-04-16 16:27:12.755892: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Test batch shape: (64, 224, 224, 3) (64, 202)

3 | Modeling - Baseline Model



💡 Modeling
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/3_BaselineModel_10epochs", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/3_BaselineModel_10epochs", exist_ok=True)

os.makedirs("./ModelCallbacks/3_BaselineModel", exist_ok=True)
os.makedirs("./ModelsEvaluation/3_BaselineModel", exist_ok=True)
# Baseline Model
class RareSpeciesCNN(Model):
    """Custom CNN for rare species classification.
    
    Architecture: Simple CNN 
    Why: Small model to establish baseline, avoiding overfitting on 202 classes.
    Alternatives: Deeper CNNs (e.g., ResNet) or transfer learning (e.g., EfficientNet).
    Allows selection of preprocessing steps like grayscale, contrast, and saturation adjustment.
    """
    def __init__(self, n_classes=202,                                   # Number of output classes
                 apply_grayscale=False,                                 # If True, convert images to grayscale
                 apply_contrast=False, contrast_factor=1.5,             # 1.5 = increase contrast
                 apply_saturation=False, saturation_factor=1.5):        # 1.5 = increase saturation
        """Initializes the model.
        
        Args:
            n_classes (int): Number of output classes.
            apply_grayscale (bool): If True, convert images to grayscale first.
            apply_contrast (bool): If True, adjust image contrast.
            contrast_factor (float): Factor for contrast adjustment.
            apply_saturation (bool): If True, adjust image saturation.
            saturation_factor (float): Factor for saturation adjustment.
        """
        super().__init__() # Call the parent class constructor
        
        # Store preprocessing flags and factors
        self.apply_grayscale = apply_grayscale
        self.apply_contrast = apply_contrast
        self.apply_saturation = apply_saturation
        
        # --- Preprocessing Layers ---        
        # Rescaling layer (always applied)
        self.rescale_layer = Rescaling(scale= 1 / 255.0, name="Rescale_Layer")    # Rescales pixel values to [0, 1]
        
        # Conditionally define Lambda layer for contrast adjustment
        if self.apply_contrast:
            # Define Lambda layer for contrast adjustment
            # Source: https://keras.io/api/layers/core_layers/lambda/
            #         https://www.tensorflow.org/api_docs/python/tf/image/adjust_contrast
            #         contrast_factor > 1 increases contrast, < 1 decreases contrast
            self.contrast_layer = Lambda(
                lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor),
                name='Adjust_Contrast'
            )
        
        # Conditionally define Lambda layer for saturation adjustment
        if self.apply_saturation:
            # Define Lambda layer for saturation adjustment
            # Source: https://www.tensorflow.org/api_docs/python/tf/image/adjust_saturation
            #         saturation_factor > 1 increases saturation, < 1 decreases saturation
            self.saturation_layer = Lambda(
                lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor),
                name='Adjust_Saturation'
            )
            
        # Conditionally define Lambda layer for grayscale conversion
        if self.apply_grayscale:
            # Define Lambda layer for grayscale conversion
            # Source: https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_grayscale
            self.grayscale_layer = Lambda(
                lambda x: tf.image.rgb_to_grayscale(x), 
                name='RGB_to_Grayscale'
            )
            # IMPORTANT: Add a Conv2D layer immediately after grayscale to ensure 
            #            the number of channels is compatible with subsequent layers 
            #            if they expect 3 channels. Here, we'll keep it 1 channel and adjust conv1.
            # Alternatively, convert grayscale back to 3 identical channels:
            # self.grayscale_to_rgb_layer = Lambda(
            #     lambda x: tf.image.grayscale_to_rgb(x),
            #     name='Grayscale_to_RGB'
            # )
            
            
        # --- Convolutional Layers ---
        # Adjust the first Conv layer's input channels if grayscale is applied and not converted back to RGB
        # If grayscale IS applied, the input to conv1 will have 1 channel.
        # If grayscale IS NOT applied, the input will have 3 channels (after rescaling).
        # We will handle this by checking the shape dynamically or assuming subsequent layers can handle 1 channel if needed.
        # For simplicity here, let's assume conv1 works with either 1 or 3 channels.
        # If grayscale is applied, the input depth is 1, otherwise 3.
        # A more robust way might involve explicitly setting input_shape or checking channels.
        # Let's define conv1 to work even if input is grayscale (1 channel)

        # Block 1
        # Source: https://stackoverflow.com/questions/60157742/convolutional-neural-network-cnn-input-shape/61075207#61075207 (Explain Conv2D)
        self.conv1 = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001), name="Conv_Block1_Conv")
        self.bn1 = BatchNormalization(name="Conv_Block1_BN")
        self.act1 = Activation('relu', name="Conv_Block1_Act")
        self.pool1 = MaxPooling2D(pool_size=(2, 2), name="Conv_Block1_Pool") # Output: 112x112x32

        # Block 2
        self.conv2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same',kernel_regularizer=regularizers.l2(0.001), name="Conv_Block2_Conv")
        self.bn2 = BatchNormalization(name="Conv_Block2_BN")
        self.act2 = Activation('relu', name="Conv_Block2_Act")
        self.pool2 = MaxPooling2D(pool_size=(2, 2), name="Conv_Block2_Pool") # Output: 56x56x64

        # Block 3
        self.conv3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same',kernel_regularizer=regularizers.l2(0.001), name="Conv_Block3_Conv")
        self.bn3 = BatchNormalization(name="Conv_Block3_BN")
        self.act3 = Activation('relu', name="Conv_Block3_Act")
        self.pool3 = MaxPooling2D(pool_size=(2, 2), name="Conv_Block3_Pool") # Output: 28x28x128

        ### Block 4
        self.conv4 = Conv2D(filters=256, kernel_size=(3, 3), padding='same',kernel_regularizer=regularizers.l2(0.001), name="Conv_Block4_Conv")
        self.bn4 = BatchNormalization(name="Conv_Block4_BN")
        self.act4 = Activation('relu', name="Conv_Block4_Act")
        self.pool4 = MaxPooling2D(pool_size=(2, 2), name="Conv_Block4_Pool") # Output: 14x14x256

        # --- Classification Head ---
        self.global_avg_pool = GlobalAveragePooling2D(name="Global_Average_Pooling")
        self.dense1 = Dense(128, name="Dense_Layer1")                                     # Smaller intermediate dense layer
        self.bn_dense1 = BatchNormalization(name="Dense_Layer1_BN")
        self.act_dense1 = Activation('relu', name="Dense_Layer1_Act")
        self.dropout = Dropout(0.5, name="Dropout_Layer")
        self.dense_output = Dense(n_classes, activation='softmax', name="Output_Layer")


    def call(self: Self, inputs: Any, training:bool=False) -> Any:
        """Defines the forward pass of the model.
        
        Args:
            inputs: Input tensor (batch of images).
            training (bool): Indicates if the model is in training mode (for Dropout).
            
        Returns:
            Output tensor (probabilities for each class).
        """
        # Apply mandatory rescaling
        x = self.rescale_layer(inputs)

        # Apply conditional preprocessing layers
        if self.apply_contrast:
            x = self.contrast_layer(x)
        if self.apply_saturation:
            x = self.saturation_layer(x)
        if self.apply_grayscale:
            x = self.grayscale_layer(x)
            # If subsequent layers strictly require 3 channels, uncomment this:
            # x = self.grayscale_to_rgb_layer(x)
            # Note: If grayscale is applied, conv1 will process a 1-channel input unless converted back.

        # Conv Block 1
        x = self.conv1(x)
        x = self.bn1(x, training=training)
        x = self.act1(x)
        x = self.pool1(x)

        # Conv Block 2
        x = self.conv2(x)
        x = self.bn2(x, training=training)
        x = self.act2(x)
        x = self.pool2(x)

        # Conv Block 3
        x = self.conv3(x)
        x = self.bn3(x, training=training)
        x = self.act3(x)
        x = self.pool3(x)

        # Conv Block 4
        x = self.conv4(x)
        x = self.bn4(x, training=training)
        x = self.act4(x)
        x = self.pool4(x)

        # Classification Head
        x = self.global_avg_pool(x)
        x = self.dense1(x)
        x = self.bn_dense1(x, training=training)
        x = self.act_dense1(x)
        x = self.dropout(x, training=training)
        outputs = self.dense_output(x)

        return outputs

# Example Instantiation and Summary
model = RareSpeciesCNN(
    n_classes=n_classes, 
    apply_grayscale=False, 
    apply_contrast=False,                         
    apply_saturation=False,
)

# Build the model by providing an input shape
inputs = Input(shape=(img_width, img_height, 3))        # Input shape
_ = model.call(inputs)                                  # Call the model to build it
model.summary()                                         # Print the model summary
Model: "rare_species_cnn"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ Rescale_Layer (Rescaling)       │ (None, 224, 224, 3)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block1_Conv (Conv2D)       │ (None, 224, 224, 32)   │           896 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block1_BN                  │ (None, 224, 224, 32)   │           128 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block1_Act (Activation)    │ (None, 224, 224, 32)   │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block1_Pool (MaxPooling2D) │ (None, 112, 112, 32)   │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block2_Conv (Conv2D)       │ (None, 112, 112, 64)   │        18,496 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block2_BN                  │ (None, 112, 112, 64)   │           256 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block2_Act (Activation)    │ (None, 112, 112, 64)   │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block2_Pool (MaxPooling2D) │ (None, 56, 56, 64)     │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block3_Conv (Conv2D)       │ (None, 56, 56, 128)    │        73,856 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block3_BN                  │ (None, 56, 56, 128)    │           512 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block3_Act (Activation)    │ (None, 56, 56, 128)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block3_Pool (MaxPooling2D) │ (None, 28, 28, 128)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block4_Conv (Conv2D)       │ (None, 28, 28, 256)    │       295,168 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block4_BN                  │ (None, 28, 28, 256)    │         1,024 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block4_Act (Activation)    │ (None, 28, 28, 256)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Conv_Block4_Pool (MaxPooling2D) │ (None, 14, 14, 256)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Global_Average_Pooling          │ (None, 256)            │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1 (Dense)            │ (None, 128)            │        32,896 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1_BN                 │ (None, 128)            │           512 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1_Act (Activation)   │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dropout_Layer (Dropout)         │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Output_Layer (Dense)            │ (None, 202)            │        26,058 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 449,802 (1.72 MB)
 Trainable params: 448,586 (1.71 MB)
 Non-trainable params: 1,216 (4.75 KB)
# Visualize the model architecture
# Source: https://www.kaggle.com/code/devsubhash/visualize-deep-learning-models-using-visualkeras
visualkeras.layered_view(model,
                         legend=True,
                         show_dimension=True,
                         scale_xy=1,                                        # Adjust the scale of the image
                         # scale_z=1,
                         # to_file='./BaselineModel_Architecture.png',
)
No description has been provided for this image
# Compile model
# optimizer = SGD(learning_rate=0.1, weight_decay=0.01, name="Optimizer")                                         # SGD with decay for stability
optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name="Optimizer")                                        # Adam for faster convergence

loss = CategoricalCrossentropy(name="Loss")                                                                       # Suitable for multi-class one-hot labels
metrics = [CategoricalAccuracy(name="accuracy"), 
           Precision(name="precision"),
           Recall(name="recall"), 
           F1Score(average="macro", name="f1_score"),
           AUC(name="auc")]
model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
# Create a directory for saving the model and logs
model_name = f"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Original"                                             # Model name 
print(f"\n\033[1mModel name:\033[0m {model_name}")
Model name: RareSpeciesCNN_20250415_Original
# Callbacks
# Create a directory for saving the model and logs
model_name = f"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Original"                                              # Model name 
callbacks = [
    ModelCheckpoint(f"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras", monitor="val_loss", save_best_only=True, verbose=0),       # Save best model
    CSVLogger(f"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv"),                                                                      # Log training metrics
    LearningRateScheduler(lambda epoch, lr: lr * 0.95),                                                                           # Exponential decay for learning rate
    EarlyStopping(monitor='val_loss', patience=3, verbose=1)                                                                      # Stop training when the validation loss stops improving
]
Note: All the following models will be trained only with 10 epochs. This is to test different image preprocessing combinations. The best 5 combinations will be trained with máx 100 epochs (until convergence).

Original Dataset | Grayscale=F | Contrast=F | Saturation=F
# Train model
start_time = time.time()
history = model.fit(train_datagen, batch_size = batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)
train_time = round(time.time() - start_time, 2)

print(f"\nTraining completed in \033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\033[0m).")
Epoch 1/10
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1744722820.259686    3243 service.cc:152] XLA service 0x7f0e80017d50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1744722820.259759    3243 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6
2025-04-15 14:13:40.835384: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1744722821.304696    3243 cuda_dnn.cc:529] Loaded cuDNN version 90300
2025-04-15 14:13:43.534152: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 40 bytes spill stores, 40 bytes spill loads

2025-04-15 14:13:43.959063: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 496 bytes spill stores, 500 bytes spill loads

2025-04-15 14:13:44.205052: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 232 bytes spill stores, 232 bytes spill loads

2025-04-15 14:13:44.324032: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 280 bytes spill stores, 280 bytes spill loads

2025-04-15 14:13:45.036819: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 284 bytes spill stores, 284 bytes spill loads

2025-04-15 14:13:45.280615: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 956 bytes spill stores, 924 bytes spill loads

2025-04-15 14:13:45.344172: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 276 bytes spill stores, 276 bytes spill loads

2025-04-15 14:13:46.601569: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 268 bytes spill stores, 268 bytes spill loads

2025-04-15 14:13:46.738549: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 408 bytes spill stores, 408 bytes spill loads

2025-04-15 14:13:46.848426: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 228 bytes spill stores, 228 bytes spill loads

2025-04-15 14:13:46.961862: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 200 bytes spill stores, 200 bytes spill loads

2025-04-15 14:13:46.993398: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 16 bytes spill stores, 16 bytes spill loads

I0000 00:00:1744722844.639330    3243 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
149/150 ━━━━━━━━━━━━━━━━━━━━ 0s 156ms/step - accuracy: 0.0406 - auc: 0.6101 - f1_score: 0.0066 - loss: 5.5474 - precision: 0.0783 - recall: 2.7204e-04
2025-04-15 14:14:30.117308: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 280 bytes spill stores, 280 bytes spill loads

2025-04-15 14:14:30.260653: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1597', 24 bytes spill stores, 24 bytes spill loads

2025-04-15 14:14:30.350542: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 88 bytes spill stores, 88 bytes spill loads

2025-04-15 14:14:30.717691: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1597', 8 bytes spill stores, 8 bytes spill loads

2025-04-15 14:14:31.320326: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 536 bytes spill stores, 500 bytes spill loads

2025-04-15 14:14:31.748686: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 1004 bytes spill stores, 964 bytes spill loads

2025-04-15 14:14:32.191339: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 492 bytes spill stores, 492 bytes spill loads

2025-04-15 14:14:32.194847: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 44 bytes spill stores, 44 bytes spill loads

2025-04-15 14:14:33.150282: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2244', 36 bytes spill stores, 36 bytes spill loads

2025-04-15 14:14:33.671475: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 20 bytes spill stores, 20 bytes spill loads

2025-04-15 14:14:33.672091: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 4 bytes spill stores, 4 bytes spill loads

2025-04-15 14:14:33.741481: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 432 bytes spill stores, 432 bytes spill loads

2025-04-15 14:14:33.851200: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 204 bytes spill stores, 204 bytes spill loads

2025-04-15 14:14:34.021846: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 412 bytes spill stores, 412 bytes spill loads

2025-04-15 14:14:34.218967: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 120 bytes spill stores, 120 bytes spill loads

2025-04-15 14:14:34.251819: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 408 bytes spill stores, 408 bytes spill loads

2025-04-15 14:14:34.285159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 516 bytes spill stores, 516 bytes spill loads

2025-04-15 14:14:34.302354: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2707', 488 bytes spill stores, 488 bytes spill loads

2025-04-15 14:14:34.465976: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2690', 444 bytes spill stores, 444 bytes spill loads

150/150 ━━━━━━━━━━━━━━━━━━━━ 0s 277ms/step - accuracy: 0.0406 - auc: 0.6104 - f1_score: 0.0066 - loss: 5.5462 - precision: 0.0786 - recall: 2.7371e-04
2025-04-15 14:14:53.259241: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_325', 32 bytes spill stores, 32 bytes spill loads

2025-04-15 14:14:53.260572: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_307', 24 bytes spill stores, 24 bytes spill loads

2025-04-15 14:14:53.877041: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_325', 16 bytes spill stores, 16 bytes spill loads

2025-04-15 14:14:54.312895: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_307', 8 bytes spill stores, 8 bytes spill loads

150/150 ━━━━━━━━━━━━━━━━━━━━ 81s 352ms/step - accuracy: 0.0407 - auc: 0.6107 - f1_score: 0.0066 - loss: 5.5450 - precision: 0.0790 - recall: 2.7535e-04 - val_accuracy: 0.0259 - val_auc: 0.6558 - val_f1_score: 0.0023 - val_loss: 5.3851 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 9.5000e-04
Epoch 2/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 26s 154ms/step - accuracy: 0.0805 - auc: 0.7302 - f1_score: 0.0117 - loss: 4.9760 - precision: 0.2666 - recall: 0.0017 - val_accuracy: 0.0267 - val_auc: 0.6633 - val_f1_score: 0.0018 - val_loss: 5.3369 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 9.0250e-04
Epoch 3/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 27s 177ms/step - accuracy: 0.0906 - auc: 0.7576 - f1_score: 0.0168 - loss: 4.7973 - precision: 0.3150 - recall: 0.0031 - val_accuracy: 0.0634 - val_auc: 0.7303 - val_f1_score: 0.0065 - val_loss: 4.8941 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 8.5737e-04
Epoch 4/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 27s 180ms/step - accuracy: 0.1009 - auc: 0.7751 - f1_score: 0.0232 - loss: 4.6361 - precision: 0.4714 - recall: 0.0048 - val_accuracy: 0.1119 - val_auc: 0.7719 - val_f1_score: 0.0215 - val_loss: 4.6342 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 8.1451e-04
Epoch 5/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 24s 161ms/step - accuracy: 0.1093 - auc: 0.7905 - f1_score: 0.0265 - loss: 4.5169 - precision: 0.5277 - recall: 0.0065 - val_accuracy: 0.1152 - val_auc: 0.7913 - val_f1_score: 0.0300 - val_loss: 4.4756 - val_precision: 0.7778 - val_recall: 0.0117 - learning_rate: 7.7378e-04
Epoch 6/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 28s 184ms/step - accuracy: 0.1221 - auc: 0.8074 - f1_score: 0.0349 - loss: 4.3955 - precision: 0.7612 - recall: 0.0137 - val_accuracy: 0.1169 - val_auc: 0.8036 - val_f1_score: 0.0243 - val_loss: 4.3959 - val_precision: 0.6923 - val_recall: 0.0075 - learning_rate: 7.3509e-04
Epoch 7/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 28s 183ms/step - accuracy: 0.1241 - auc: 0.8167 - f1_score: 0.0380 - loss: 4.3190 - precision: 0.7893 - recall: 0.0143 - val_accuracy: 0.1319 - val_auc: 0.7900 - val_f1_score: 0.0393 - val_loss: 4.4481 - val_precision: 1.0000 - val_recall: 0.0067 - learning_rate: 6.9834e-04
Epoch 8/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 27s 178ms/step - accuracy: 0.1305 - auc: 0.8241 - f1_score: 0.0429 - loss: 4.2442 - precision: 0.8085 - recall: 0.0216 - val_accuracy: 0.1411 - val_auc: 0.7844 - val_f1_score: 0.0420 - val_loss: 4.4773 - val_precision: 0.7778 - val_recall: 0.0058 - learning_rate: 6.6342e-04
Epoch 9/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 33s 217ms/step - accuracy: 0.1434 - auc: 0.8267 - f1_score: 0.0492 - loss: 4.1923 - precision: 0.7659 - recall: 0.0245 - val_accuracy: 0.1636 - val_auc: 0.8115 - val_f1_score: 0.0594 - val_loss: 4.2690 - val_precision: 0.8889 - val_recall: 0.0067 - learning_rate: 6.3025e-04
Epoch 10/10
150/150 ━━━━━━━━━━━━━━━━━━━━ 33s 216ms/step - accuracy: 0.1563 - auc: 0.8362 - f1_score: 0.0618 - loss: 4.1218 - precision: 0.8379 - recall: 0.0271 - val_accuracy: 0.1344 - val_auc: 0.8116 - val_f1_score: 0.0453 - val_loss: 4.3064 - val_precision: 0.7333 - val_recall: 0.0092 - learning_rate: 5.9874e-04

Training completed in 332.99 seconds (0:05:32.990000 h)).
🧪 Model Selection & 📏 Model Evaluation
# Evaluate model
from utilities import plot_metrics
plot_metrics(history, file_path=f"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png")
No description has been provided for this image
# Evaluate on validation and test sets
train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}
val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)
test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)
19/19 ━━━━━━━━━━━━━━━━━━━━ 2s 123ms/step - accuracy: 0.1819 - auc: 0.8354 - f1_score: 0.0282 - loss: 4.0895 - precision: 0.5518 - recall: 0.0087
18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 122ms/step - accuracy: 0.1946 - auc: 0.8377 - f1_score: 0.0267 - loss: 4.0343 - precision: 0.7297 - recall: 0.0115
2025-04-15 14:19:16.905921: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_307', 8 bytes spill stores, 8 bytes spill loads

2025-04-15 14:19:17.094808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_325', 16 bytes spill stores, 16 bytes spill loads

2025-04-15 14:19:17.668843: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_307', 24 bytes spill stores, 24 bytes spill loads

2025-04-15 14:19:17.885224: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_325', 32 bytes spill stores, 32 bytes spill loads

2025-04-15 14:19:23.229825: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=2,k4=1,k5=0,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.14 = (f32[47,128,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[47,64,56,56]{3,2,1,0} %bitcast.1785, f32[128,64,3,3]{3,2,1,0} %bitcast.1792, f32[128]{0} %bitcast.1794), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="rare_species_cnn_1/Conv_Block3_Conv_1/convolution" source_file="/home/diogo/projects/tf218/tf218/lib/python3.12/site-packages/tensorflow/python/framework/ops.py" source_line=1200}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
2025-04-15 14:19:23.236311: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.96277301s
Trying algorithm eng20{k2=2,k4=1,k5=0,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.14 = (f32[47,128,56,56]{3,2,1,0}, u8[0]{0}) custom-call(f32[47,64,56,56]{3,2,1,0} %bitcast.1785, f32[128,64,3,3]{3,2,1,0} %bitcast.1792, f32[128]{0} %bitcast.1794), window={size=3x3 pad=1_1x1_1}, dim_labels=bf01_oi01->bf01, custom_call_target="__cudnn$convBiasActivationForward", metadata={op_type="Conv2D" op_name="rare_species_cnn_1/Conv_Block3_Conv_1/convolution" source_file="/home/diogo/projects/tf218/tf218/lib/python3.12/site-packages/tensorflow/python/framework/ops.py" source_line=1200}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"cudnn_conv_backend_config":{"conv_result_scale":1,"activation_mode":"kNone","side_input_scale":0,"leakyrelu_alpha":0},"force_earliest_schedule":false} is taking a while...
19/19 ━━━━━━━━━━━━━━━━━━━━ 13s 690ms/step - accuracy: 0.1883 - auc: 0.8342 - f1_score: 0.0282 - loss: 4.0641 - precision: 0.7391 - recall: 0.0115
# Display results
from utilities import display_side_by_side, create_evaluation_dataframe
results_df = create_evaluation_dataframe(
    model_name="Baseline Model",
    variation="Original | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001",           # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate
    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,
    csv_save_path= f"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv"      # Save the results to a CSV file
)
display_side_by_side(results_df, super_title="Model Evaluation Results")
Model Evaluation Results

Train	Validation	Test
Time of Execution	Accuracy	Precision	Recall	F1 Score	AUROC	Accuracy	Precision	Recall	F1 Score	AUROC	Accuracy	Precision	Recall	F1 Score	AUROC
Model	Variation																
Baseline Model	Original | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001	332.99	0.1487	0.7743	0.0258	0.0602	0.8334	0.1344	0.7333	0.0092	0.0453	0.8116	0.1309	0.8235	0.0117	0.0414	0.8023
Original Dataset | Grayscale=T | Contrast=F | Saturation=F

[...] - ALL RESULTAS ARE IN THE TABLE OF APPENDIX C

======================================================================================== 4_Modeling_VGG-19_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.ops import add
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
from tensorflow.keras import regularizers                                                                           # For L2 regularization
#import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Other imports
from itertools import product

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
2025-04-18 13:05:44.440343: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-18 13:05:44.705593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744977944.806451    1709 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744977944.835530    1709 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-18 13:05:45.093080: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Creates a SSL context that does not verify the server’s certificate - Needed for downloading pretrained models
# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
TensorFlow Version: 2.18.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744977948.576422    1709 gpu_device.cc:2022] Created device /device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
TensorFlow version: 2.18.0
Python version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]
CUDA version: 12.5.1
cuDNN version: 9
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size
input_shape = (img_height, img_width, 3)            # Input shape of the model
# Get class names from directory
class_names = sorted(os.listdir(train_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.
I0000 00:00:1744977949.401883    1709 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)
Train batch shape: (64, 224, 224, 3) (64, 202)
Val batch shape: (64, 224, 224, 3) (64, 202)
2025-04-18 13:05:50.759233: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
2025-04-18 13:05:50.873917: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Test batch shape: (64, 224, 224, 3) (64, 202)

3 | Modeling - VGG-19

No description has been provided for this image
💡 Modeling
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/4_VGG19Model", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/4_VGG19Model", exist_ok=True)
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input

class RareSpeciesCNN_VGG19(Model):
    """Custom CNN for rare species classification using VGG19.
    
    Architecture: VGG19 
    """
    def __init__(self, n_classes=202,
                 apply_grayscale=False,
                 apply_contrast=False, contrast_factor=1.5,
                 apply_saturation=False, saturation_factor=1.5):
        super().__init__()                          # Call the parent class constructor

        # Store preprocessing flags and factors
        self.apply_grayscale = apply_grayscale
        self.apply_contrast = apply_contrast
        self.apply_saturation = apply_saturation

        # Preprocessing Layers
        # self.rescale_layer = Rescaling(scale=1 / 255.0, name="Rescale_Layer") (Since we are using VGG19, we will use the preprocess_input function)
        # Rescaling layer is not needed as VGG19 has its own preprocessing function
        
        # Contrast, Saturation, and Grayscale layers
        if self.apply_contrast:
            self.contrast_layer = Lambda(lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor), name='Adjust_Contrast') 
        if self.apply_saturation:
            self.saturation_layer = Lambda(lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor), name='Adjust_Saturation')
        if self.apply_grayscale:
            self.grayscale_layer = Lambda(lambda x: tf.image.rgb_to_grayscale(x), name='RGB_to_Grayscale')
        
        # ----------------------------------------------------------------------------------------------------------------------------------------
        # Preprocess image for VGG19
        # VGG19 requires images to be preprocessed in a specific way (subtracting the mean RGB values)
        self.vgg19_preprocessing = Lambda(lambda x: preprocess_input(x), name='VGG19_Preprocess')
        
        # Load VGG19 Pretrained Model
        self.vgg19 = VGG19(include_top=False,           # Do not include the top classification layer (we will add our own because we have a different number of classes)
                                classes=n_classes,      # Number of classes
                                weights='imagenet')     # Use ImageNet weights
        self.vgg19.trainable = False                    # Freeze the convolutional layers in the model (transfer learning)
        # -----------------------------------------------------------------------------------------------------------------------------------------
        
        # --- Classification Head ---
        self.global_avg_pool = GlobalAveragePooling2D(name="Global_Average_Pooling")      # Global Average Pooling layer
        self.dense1 = Dense(128, name="Dense_Layer1")                                     # Smaller intermediate dense layer
        self.dropout = Dropout(0.5, name="Dropout_Layer")                                 # Dropout layer for regularization
        self.dense_output = Dense(n_classes, activation='softmax', name="Output_Layer")   # Output layer with softmax activation (for multi-class classification)

    def call(self, inputs, training=False):
        x = inputs                                     # Input layer
        
        # Apply conditional preprocessing layers
        if self.apply_contrast:
            x = self.contrast_layer(x)
        if self.apply_saturation:
            x = self.saturation_layer(x)
        if self.apply_grayscale:
            x = self.grayscale_layer(x)
            
        # Preprocess for VGG19
        x = self.vgg19_preprocessing(x)                # Preprocess the input for VGG19
        
        # Pass through VGG19 model
        x = self.vgg19(x, training=training)           # Pass through the model (VGG19)

        # Classification Head
        x = self.global_avg_pool(x)                    # Global Average Pooling
        x = self.dense1(x)                             # Dense layer
        x = self.dropout(x, training=training)         # Dropout layer
        outputs = self.dense_output(x)                 # Output layer
        return outputs

# Example Instantiation and Summary
model = RareSpeciesCNN_VGG19(
    n_classes=n_classes,
    apply_grayscale=False,
    apply_contrast=False,
    apply_saturation=False,
)

# Build the model by providing an input shape
inputs = Input(shape=(img_width, img_height, 3))        # Input shape
_ = model.call(inputs)                                  # Call the model to build it
model.summary()                                         # Print the model summary
Model: "rare_species_cnn_vgg19"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ VGG19_Preprocess (Lambda)       │ (None, 224, 224, 3)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ vgg19 (Functional)              │ (None, 7, 7, 512)      │    20,024,384 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Global_Average_Pooling          │ (None, 512)            │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1 (Dense)            │ (None, 128)            │        65,664 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dropout_Layer (Dropout)         │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Output_Layer (Dense)            │ (None, 202)            │        26,058 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 20,116,106 (76.74 MB)
 Trainable params: 91,722 (358.29 KB)
 Non-trainable params: 20,024,384 (76.39 MB)
🥇 Best Combinations Models
Original | Grayscale=F | Contrast=F | Saturation=F

[...] - ALL RESULTAS ARE IN THE TABLE OF APPENDIX C


======================================================================================== 5_Modeling_ResNet50V2_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.ops import add
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
from tensorflow.keras import regularizers                                                                           # For L2 regularization
# import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Other imports
from itertools import product

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
2025-04-18 16:02:37.183772: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-18 16:02:37.473625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744988557.579566    1155 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744988557.610413    1155 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-18 16:02:37.880987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Creates a SSL context that does not verify the server’s certificate - Needed for downloading pretrained models
# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
TensorFlow Version: 2.18.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744988560.543983    1155 gpu_device.cc:2022] Created device /device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
TensorFlow version: 2.18.0
Python version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]
CUDA version: 12.5.1
cuDNN version: 9
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size
input_shape = (img_height, img_width, 3)            # Input shape of the model
# Get class names from directory
class_names = sorted(os.listdir(train_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.
I0000 00:00:1744988561.311530    1155 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)
Train batch shape: (64, 224, 224, 3) (64, 202)
Val batch shape: (64, 224, 224, 3) (64, 202)
2025-04-18 16:02:42.722297: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
2025-04-18 16:02:42.853634: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Test batch shape: (64, 224, 224, 3) (64, 202)

3 | Modeling - ResNet50V2



No description has been provided for this image
💡 Modeling
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/5_ResNet152V2Model", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/5_ResNet152V2Model", exist_ok=True)
from tensorflow.keras.applications import ResNet152V2
from tensorflow.keras.applications.resnet_v2 import preprocess_input


class RareSpeciesCNN_ResNet152V2(Model):
    """Custom CNN for rare species classification using ResNet152V2.
    
    Architecture: ResNet152V2 
    """
    def __init__(self, n_classes=202,
                 apply_grayscale=False,
                 apply_contrast=False, contrast_factor=1.5,
                 apply_saturation=False, saturation_factor=1.5):
        super().__init__()                          # Call the parent class constructor

        # Store preprocessing flags and factors
        self.apply_grayscale = apply_grayscale
        self.apply_contrast = apply_contrast
        self.apply_saturation = apply_saturation

        # Preprocessing Layers (Same as in the previous notebook)
        # self.rescale_layer = Rescaling(scale=1 / 255.0, name="Rescale_Layer") (Since we are using ResNet152V2, we will use the preprocess_input function)
        
        # Contrast, Saturation, and Grayscale layers
        if self.apply_contrast:
            self.contrast_layer = Lambda(lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor), name='Adjust_Contrast') 
        if self.apply_saturation:
            self.saturation_layer = Lambda(lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor), name='Adjust_Saturation')
        if self.apply_grayscale:
            self.grayscale_layer = Lambda(lambda x: tf.image.rgb_to_grayscale(x), name='RGB_to_Grayscale')

        # ----------------------------------------------------------------------------------------------------------------------------------------
        # Preprocess image for ResNet152V2
        # ResNet152V2 requires images to be preprocessed in a specific way (Source: https://keras.io/api/applications/resnet/#resnet152v2-function)
        self.resnet152v2_preprocess = Lambda(lambda x: preprocess_input(x), name='ResNet152V2_Preprocess')  # Preprocess input for ResNet152V2
        
        # Load ResNet152V2 Pretrained Model
        self.resnet152v2 = ResNet152V2(include_top=False,           # Do not include the top classification layer (we will add our own because we have a different number of classes)
                                            classes=n_classes,      # Number of classes
                                            weights='imagenet')     # Use ImageNet weights
        self.resnet152v2.trainable = False                          # Freeze the convolutional layers in the model (transfer learning)
        # -----------------------------------------------------------------------------------------------------------------------------------------
        
        # --- Classification Head ---
        self.global_avg_pool = GlobalAveragePooling2D(name="Global_Average_Pooling")      # Global Average Pooling layer
        self.dense1 = Dense(128, name="Dense_Layer1")                                     # Smaller intermediate dense layer
        self.dropout = Dropout(0.5, name="Dropout_Layer")                                 # Dropout layer for regularization
        self.dense_output = Dense(n_classes, activation='softmax', name="Output_Layer")   # Output layer with softmax activation (for multi-class classification)

    def call(self, inputs, training=False):
        x = inputs                                     # Input layer
        
        # Apply conditional preprocessing layers
        if self.apply_contrast:
            x = self.contrast_layer(x)
        if self.apply_saturation:
            x = self.saturation_layer(x)
        if self.apply_grayscale:
            x = self.grayscale_layer(x)
        
        # Preprocess input for ResNet152V2
        x = self.resnet152v2_preprocess(x)             # Preprocess input for ResNet152V2
        
        # Pass through ResNet152V2 model
        x = self.resnet152v2(x, training=training)     # Pass through the model (ResNet152V2)

        # Classification Head
        x = self.global_avg_pool(x)                    # Global Average Pooling
        x = self.dense1(x)                             # Dense layer
        x = self.dropout(x, training=training)         # Dropout layer
        outputs = self.dense_output(x)                 # Output layer
        return outputs

# Example Instantiation and Summary
model = RareSpeciesCNN_ResNet152V2(
    n_classes=n_classes,
    apply_grayscale=False,
    apply_contrast=False,
    apply_saturation=False,
)

# Build the model by providing an input shape
inputs = Input(shape=(img_width, img_height, 3))        # Input shape
_ = model.call(inputs)                                  # Call the model to build it
model.summary()                                         # Print the model summary
Model: "rare_species_cnn__res_net152v2"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ ResNet152V2_Preprocess (Lambda) │ (None, 224, 224, 3)    │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ resnet152v2 (Functional)        │ (None, 7, 7, 2048)     │    58,331,648 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Global_Average_Pooling          │ (None, 2048)           │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1 (Dense)            │ (None, 128)            │       262,272 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dropout_Layer (Dropout)         │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Output_Layer (Dense)            │ (None, 202)            │        26,058 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 58,619,978 (223.62 MB)
 Trainable params: 288,330 (1.10 MB)
 Non-trainable params: 58,331,648 (222.52 MB)
🥇 Best Combinations Models
Original | Grayscale=F | Contrast=F | Saturation=F

[...] - ALL RESULTAS ARE IN THE TABLE OF APPENDIX C
 
======================================================================================== 6_Modeling_ConvNeXt_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.ops import add
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
from tensorflow.keras import regularizers                                                                           # For L2 regularization
# import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Other imports
from itertools import product

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
2025-04-18 03:30:56.464215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744943456.557767    1817 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744943456.584301    1817 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744943456.786554    1817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744943456.786579    1817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744943456.786582    1817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744943456.786584    1817 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-18 03:30:56.807310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Creates a SSL context that does not verify the server’s certificate - Needed for downloading pretrained models
# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
TensorFlow Version: 2.19.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744943460.077223    1817 gpu_device.cc:2019] Created device /device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
TensorFlow version: 2.19.0
Python version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]
CUDA version: 12.5.1
cuDNN version: 9
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size
input_shape = (img_height, img_width, 3)            # Input shape of the model
# Get class names from directory
class_names = sorted(os.listdir(train_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.
I0000 00:00:1744943461.394724    1817 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)
Train batch shape: (64, 224, 224, 3) (64, 202)
2025-04-18 03:31:03.239234: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Val batch shape: (64, 224, 224, 3) (64, 202)
2025-04-18 03:31:03.490399: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Test batch shape: (64, 224, 224, 3) (64, 202)

3 | Modeling - ConvNeXtBase



No description has been provided for this image
💡 Modeling
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/6_ConvNeXtBase", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/6_ConvNeXtBase", exist_ok=True)
from tensorflow.keras.applications import ConvNeXtBase
from tensorflow.keras.applications.convnext import preprocess_input

class RareSpeciesCNN_ConvNeXtBase(Model):
    """Custom CNN for rare species classification using ConvNeXtBase.
    
    Architecture: ConvNeXtBase 
    """
    def __init__(self, n_classes=202,
                 apply_grayscale=False,
                 apply_contrast=False, contrast_factor=1.5,
                 apply_saturation=False, saturation_factor=1.5):
        super().__init__()                          # Call the parent class constructor

        # Store preprocessing flags and factors
        self.apply_grayscale = apply_grayscale
        self.apply_contrast = apply_contrast
        self.apply_saturation = apply_saturation

        # Preprocessing Layers (Same as in the previous notebook)
        # self.rescale_layer = Rescaling(scale=1 / 255.0, name="Rescale_Layer") (Since we are using ConvNeXtBase, we will use the preprocess_input function)

        # Contrast, Saturation, and Grayscale layers
        if self.apply_contrast:
            self.contrast_layer = Lambda(lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor), name='Adjust_Contrast') 
        if self.apply_saturation:
            self.saturation_layer = Lambda(lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor), name='Adjust_Saturation')
        if self.apply_grayscale:
            self.grayscale_layer = Lambda(lambda x: tf.image.rgb_to_grayscale(x), name='RGB_to_Grayscale')

        # ----------------------------------------------------------------------------------------------------------------------------------------
        # Preprocess image for ConvNeXtBase
        # ResNet152V2 requires images to be preprocessed in a specific way (Source: https://keras.io/api/applications/convnext/#convnextbase-function)
        self.convnextbase_preprocess = Lambda(lambda x: preprocess_input(x), name='ConvNeXtBase_Preprocess')
        
        # Load ConvNeXtBase Pretrained Model
        self.convnextbase = ConvNeXtBase(include_top=False,         # Do not include the top classification layer (we will add our own because we have a different number of classes)
                                         classes=n_classes,         # Number of classes
                                         weights='imagenet')        # Use ImageNet weights
        self.convnextbase.trainable = False                         # Freeze the convolutional layers in the base model (transfer learning)
        # -----------------------------------------------------------------------------------------------------------------------------------------

        # --- Classification Head ---
        self.global_avg_pool = GlobalAveragePooling2D(name="Global_Average_Pooling")      # Global Average Pooling layer
        self.dense1 = Dense(128, name="Dense_Layer1")                                     # Smaller intermediate dense layer
        self.dropout = Dropout(0.5, name="Dropout_Layer")                                 # Dropout layer for regularization
        self.dense_output = Dense(n_classes, activation='softmax', name="Output_Layer")   # Output layer with softmax activation (for multi-class classification)

    def call(self, inputs, training=False):
        x = inputs                                     # Input layer
        
        # Apply conditional preprocessing layers
        if self.apply_contrast:
            x = self.contrast_layer(x)
        if self.apply_saturation:
            x = self.saturation_layer(x)
        if self.apply_grayscale:
            x = self.grayscale_layer(x)

        # Preprocess image for ConvNeXtBase
        x = self.convnextbase_preprocess(x)              # Preprocess the image for ConvNeXtBase
        
        # Pass through ConvNeXtBase model
        x = self.convnextbase(x, training=training)      # Pass through the model (ConvNeXtBase)

        # Classification Head
        x = self.global_avg_pool(x)                      # Global Average Pooling
        x = self.dense1(x)                               # Dense layer
        x = self.dropout(x, training=training)           # Dropout layer
        outputs = self.dense_output(x)                   # Output layer
        return outputs

# Example Instantiation and Summary
model = RareSpeciesCNN_ConvNeXtBase(
    n_classes=n_classes,
    apply_grayscale=False,
    apply_contrast=False,
    apply_saturation=False,
)

# Build the model by providing an input shape
inputs = Input(shape=(img_width, img_height, 3))        # Input shape
_ = model.call(inputs)                                  # Call the model to build it
model.summary()                                         # Print the model summary
Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_base_notop.h5
350926856/350926856 ━━━━━━━━━━━━━━━━━━━━ 8s 0us/step
Model: "rare_species_cnn__conv_ne_xt_base"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ ConvNeXtBase_Preprocess         │ (None, 224, 224, 3)    │             0 │
│ (Lambda)                        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ convnext_base (Functional)      │ (None, 7, 7, 1024)     │    87,566,464 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Global_Average_Pooling          │ (None, 1024)           │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1 (Dense)            │ (None, 128)            │       131,200 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dropout_Layer (Dropout)         │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Output_Layer (Dense)            │ (None, 202)            │        26,058 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 87,723,722 (334.64 MB)
 Trainable params: 157,258 (614.29 KB)
 Non-trainable params: 87,566,464 (334.04 MB)
🥇 Best Combinations Models
Original | Grayscale=F | Contrast=F | Saturation=F

[...] - ALL RESULTAS ARE IN THE TABLE OF APPENDIX C

======================================================================================== 7_Modeling_EfficientNetB0_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.ops import add
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
from tensorflow.keras import regularizers                                                                           # For L2 regularization
# import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Other imports
from itertools import product

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
2025-04-18 17:56:18.781661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744995378.874188    1751 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744995378.900180    1751 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744995379.102812    1751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744995379.102840    1751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744995379.102841    1751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744995379.102843    1751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-18 17:56:19.123970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
# Creates a SSL context that does not verify the server’s certificate - Needed for downloading pretrained models
# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
TensorFlow Version: 2.19.0
Is TensorFlow built with CUDA? True
GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
GPU Device Name: /device:GPU:0
I0000 00:00:1744995382.431748    1751 gpu_device.cc:2019] Created device /device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
TensorFlow version: 2.19.0
Python version: 3.12.3 (main, Feb  4 2025, 14:48:35) [GCC 13.3.0]
CUDA version: 12.5.1
cuDNN version: 9
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size
input_shape = (img_height, img_width, 3)            # Input shape of the model
# Get class names from directory
class_names = sorted(os.listdir(train_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.
I0000 00:00:1744995383.733615    1751 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
Found 1198 files belonging to 202 classes.
Found 1199 files belonging to 202 classes.

Loaded: Train (9600), Val (1216), Test (1216)
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)
Train batch shape: (64, 224, 224, 3) (64, 202)
2025-04-18 17:56:25.732171: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Val batch shape: (64, 224, 224, 3) (64, 202)
2025-04-18 17:56:26.031678: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
Test batch shape: (64, 224, 224, 3) (64, 202)

3 | Modeling - EfficientNetV2B0



No description has been provided for this image
💡 Modeling
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/7_EfficientNetV2B0", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/7_EfficientNetV2B0", exist_ok=True)
from tensorflow.keras.applications import EfficientNetV2B0
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input

class RareSpeciesCNN_EfficientNetV2B0(Model):
    """Custom CNN for rare species classification using EfficientNetV2B0.
    
    Architecture: EfficientNetV2B0 
    """
    def __init__(self, n_classes=202,
                 apply_grayscale=False,
                 apply_contrast=False, contrast_factor=1.5,
                 apply_saturation=False, saturation_factor=1.5):
        super().__init__()                          # Call the parent class constructor
        
        # Preprocessing Layers (Same as in the previous notebook)
        # self.rescale_layer = Rescaling(scale=1 / 255.0, name="Rescale_Layer") (Since we are using EfficientNetV2B0, we will use the preprocess_input function)
        
        # Store preprocessing flags and factors
        self.apply_grayscale = apply_grayscale
        self.apply_contrast = apply_contrast
        self.apply_saturation = apply_saturation

        # # Preprocessing Layers (We will not use the Rescaling layer here, as EfficientNetV2B0 already includes it)
        if self.apply_contrast:
            self.contrast_layer = Lambda(lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor), name='Adjust_Contrast') 
        if self.apply_saturation:
            self.saturation_layer = Lambda(lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor), name='Adjust_Saturation')
        if self.apply_grayscale:
            self.grayscale_layer = Lambda(lambda x: tf.image.rgb_to_grayscale(x), name='RGB_to_Grayscale')

        # ----------------------------------------------------------------------------------------------------------------------------------------
        # Preprocess image for EfficientNetV2B0
        self.efficientnetv2_preprocessing = Lambda(lambda x: preprocess_input(x), name='EfficientNetV2_Preprocess')
        
        # Load EfficientNetV2B0 Pretrained Model
        self.efficientnetv2b0 = EfficientNetV2B0(include_top=False,           # Do not include the top classification layer (we will add our own because we have a different number of classes)
                                                 classes=n_classes,           # Number of classes
                                                 include_preprocessing=True,  # Include preprocessing layers (rescaling) of EfficientNetV2B0
                                                 weights='imagenet')          # Use ImageNet weights
        self.efficientnetv2b0.trainable = False                               # Freeze the convolutional layers in the base model (transfer learning)
        # -----------------------------------------------------------------------------------------------------------------------------------------

        # --- Classification Head ---
        self.global_avg_pool = GlobalAveragePooling2D(name="Global_Average_Pooling")      # Global Average Pooling layer
        self.dense1 = Dense(128, name="Dense_Layer1")                                     # Smaller intermediate dense layer
        self.dropout = Dropout(0.5, name="Dropout_Layer")                                 # Dropout layer for regularization
        self.dense_output = Dense(n_classes, activation='softmax', name="Output_Layer")   # Output layer with softmax activation (for multi-class classification)

    def call(self, inputs, training=False):
        x = inputs                                     # Input layer
                
        # Apply conditional preprocessing layers
        if self.apply_contrast:
            x = self.contrast_layer(x)
        if self.apply_saturation:
            x = self.saturation_layer(x)
        if self.apply_grayscale:
            x = self.grayscale_layer(x)
            
        # Preprocess image for EfficientNetV2B0
        x = self.efficientnetv2_preprocessing(x)             # Preprocess the image for EfficientNetV2B0
        
        # Pass through EfficientNetV2B0 model
        x = self.efficientnetv2b0(x, training=training)      # Pass through the model (EfficientNetV2B0)

        # Classification Head
        x = self.global_avg_pool(x)                          # Global Average Pooling
        x = self.dense1(x)                                   # Dense layer
        x = self.dropout(x, training=training)               # Dropout layer
        outputs = self.dense_output(x)                       # Output layer
        return outputs

# Example Instantiation and Summary
model = RareSpeciesCNN_EfficientNetV2B0(
    n_classes=n_classes,
    apply_grayscale=False,
    apply_contrast=False,
    apply_saturation=False,
)

# Build the model by providing an input shape
inputs = Input(shape=(img_width, img_height, 3))        # Input shape
_ = model.call(inputs)                                  # Call the model to build it
model.summary()                                         # Print the model summary
Model: "rare_species_cnn__efficient_net_v2b0"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ EfficientNetV2_Preprocess       │ (None, 224, 224, 3)    │             0 │
│ (Lambda)                        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ efficientnetv2-b0 (Functional)  │ (None, 7, 7, 1280)     │     5,919,312 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Global_Average_Pooling          │ (None, 1280)           │             0 │
│ (GlobalAveragePooling2D)        │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dense_Layer1 (Dense)            │ (None, 128)            │       163,968 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Dropout_Layer (Dropout)         │ (None, 128)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Output_Layer (Dense)            │ (None, 202)            │        26,058 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 6,109,338 (23.31 MB)
 Trainable params: 190,026 (742.29 KB)
 Non-trainable params: 5,919,312 (22.58 MB)
🥇 Best Combinations Models
Original | Grayscale=F | Contrast=F | Saturation=F

[...] - ALL RESULTAS ARE IN THE TABLE OF APPENDIX C

========================================================================================  8_TuningBestModel_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.ops import add
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
# import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Other imports
from itertools import product

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)

# Set random seed for reproducibility
# Source: https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development
tf.keras.utils.set_random_seed(2025)
# Creates a SSL context that does not verify the server’s certificate - Needed for downloading pretrained models
# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
🖌️ SMOTE (Data Augmentation)
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size (keep consistent with previous training)
input_shape = (img_height, img_width, 3)            # Input shape of the model
value_range = (0.0, 1.0)                            # Range of pixel values
# Import SMOTE training data
train_DataAugmentationSMOTE_dir = Path("data/RareSpecies_Split/train_DataAugmentationSMOTE")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# train_DataAugmentationSMOTE_dir = Path("/content/RareSpecies_Split/train_DataAugmentationSMOTE")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")

# Get class names from directory
class_names = sorted(os.listdir(train_DataAugmentationSMOTE_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_DataAugmentationSMOTE_datagen, val_datagen, test_datagen = load_images_from_directory(train_DataAugmentationSMOTE_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_DataAugmentationSMOTE_datagen.take(1):
    print("Train batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)

Tuning Best Model



💡 Best Model Building Function for Keras Tuner (ConvNeXtBase)
# Function to tune best combination model
# Source: https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/
#         https://keras.io/guides/transfer_learning/
#         https://keras.io/keras_tuner/getting_started/
# --- Keras Tuner ---
# Source: https://keras.io/keras_tuner/
# Make sure it's installed: 

# !pip install keras-tuner -q
import keras_tuner as kt
from tensorflow.keras.applications import ConvNeXtBase               # Specific model
from tensorflow.keras.applications.convnext import preprocess_input  # Specific preprocessing

def build_model(hp):
    """
    Builds a ConvNeXtBase model with hyperparameters for tuning.

    Args:
        hp (keras_tuner.HyperParameters): Hyperparameters object from Keras Tuner.

    Returns:
        keras.Model: Compiled Keras model.
    """
    
    # Hyperparameters
    # Source: https://keras.io/keras_tuner/api/hyperparameters/
    hp_unfreeze = hp.Boolean("unfreeze_base", default=False)             # default=True for debugging
    # Note: If unfreezing, use a lower learning rate -> We choose LR < 1e-3 (used in all previous models)
    # Source: https://keras.io/guides/transfer_learning/#do-a-round-of-finetuning-of-the-entire-model
    hp_learning_rate = hp.Choice("learning_rate", values=[1e-3, 1e-4, 1e-5])
    hp_optimizer_choice = hp.Choice("optimizer", values=['adam', 'sgd', 'rmsprop'])
    hp_dropout_rate = hp.Float("dropout_rate", min_value=0.4, max_value=0.7, step=0.1)

    # ----------------------------------------------------------------------------------------------------------------
    # Base Model (ConvNeXtBase)
    # Source: https://keras.io/api/applications/convnext/#convnextbase-function
    base_model = ConvNeXtBase(
        include_top=False,
        weights='imagenet',
        input_shape=input_shape
    )

    # Set Trainability
    num_layers_base = len(base_model.layers)
    fine_tune_layers_count = 22                                                     # Number of layers FROM THE END to unfreeze
    
    # print(f"NOTE: Base model ConvNeXtBase has {num_layers_base} layers.")         # Debugging

    # Unfreeze layers if the hyperparameter is set
    # Source: https://keras.io/guides/transfer_learning/
    if hp_unfreeze:
        print(f"NOTE: Fine-tuning ConvNeXtBase. Unfreezing the last {fine_tune_layers_count} layers.")
        
        # First, set the entire base as non-trainable
        base_model.trainable = False 
        
        # Then, unfreeze the top layers
        for layer in base_model.layers[-fine_tune_layers_count:]:
            
             # Be careful with Batch Normalization layers, standard practice is to keep them frozen
             # See: https://keras.io/guides/transfer_learning/#build-a-model
             if not isinstance(layer, BatchNormalization):
                 layer.trainable = True
        
        # Crucially, ensure the base_model itself is marked as trainable IF any layers within it are
        # This might not be strictly necessary if layer.trainable works directly, but safer.
        base_model.trainable = True 
        
        # print("Trainable status of the LAST few layers:")                                          # Debugging
        # unfrozen_names = []
        # for layer in base_model.layers[-(fine_tune_layers_count):]: # Print a bit more context
        #     print(f"  Layer {layer.name}: Trainable = {layer.trainable}")
        #     if layer.trainable:
        #          unfrozen_names.append(layer.name)
        # print("-" * 30)
        # print(f"Total unfrozen layers in base: {len(unfrozen_names)}")
        # print("-" * 30)
        
    else:
        print("NOTE: Keeping the base ConvNeXtBase model frozen.")
        base_model.trainable = False

    # Model Construction
    inputs = Input(shape=input_shape, name="Input_Layer")

    # Apply the specific preprocessing for ConvNeXtBase
    # Source: https://keras.io/api/applications/convnext/#preprocessinput-function
    x = Lambda(lambda img: preprocess_input(img), name='ConvNeXtBase_Preprocess')(inputs)

    # Pass through the base model
    x = base_model(x, training=base_model.trainable) # training=False if frozen, True if unfrozen

    # Classification Head (Same as class RareSpeciesCNN_ConvNeXtBase - 7nd notebook)
    x = GlobalAveragePooling2D(name="Global_Average_Pooling")(x)
    x = Dense(128, name="Dense_Layer1")(x)
    # Use the hyperparameter for dropout rate
    x = Dropout(hp_dropout_rate, name="Dropout_Layer")(x)
    outputs = Dense(n_classes, activation='softmax', name="Output_Layer")(x)

    model = Model(inputs, outputs)

    # Optimizer Selection
    # Source: https://keras.io/api/optimizers/
    if hp_optimizer_choice == 'adam':
        optimizer = Adam(learning_rate=hp_learning_rate)               # Removed weight decay for simplicity in tuning
    elif hp_optimizer_choice == 'sgd':
        optimizer = SGD(learning_rate=hp_learning_rate, momentum=0.9) # Added momentum, common for SGD
    elif hp_optimizer_choice == 'rmsprop':
        optimizer = RMSprop(learning_rate=hp_learning_rate)

    # --- Compile Model ---
    # Source: https://keras.io/api/models/model_training_apis/
    loss = CategoricalCrossentropy(name="Loss")
    metrics = [
        CategoricalAccuracy(name="accuracy"),
        F1Score(average="macro", name="f1_score"), # Using F1 Macro as the primary metric
        Precision(name="precision"),
        Recall(name="recall"),
        AUC(name="auc")
    ]

    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

    return model
# Test the build function
hp = kt.HyperParameters()
test_model = build_model(hp)
test_model.summary()
⚙️ Keras Tuner Setup (Hyperband)
# # Source: https://keras.io/keras_tuner/api/tuners/hyperband/#hyperband-class
# tuner = kt.Hyperband(
#     hypermodel=build_model,
#     objective=kt.Objective("val_f1_score", direction="max"), # Primary objective: maximize validation F1 macro score
#     max_epochs=15,                                           # Max epochs *per trial execution* within Hyperband brackets
#     factor=3,                                                # Reduction factor for epochs and number of models per bracket
#     hyperband_iterations=1,                                  # Number of times to iterate over the full Hyperband algorithm
#     seed=2025,                                               # Seed for reproducibility within the tuner
#     directory="keras_tuner_dir",
#     project_name=f"ConvNeXtBase_FineTune_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}",
#     overwrite=True                                           # Set to False to resume previous tuning runs
# )

# # Print a summary of the search space
# tuner.search_space_summary()                     ############### Error: Out of memory (OOM)
⚙️ Keras Tuner Setup (RandomSearch)
# Source: https://keras.io/keras_tuner/api/tuners/random/#randomsearch-class
tuner = kt.RandomSearch(
    hypermodel=build_model,
    objective=kt.Objective("val_f1_score", direction="max"), # Primary objective: maximize validation F1 macro score
    max_trials=10,                                           # Number of trials to run
    executions_per_trial=1,                                  # Number of executions per trial
    directory="keras_tuner_dir",
    project_name=f"ConvNeXtBase_FineTune_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}",
    overwrite=True                                           # Set to False to resume previous tuning runs
)
# Print a summary of the search space
tuner.search_space_summary()
🚀 Run the Hyperparameter Search
# Define callbacks for the search phase (applied to each trial)
# Early stopping is crucial here to stop non-promising trials quickly
search_callbacks = [
    EarlyStopping(
        monitor='val_loss', # Monitor validation loss for stopping trials
        patience=5,         # Stop after 5 epochs of no improvement in val_loss
        verbose=1,
        restore_best_weights=True # Restore weights from the epoch with the best val_loss
    )
    
    # Note: ReduceLROnPlateau is usually NOT used during the search itself,
    #       as the tuner is already exploring different learning rates.
    # We will use it when retraining the final best model.
]
# Start the search
# Use a moderate number of epochs for the overall search.
# `max_epochs` in the tuner definition controls the limit *within* Hyperband.
search_start_time = time.time()
tuner.search(
    train_DataAugmentationSMOTE_datagen,
    epochs=20,                              # Overall epochs budget for the search process
    validation_data=val_datagen,
    callbacks=search_callbacks,
    verbose=1                               # Set to 2 for more detailed logs per epoch, 1 for progress bar per epoch
)
search_time = round(time.time() - search_start_time, 2)
print(f"\n\033[1mHyperparameter Search Completed in\033[0m {search_time} seconds ({str(datetime.timedelta(seconds=search_time))} h)")
📊 Get and Display Best Hyperparameters
# Get the optimal hyperparameters
# Source: https://keras.io/keras_tuner/api/tuners/base_tuner/
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete.
- \033[1mOptimal Learning Rate:\033[0m {best_hps.get('learning_rate')}
- \033[1mOptimal Optimizer:\033[0m {best_hps.get('optimizer')}
- \033[1mOptimal Dropout Rate:\033[0m {best_hps.get('dropout_rate')}
- \033[1mUnfreeze Base Model:\033[0m {best_hps.get('unfreeze_base')}
""")

# Show summary of top results
tuner.results_summary(num_trials=5)
🚂 Retrain the Best Model
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/8_ConvNeXtBaseFinetuned", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/8_ConvNeXtBaseFinetuned", exist_ok=True)
# Build the model with the optimal hyperparameters
final_model = build_model(best_hps)
final_model.summary()

# Define callbacks for the final training phase
model_name = f"ConvNeXtBase_BestFineTuned_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}"
final_callbacks = [
    ModelCheckpoint(
        f"./ModelCallbacks/8_ConvNeXtBaseFinetuned/BestModel_checkpoint_{model_name}.keras",
        monitor="val_f1_score", # Save based on best validation F1 score
        mode="max",             # Maximize F1 score
        save_best_only=True,
        verbose=1
    ),
    CSVLogger(f"./ModelCallbacks/8_ConvNeXtBaseFinetuned/BestModel_metrics_{model_name}.csv"),
    ReduceLROnPlateau(          # Now we use ReduceLROnPlateau
        monitor='val_loss',
        factor=0.2,
        patience=5,
        verbose=1,
        min_lr=1e-7             # Set a lower minimum learning rate
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=10,              # Allow more patience for final training
        verbose=1,
        restore_best_weights=True # Restore best weights based on val_loss
    )
]

# Train the final model
# Use a larger number of epochs for final training, relying on EarlyStopping
print(f"\nStarting Final Training for {model_name}")
start_time = time.time()
history = final_model.fit(
    train_DataAugmentationSMOTE_datagen,
    epochs=100,                                 # Set a high epoch number, EarlyStopping will handle it
    validation_data=val_datagen,
    callbacks=final_callbacks,
    verbose=1
)
train_time = round(time.time() - start_time, 2)
print(f"\nFinal Training completed in {train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)")
🧪 Final Model Evaluation
# Evaluate model
from utilities import plot_metrics
plot_metrics(history, file_path=f"./ModelsEvaluation/8_ConvNeXtBaseFinetuned/8_BestModel_TrainingValidationMetrics_{model_name}.png")
# Evaluate on validation and test sets
train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}
val_results = final_model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)
test_results = final_model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)
# Display results
from utilities import display_side_by_side, create_evaluation_dataframe
results_df = create_evaluation_dataframe(
    model_name="ConvNeXtBase_FineTuned",
    variation=f"LR={best_hps.get('learning_rate')}, Optim={best_hps.get('optimizer')}, Dropout={best_hps.get('dropout_rate'):.2f}, Unfreeze={best_hps.get('unfreeze_base')}",
    train_metrics=train_results,
    val_metrics=val_results,
    test_metrics=test_results,
    train_time=train_time, # This is the final retraining time
    csv_save_path=f"./ModelsEvaluation/8_ConvNeXtBaseFinetuned/8_BestModel_EvaluationResults_{model_name}.csv"
)
display_side_by_side(results_df, super_title="Final Fine-Tuned Model Evaluation Results")
# Plot n right and n wrong predictions
from utilities import plot_predictions
plot_predictions(
    model=final_model,
    class_names=class_names,
    train_dir=train_DataAugmentationSMOTE_dir,
    test_data=test_datagen,
    num_images=10,
    file_path=f"./ModelsEvaluation/8_ConvNeXtBaseFinetuned/8_TestPredictions_{model_name}.png",
)
🔗 Bibliography/References
[1] Team, K. (2025). Keras documentation: KerasTuner API documentation. Keras.io. https://keras.io/keras_tuner/api/

======================================================================================== 9_InnovativeApproaches_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")
from PIL import Image

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Other imports
from itertools import product
import random 

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
# Import custom module for importing data, visualization, and utilities
import utilities_InnovativeApproaches
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# # Import DataFrame with class names and labels
df_train = pd.read_csv("./data/train.csv")
df_val = pd.read_csv("./data/val.csv")
df_test = pd.read_csv("./data/test.csv")

# For Google Collab
# df_train = pd.read_csv("/content/train.csv")
# df_val = pd.read_csv("/content/val.csv")
# df_test = pd.read_csv("/content/test.csv")

# Check the shape of the DataFrame
print("Train DataFrame shape:", df_train.shape)
print("Val DataFrame shape:", df_val.shape)
print("Test DataFrame shape:", df_test.shape)

# Show the first 5 rows of the DataFrame
print("\nTrain DataFrame:")
df_train.head()
Train DataFrame shape: (9586, 8)
Val DataFrame shape: (1198, 8)
Test DataFrame shape: (1199, 8)

Train DataFrame:
rare_species_id	eol_content_id	eol_page_id	phylum	family	file_path	family_encoded	phylum_encoded
0	093761c7-1130-4673-820e-e8ba1b05b108	20642994	46560177	chordata	dalatiidae	chordata_dalatiidae/20642994_46560177_eol-full...	61	1
1	34173b57-7adc-413b-b51b-c4145fbce8b2	24577109	46579700	chordata	serranidae	chordata_serranidae/24577109_46579700_eol-full...	169	1
2	cce03336-6666-4a70-af94-8b87c65f772a	21446702	323892	chordata	callitrichidae	chordata_callitrichidae/21446702_323892_eol-fu...	29	1
3	b94d36f2-dbe6-4c9d-8f56-4e88d5ac5ce8	20153343	46579700	chordata	serranidae	chordata_serranidae/20153343_46579700_eol-full...	169	1
4	ead7040f-4a6a-4d29-ac5b-6fae12bf874b	8797211	4890709	mollusca	zonitidae	mollusca_zonitidae/8797211_4890709_eol-full-si...	201	4

9 | Innovative Approaches - Zero-Shot Image Classification



Since we already detect that many of the images aren't the animal, but information about the animal (or similar), we decided to use a zero-shot image classification model to classify the images (Animal or Not Animal).
We will use the CLIP model from Hugging Face, which is a zero-shot image classification model that can classify images into a set of labels without any training on those labels.
The model is trained on a large dataset of images and text, allowing it to understand the relationship between images and their corresponding labels.
🕵 Zero-Shot Image Classification with CLIP
Note: The code for this section was run in a environment with PyTorch and CUDA installed, which allows for GPU acceleration. The reason for this is that the CLIP model have a error when running with TensorFlow (as we can see in the comments below).

Comparison of CLIP Models

clip-vit-large-patch14 VS clip-vit-base-patch32 vs clip-vit-base-patch16

After some research, we decide to use clip-vit-base-patch16 since we need to classify all the images in the dataset, and we have limited computational resources and time. (Source https://github.com/UKPLab/sentence-transformers/issues/1658)
# !pip install transformers
# Transformers works better with PyTorch
# So we will run the next part of code in other environment with PyTorch

# RuntimeError: Failed to import transformers.models.clip.modeling_tf_clip because of the following error (look up to see its traceback):
#               Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers.

# Source to install PyTorch: https://pytorch.org/get-started/locally/
# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
# !pip install transformers
# !pip install accelerate
# Imports for Hugging Face models
from transformers import CLIPProcessor, CLIPModel
# Confirm if the GPU is available
import torch
gpu_available = torch.cuda.is_available()
gpu_name = torch.cuda.get_device_name(0) if gpu_available else "No GPU found"
print(f"GPU Available: {gpu_available}, GPU Name: {gpu_name}")
GPU Available: True, GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU
# File paths of examples
# Not Animal : train/arthropoda_formicidae/28605454_468536_eol-full-size-copy.jpg
#              train/arthropoda_formicidae/28657539_491832_eol-full-size-copy.jpg (Same as the 1st Notebook)

# Difficult to classify : train/arthropoda_fringillidae/22375397_45518969_eol-full-size-copy.jpg
#                         train/cnidaria_fungiidae/28267855_45276583_eol-full-size-copy.jpg
# Plot the images in a row
from utilities_InnovativeApproaches import plot_images_from_directory

# Define the file paths of the images to be plotted and classified
images_paths = [
    Path(train_dir) / "arthropoda_formicidae/28605454_468536_eol-full-size-copy.jpg",
    Path(train_dir) / "arthropoda_formicidae/28657539_491832_eol-full-size-copy.jpg",
    Path(train_dir) / "chordata_fringillidae/22375397_45518969_eol-full-size-copy.jpg",
    Path(train_dir) / "cnidaria_fungiidae/28267855_45276583_eol-full-size-copy.jpg",
    Path(train_dir) / "chordata_indriidae/28911614_7250886_eol-full-size-copy.jpg"
]

# Plot the images in a row
plot_images_from_directory(images_paths, ["Not Animal", "Not Animal", "Difficult to classify", "Difficult to classify", "Animal"])
No description has been provided for this image
# Model Link: https://huggingface.co/openai/clip-vit-base-patch16
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")

# Load an image from the dataset
image_path = Path(train_dir) / "chordata_indriidae/28911614_7250886_eol-full-size-copy.jpg"
image = Image.open(image_path)

# Preprocess the image
inputs = processor(text=["animal", "not animal"],           # Text descriptions for the image
                   images=image,                            # Image to be processed
                   return_tensors="pt",                     # Return as PyTorch tensors | Source: https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/processing_clip.py
                   padding=True)                            # Pad the images to the same size

# Perform inference
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image     # This is the image-text similarity score
probs = logits_per_image.softmax(dim=1)         # We can take the softmax to get the label probabilities
print("Logits per image:", logits_per_image)
print("Probabilities:", probs)

# Print the class classified as the most probable
print("Predicted class:", probs.argmax(dim=1).item())
Logits per image: tensor([[23.3938, 21.9374]], grad_fn=<TBackward0>)
Probabilities: tensor([[0.8110, 0.1890]], grad_fn=<SoftmaxBackward0>)
Predicted class: 0
from utilities_InnovativeApproaches import visualize_images_and_probs

# Visualize the images and probabilities (loop through the images and classify them)
for img in images_paths:
    
    # Load the image using PIL
    image = Image.open(img)
    
    # Preprocess the image
    inputs = processor(text=["animal", "not animal"], images=image, return_tensors="pt", padding=True)
    
    # Perform inference
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image
    probs = logits_per_image.softmax(dim=1)
    
    # Visualize the images and probabilities
    visualize_images_and_probs(images=image, 
                                probs=probs, 
                                classes=["animal", "not animal"],
                                title="CLIP Model Predictions")
No description has been provided for this image
No description has been provided for this image
No description has been provided for this image
No description has been provided for this image
No description has been provided for this image
# Check file types in the 'file_path' column of the DataFrame

# Add a new column 'file_type' to each DataFrame
df_train['file_type'] = df_train['file_path'].apply(lambda x: os.path.splitext(x)[1])
df_val['file_type'] = df_val['file_path'].apply(lambda x: os.path.splitext(x)[1])
df_test['file_type'] = df_test['file_path'].apply(lambda x: os.path.splitext(x)[1])

# Check the unique file types in the DataFrames
print("Train file types:", df_train['file_type'].unique())
print("Val file types:", df_val['file_type'].unique())
print("Test file types:", df_test['file_type'].unique())
Train file types: ['.jpg']
Val file types: ['.jpg']
Test file types: ['.jpg']
# Function to classify all images in a directory and save the results as a DataFrame (will be joined with the original DataFrame)
def classify_images_in_directory(directory: str, 
                                 model: CLIPModel, processor: CLIPProcessor, 
                                 output_csv: str = None) -> pd.DataFrame:
    """Classify all images in a directory using a CLIP model and save the results as a DataFrame.

    Args:
        directory (str): The directory containing images to classify.
        model (CLIPModel): The CLIP model used for classification.
        processor (CLIPProcessor): The processor for preparing images and text.
        output_csv (str, optional): Path to save the results as a CSV file. Defaults to None.

    Returns:
        df_results(pd.DataFrame): A DataFrame containing the classification results.
    """
    # --- Setup ---
    main_dir = Path(directory)
    if not main_dir.is_dir():
        raise NotADirectoryError(f"Provided directory does not exist: {directory}")

    # List to store the results
    results = []
    image_extensions = ['.jpg'] # We already checked the file types in the DataFrame

    # Find all image files recursively using rglob
    all_image_paths = []
    for ext in image_extensions:

        # rglob to find files with the current extension recursively
        # Source: https://docs.python.org/3/library/pathlib.html#pathlib.Path.rglob
        all_image_paths.extend(list(main_dir.rglob(f'*{ext}')))         # Include lowercase extensions
        all_image_paths.extend(list(main_dir.rglob(f'*{ext.upper()}'))) # Include uppercase extensions

    if not all_image_paths:
        print(f"Warning: No image files found in {directory} with extensions {image_extensions}")
        return
    
    # Remove duplicates and convert to string paths
    all_image_paths = list(set([str(path) for path in all_image_paths]))
    print(f"Found {len(all_image_paths)} images to process.")
    # print(f"Examples of images found: {all_image_paths[:50]}")         # For debugging
        
    # Iterate over all files in the directory 
    # tqdm is used to show a progress bar for the loop | Source: https://github.com/tqdm/tqdm
    for filename in tqdm(all_image_paths, desc="Processing images"):
          
        # Load the image
        image_path = os.path.join(directory, filename)
        image = Image.open(image_path)
        image = Image.open(image_path).convert("RGB")    # Ensure image is RGB

        # Preprocess the image
        classification_labels = ["animal", "not animal"] # Define the labels for classification
        inputs = processor(text=classification_labels, images=image, return_tensors="pt", padding=True)

        # Perform inference
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image     # This is the image-text similarity score
        probs = logits_per_image.softmax(dim=1)         # We can take the softmax to get the label probabilities
        probs = probs.cpu()                             # Move results back to CPU for numpy/item() | Source: https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html
                    
        # Append the results to the list
        results.append({
            "file_path": filename,
            "animal_prob": probs[0][0].item(),       # Probability of being an animal     [0]
            "not_animal_prob": probs[0][1].item(),   # Probability of not being an animal [1]
            "predicted_label": "animal" if probs[0][0].item() > probs[0][1].item() else "not animal"
        })

    # Create a DataFrame from the results
    df_results = pd.DataFrame(results)

    # Save the DataFrame as a CSV file if output_csv is provided
    if output_csv is not None:
        df_results.to_csv(output_csv, index=False)
    
    return df_results
def classify_images_in_directory(directory: str,
                                 model: CLIPModel, processor: CLIPProcessor,
                                 output_csv: str = None) -> pd.DataFrame:
    """Classify all images in a directory using a CLIP model and save the results.

    Outputs a DataFrame with relative file_path, animal_prob, not_animal_prob,
    and a binary IsAnimal? column (1 for animal, 0 for not animal).

    Args:
        directory (str): The base directory (e.g., train_dir) containing class subdirectories.
        model (CLIPModel): The CLIP model used for classification.
        processor (CLIPProcessor): The processor for preparing images and text.
        output_csv (str, optional): Path to save the results as a CSV file. Defaults to None.

    Returns:
        pd.DataFrame: A DataFrame containing the classification results.
    """
    # --- Setup ---
    main_dir = Path(directory)
    if not main_dir.is_dir():
        raise NotADirectoryError(f"Provided directory does not exist: {directory}")

    results = []
    image_extensions = ['.jpg']  # From your previous check

    # Find all image files recursively
    all_image_paths = []
    for ext in image_extensions:
        # Use rglob to find files, ensuring case-insensitivity if needed (though extensions seem lowercase)
        all_image_paths.extend(list(main_dir.rglob(f'*{ext}')))

    print(f"Found {len(all_image_paths)} images to process.")

    # Define the classification labels for CLIP
    classification_labels = ["animal", "not animal"]

    # Process images with a progress bar
    for image_path_obj in tqdm(all_image_paths, desc=f"Processing images in {main_dir.name}"):

        # --- Image Loading ---
        # image_path_obj is a Path object from rglob
        image = Image.open(image_path_obj).convert("RGB") # Ensure image is RGB

        # --- CLIP Inference ---
        # Move model to GPU if available
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)

        inputs = processor(
            text=classification_labels,
            images=image,
            return_tensors="pt", # PyTorch tensors
            padding=True
        ).to(device) # Move inputs to the same device as the model

        with torch.no_grad(): # Disable gradient calculation for inference
            outputs = model(**inputs)

        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1).cpu() # Softmax and move back to CPU

        # --- Extract Results ---
        prob_animal = probs[0][0].item()     # Probability for "animal"
        prob_not_animal = probs[0][1].item() # Probability for "not animal"
        is_animal_label = 1 if prob_animal >= prob_not_animal else 0 # Binary label (1 if animal, 0 otherwise)

        # --- Get Relative Path for Joining ---
        # Calculate the relative path from the main directory (train_dir, val_dir, etc.)
        # This should match the format in your original CSVs
        relative_path = str(image_path_obj.relative_to(main_dir))
        # Ensure consistent path separators (use forward slash)
        relative_path = relative_path.replace(os.sep, '/')


        # Append results
        results.append({
            "file_path": relative_path,        # Relative path used for joining
            "animal_prob": prob_animal,
            "not_animal_prob": prob_not_animal,
            "IsAnimal?": is_animal_label       # Binary classification result
        })

    # Create DataFrame
    df_results = pd.DataFrame(results)

    # Save if path provided
    if output_csv is not None:
        # Create directory if it doesn't exist
        output_dir = Path(output_csv).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        df_results.to_csv(output_csv, index=False)
        print(f"Results saved to {output_csv}")

    return df_results
def join_dataframes(df1: pd.DataFrame, df2: pd.DataFrame, output_csv: str) -> pd.DataFrame:
    """Join two DataFrames on the 'file_path' column and save the result.

    Assumes 'file_path' in both DataFrames uses the same relative path format.

    Args:
        df1 (pd.DataFrame): The first DataFrame (e.g., original metadata).
        df2 (pd.DataFrame): The second DataFrame (e.g., CLIP classification results).
        output_csv (str): The path where the merged DataFrame will be saved.

    Returns:
        pd.DataFrame: The merged DataFrame.
    """
    # Merge the two DataFrames on the 'file_path' column
    # Use left join to keep all rows from df1 (original metadata)
    merged_df = pd.merge(df1, df2, on="file_path", how="left")

    # Fill NaN values potentially introduced by the left join in CLIP columns
    # (e.g., if an image failed processing in classify_images_in_directory)
    # You might want to decide on a default value (e.g., -1 or 0 for IsAnimal?)
    # For now, let's fill IsAnimal? with -1 to indicate missing classification
    clip_cols = ["animal_prob", "not_animal_prob", "IsAnimal?"]
    for col in clip_cols:
        if col in merged_df.columns:
            if col == "IsAnimal?":
                    merged_df[col].fillna(-1, inplace=True) # Use -1 for missing label
                    merged_df[col] = merged_df[col].astype(int) # Ensure integer type
            else:
                    merged_df[col].fillna(-1.0, inplace=True) # Use -1.0 for missing probability

    # Save the merged DataFrame
    output_dir = Path(output_csv).parent
    output_dir.mkdir(parents=True, exist_ok=True) # Ensure directory exists
    merged_df.to_csv(output_csv, index=False)
    print(f"Joined DataFrame saved to {output_csv}")

    return merged_df
# Create a directory to save the results
os.makedirs("AnimalClassification", exist_ok=True)

# Classify images in the training directory and join with the original DataFrame
train_results = classify_images_in_directory(train_dir, model, processor, "./AnimalClassification/TrainAnimalClassification.csv")
df_train_results = join_dataframes(df_train, train_results, "./AnimalClassification/TrainAnimalClassification_Joined.csv")

# Classify images in the validation directory
val_results = classify_images_in_directory(val_dir, model, processor, "./AnimalClassification/ValAnimalClassification.csv")
df_val_results = join_dataframes(df_val, val_results, "./AnimalClassification/ValAnimalClassification_Joined.csv")

# Classify images in the test directory
test_results = classify_images_in_directory(test_dir, model, processor, "./AnimalClassification/TestAnimalClassification.csv")
df_test_results = join_dataframes(df_test, test_results, "./AnimalClassification/TestAnimalClassification_Joined.csv")
Found 9586 images to process.
Processing images in train: 100%|██████████| 9586/9586 [15:33<00:00, 10.27it/s]
Results saved to ./AnimalClassification/TrainAnimalClassification.csv
Joined DataFrame saved to ./AnimalClassification/TrainAnimalClassification_Joined.csv
Found 1198 images to process.
Processing images in val: 100%|██████████| 1198/1198 [02:01<00:00,  9.87it/s]
Results saved to ./AnimalClassification/ValAnimalClassification.csv
Joined DataFrame saved to ./AnimalClassification/ValAnimalClassification_Joined.csv
Found 1199 images to process.
Processing images in test: 100%|██████████| 1199/1199 [02:00<00:00,  9.96it/s]
Results saved to ./AnimalClassification/TestAnimalClassification.csv
Joined DataFrame saved to ./AnimalClassification/TestAnimalClassification_Joined.csv
# Colors list for plotting
colors = ['#22c1c3', '#27b1dd', '#2d9cfd', '#090979']
# Plot the results
def plot_classification_results_subplot(df: pd.DataFrame, ax: plt.Axes, title: str):
    """Plot the binary classification results (IsAnimal?) on a given subplot axis.

    Args:
        df (pd.DataFrame): DataFrame containing the 'IsAnimal?' column.
        ax (plt.Axes): The matplotlib Axes object to plot on.
        title (str): The title for the subplot.
    """
    # Create the count plot on the provided axis 'ax'
    sns.countplot(data=df, x="IsAnimal?", palette=colors, ax=ax, order=[0, 1]) # Ensure order is 0 then 1

    # --- Add Percentage Labels ---
    total = len(df['IsAnimal?'])
    for p in ax.patches:
        percentage = f'{100 * p.get_height() / total:.1f}%' # Calculate percentage
        x = p.get_x() + p.get_width() / 2                    # X coordinate for the text
        y = p.get_height()                                   # Y coordinate for the text (top of the bar)
        ax.annotate(percentage, (x, y), # Annotate the bar with the percentage
                    ha='center',        # Horizontal alignment
                    va='bottom',        # Vertical alignment (place text above the bar)
                    fontsize=10,
                    fontweight='bold',
                    xytext=(0, 5),      # Offset text slightly above the bar
                    textcoords='offset points')

    # --- Aesthetics ---
    # Set clear labels for the x-axis ticks
    ax.set_xticks([0, 1]) # Ensure ticks are at 0 and 1
    ax.set_xticklabels(["Not Animal (0)", "Animal (1)"])

    ax.set_title(title, fontsize=14, fontweight='bold', y=1.08)
    ax.set_xlabel("\nPredicted Label", fontsize=12, fontweight='bold')
    ax.set_ylabel("n\n", fontsize=12, fontweight='bold')
    sns.despine(right=True, top=True, ax=ax)
        
# Plot the classification results for each DataFrame
fig, axes = plt.subplots(1, 3, figsize=(14, 5)) # 1 row, 3 columns. Adjust figsize as needed.

# Plot Training/Validation/Test Results on the first, second, and third subplots respectively
plot_classification_results_subplot(df_train_results, axes[0], "Train Classification")
plot_classification_results_subplot(df_val_results, axes[1], "Validation Classification")
plot_classification_results_subplot(df_test_results, axes[2], "Test Classification")

# Add a main title to the figure
fig.suptitle("CLIP Model Classification Results (Animal vs. Not Animal)", fontsize=16, fontweight='bold')

# Adjust layout to prevent overlapping titles/labels
plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust rect to make space for suptitle
plt.show()
No description has been provided for this image
# Plot the distribution of probabilities for each predicted label
def plot_probability_distribution_subplot(df: pd.DataFrame, ax: plt.Axes, title: str):
    """Plot the distribution of animal probabilities on a given subplot axis.

    Args:
        df (pd.DataFrame): DataFrame containing 'animal_prob' and 'IsAnimal?'.
        ax (plt.Axes): The matplotlib Axes object to plot on.
        title (str): The title for the subplot.
    """
    # Plot histogram of 'animal_prob', colored by 'IsAnimal?' on the provided axis
    sns.histplot(data=df, x="animal_prob", hue="IsAnimal?", kde=True, bins=30,
                 palette=colors, # Pass the color list
                 stat="count", alpha=0.6,
                 ax=ax,
                 hue_order=[0, 1], # Explicitly set order
                 legend=False) # Disable seaborn's automatic legend generation for now

    # --- Manual Legend Creation ---
    label_map = {0: 'Not Animal (0)', 1: 'Animal (1)'}
    hue_order = [0, 1] # Match the hue_order used in histplot

    # Create patch handles for the legend with the specified colors
    import matplotlib.patches as mpatches # Import for manual legend creation
    handles = [mpatches.Patch(color=colors[i], label=label_map[hue_val]) for i, hue_val in enumerate(hue_order)]

    # Set the legend on the axis using the manually created handles
    # Make the legend title bold using title_fontproperties
    # Source: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.legend.html#matplotlib.axes.Axes.legend
    ax.legend(handles=handles,
              title="Predicted Label",
              fontsize=10,
              title_fontproperties={'weight':'bold', 'size':11}) 

    # --- Aesthetics ---
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel("Probability of being 'Animal'", fontsize=12, fontweight='bold')
    ax.set_ylabel("n\n", fontsize=12, fontweight='bold')
    sns.despine(right=True, top=True, ax=ax)
        
# Plot the probability distribution for each DataFrame

fig, axes = plt.subplots(1, 3, figsize=(20, 6)) # 1 row, 3 columns. Adjusted figsize.

# Plot Training/Validation/Test Results on the first, second, and third subplots respectively
plot_probability_distribution_subplot(df_train_results, axes[0], "Train Probability Distribution")
plot_probability_distribution_subplot(df_val_results, axes[1], "Validation Probability Distribution")
plot_probability_distribution_subplot(df_test_results, axes[2], "Test Probability Distribution")

# Add a main title to the figure
fig.suptitle("CLIP Model Probability Distributions (Animal vs. Not Animal)", fontsize=16, fontweight='bold')

# Adjust layout to prevent overlapping titles/labels
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
No description has been provided for this image
# Frequency table of each animal classification in each family

# Group by family and IsAnimal? and count occurrences and % of "Animal"
df_class_counts = df_train_results.groupby(["family"])["IsAnimal?"].value_counts().unstack(fill_value=0)
df_class_counts.rename(columns={0: "n Not Animal", 1: "n Animal"}, inplace=True)
df_class_counts["% Animal"] = round(df_class_counts["n Animal"] / (df_class_counts["n Animal"] + df_class_counts["n Not Animal"]) * 100, 2)
df_class_counts["% Not Animal"] = round(df_class_counts["n Not Animal"] / (df_class_counts["n Animal"] + df_class_counts["n Not Animal"]) * 100, 2)
df_class_counts[["n Animal", "% Animal", "n Not Animal", "% Not Animal"]]
IsAnimal?	n Animal	% Animal	n Not Animal	% Not Animal
family				
accipitridae	90	94.74	5	5.26
acipenseridae	55	76.39	17	23.61
acroporidae	116	69.05	52	30.95
agamidae	46	95.83	2	4.17
agariciidae	71	73.96	25	26.04
...	...	...	...	...
vespertilionidae	39	81.25	9	18.75
viperidae	23	95.83	1	4.17
vireonidae	23	95.83	1	4.17
vombatidae	21	87.50	3	12.50
zonitidae	23	95.83	1	4.17
202 rows × 4 columns

# Find cases where "n Animal" < "n Not Animal"
more_notAnimal_df_class_counts = df_class_counts[df_class_counts["n Animal"] < df_class_counts["n Not Animal"]]
more_notAnimal_df_class_counts
IsAnimal?	n Not Animal	n Animal	% Animal	% Not Animal
family				
cheloniidae	18	6	25.00	75.00
delphinidae	61	35	36.46	63.54
dendrophylliidae	15	9	37.50	62.50
lamnidae	14	10	41.67	58.33
# Select file_paths of 'cheloniidae', 'delphinidae', 'dendrophylliidae' and 'lamnidae' families with 'Not Animal' classification and plot 10 of them
# Loop over each class that has more "Not Animal" than "Animal"
for cls in more_notAnimal_df_class_counts.index:
    # Filter train_results for this class and IsAnimal? == 0
    df_cls_not_animal = train_results[
        train_results["file_path"].str.contains(f"{cls}/") &
        (train_results["IsAnimal?"] == 0)
    ]
    
    # Build full Paths to the image files
    image_paths = [train_dir / rel_path for rel_path in df_cls_not_animal["file_path"]]

    # Sample up to 5 random images
    sample_paths = random.sample(image_paths, k=min(5, len(image_paths)))
        
    # Plot the sampled images
    plot_images_from_directory(sample_paths, titles=["","","","",""], num_rows=1)
No description has been provided for this image
No description has been provided for this image
No description has been provided for this image
No description has been provided for this image
🖌️ Redo SMOTE with Data Augmentation
Since we are using a zero-shot image classification model to classify the images into Animal or Not Animal, we need to redo the SMOTE with data augmentation to balance the new dataset with only Animals.

Note: The code below is the same as 2nd Notebook but with the new dataset.

Note: The code below was run with a environment with TensorFlow.

# Import custom module for importing data, visualization, and utilities
import utilities
2025-04-19 22:15:39.325279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1745097339.452820   20008 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1745097339.489457   20008 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1745097339.789621   20008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1745097339.789682   20008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1745097339.789684   20008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1745097339.789686   20008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-19 22:15:39.823557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
🐢🌿 Import Classification Results
# Load the DataFrame with classification results
df_train_results = pd.read_csv("./AnimalClassification/TrainAnimalClassification_Joined.csv")
df_val_results = pd.read_csv("./AnimalClassification/ValAnimalClassification_Joined.csv")
df_test_results = pd.read_csv("./AnimalClassification/TestAnimalClassification_Joined.csv")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 32                                     # Batch size
input_shape = (img_width, img_height, 3)            # Input shape of the model
value_range = (0.0, 1.0)                            # Range of pixel values
# Import custom module for importing data, visualization, and utilities
import utilities
# Applying Random Augmentation to handle class imbalance
# Source Code: https://medium.com/@fatimazahra.belharar/enhancing-classification-accuracy-for-imbalanced-image-data-using-smote-41737783a720
#              https://www.kaggle.com/code/souryadipstan/imbalanced-classes-image-classification-smote
# Step 1: Prepare the Augmented Training Directory

# Copy the training set folder to a new folder ('OnlyAnimal_train_DataAugmentationSMOTE') - Terminal
# !mkdir -p data/OnlyAnimal_train_DataAugmentationSMOTE
# !cp -r data/RareSpecies_Split/train/* data/OnlyAnimal_train_DataAugmentationSMOTE/
# !ls -lh data/OnlyAnimal_train_DataAugmentationSMOTE/
# Training data generator (With Data Augmentation SMOTE)
from tensorflow.keras.preprocessing import image_dataset_from_directory

onlyAnimal_train_DataAugmentationSMOTE_dir = Path("data/OnlyAnimal_train_DataAugmentationSMOTE")

# Get class names from directory (OnlyAnimal_train_DataAugmentationSMOTE)
class_names = sorted(os.listdir(onlyAnimal_train_DataAugmentationSMOTE_dir))
class_indices = {name: i for i, name in enumerate(class_names)}


train_datagen = image_dataset_from_directory(onlyAnimal_train_DataAugmentationSMOTE_dir, labels='inferred', label_mode='categorical', class_names=class_names,
                                             color_mode='rgb', batch_size=batch_size, image_size=image_size, shuffle=False, seed=2025,  
                                             interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)
print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size})")
Found 9586 files belonging to 202 classes.

Loaded: Train (9600)
I0000 00:00:1745097945.534446   20008 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
# Step 1.1: Delete all the images that are not classified as "Animal" in the training set

# Get the indices of the classes that are not classified as "Animal"
not_animal_indices = df_train_results[df_train_results["IsAnimal?"] == 0].index.tolist()

# Loop through the indices and remove the corresponding images
for index in not_animal_indices:
    # Get the class name from the index
    class_name = df_train_results.loc[index, "family"]
    # Construct the file path
    file_path = os.path.join(onlyAnimal_train_DataAugmentationSMOTE_dir, df_train_results.loc[index, "file_path"])
    
    # Remove the image file
    if os.path.exists(file_path):
        os.remove(file_path)
        # print(f"Removed: {file_path}")        # For debugging
    else:
        print(f"File not found: {file_path}")
# Step 2: Calculate Class Frequencies
class_counts = {}
for class_name in os.listdir(onlyAnimal_train_DataAugmentationSMOTE_dir):      # Count in the new dir
    class_path = onlyAnimal_train_DataAugmentationSMOTE_dir / class_name
    if class_path.is_dir():
        class_counts[class_name] = len(list(class_path.glob('*')))  # Count files

# Determine the target number of images per class (We decide to use the size of the largest class)
target_count = max(class_counts.values())
print(f"Target number of images per class (max count): {target_count}")

# Identify minority classes
minority_classes = {
    cls: count for cls, count in class_counts.items() if count < target_count
}
print(f"Found {len(minority_classes)} minority classes to augment.")
Target number of images per class (max count): 237
Found 201 minority classes to augment.
# Step 3: Define Data Augmentation Pipeline
from tensorflow.keras.models import Sequential      # To chain augmentation layers
from tensorflow.keras.layers import RandAugment     # Randomly apply augmentations to the images
# Define a sequential model for applying augmentations
data_augmentation = Sequential([
    # Apply augmentations suitable for potentially preserving species features
    RandAugment(value_range=(0, 1))                    # Randomly apply augmentations
], name="data_augmentation")
# Step 4: Generate and Save Augmented Images for Minority Classes
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img
import random

# Loop through each minority class
for class_name, current_count in tqdm(minority_classes.items(), desc="Augmenting Classes"):
    
    # Calculate the number of images to generate
    num_to_generate = target_count - current_count
    
    # Verify if the number is greater than zero
    if num_to_generate <= 0:
        continue                                        # Should not happen based on filter, but safe check
    
    # Create a directory for the augmented images of this class
    class_path = onlyAnimal_train_DataAugmentationSMOTE_dir / class_name       # Path to the class directory
    original_image_paths = list(class_path.glob('*'))               # Get list of original images for this class

    # Check if original images exist (Debugging)
    if not original_image_paths:
        print(f"Warning: No original images found for class {class_name}. Skipping.")
        continue
    
    # Generate the required number of augmented images
    for i in range(num_to_generate):
        # Randomly select an original image from this class
        original_image_path = random.choice(original_image_paths)

        try:
            # Load the original image
            img = load_img(original_image_path, target_size=image_size)
            img_array = img_to_array(img)            # Convert to numpy array
            img_array = img_array / 255.0            # Rescale to [0, 1] (important because augmentation layers expect this range)
            img_array = tf.expand_dims(img_array, 0) # Add batch dimension (1, H, W, C)

            # Apply the Augmentation Pipeline
            # Source: https://stackoverflow.com/questions/71455053/data-augmentation-layer-in-keras-sequential-model
            # Note: training=True ensures that random layers are activated
            augmented_img_array = data_augmentation(img_array, training=True)

            # Remove batch dimension and ensure correct data type/range for saving
            # Source: https://www.tensorflow.org/api_docs/python/tf/squeeze
            augmented_img_squeezed = tf.squeeze(augmented_img_array, axis=0).numpy()
            
            # Clip values to be safe, though rescaling usually handles this
            # Source: https://numpy.org/doc/stable/reference/generated/numpy.clip.html
            augmented_img_clipped = np.clip(augmented_img_squeezed * 255.0, 0, 255).astype(np.uint8)

            # Create a unique filename for the augmented image
            # Source: https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.stem
            #         https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes
            #         https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior
            original_stem = original_image_path.stem # Filename without extension
            timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S%f")          # High precision timestamp
            augmented_filename = f"{original_stem}_aug_{i+1}_{timestamp}.jpg"       # Added counter and timestamp
            save_path = class_path / augmented_filename

            # Save the augmented image
            # Source: https://www.tensorflow.org/api_docs/python/tf/keras/utils/save_img
            save_img(save_path, augmented_img_clipped)

        except Exception as e:
            print(f"\nError processing {original_image_path} for class {class_name}: {e}")
            
## Time to Execute the "SMOTE" Augmentation with OnlyAnimal dataset = 
##                                           Augmenting Classes: 100%|██████████| 200/200 [29:07<00:00,  8.74s/it]
Augmenting Classes: 100%|██████████| 201/201 [27:49<00:00,  8.30s/it]
# Verify New Counts of Augmented Images
# Source: https://docs.python.org/3/library/os.html#os.listdir
#         https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.glob
final_class_counts = {}
for class_name in os.listdir(onlyAnimal_train_DataAugmentationSMOTE_dir):
    class_path = onlyAnimal_train_DataAugmentationSMOTE_dir / class_name
    if class_path.is_dir():
        final_class_counts[class_name] = len(list(class_path.glob('*')))
     
# Display 10 classes with their counts
print("\n\033[1mSample final counts:\033[0m")
for i, (cls, count) in enumerate(final_class_counts.items()):
    print(f"- {cls}: {count}")
    
    # Print first 10
    if i >= 9: 
        break
Sample final counts:
- chordata_nesospingidae: 237
- mollusca_cardiidae: 237
- chordata_callitrichidae: 237
- chordata_glareolidae: 237
- arthropoda_lucanidae: 237
- chordata_elapidae: 237
- chordata_mimidae: 237
- chordata_indriidae: 237
- chordata_cetorhinidae: 237
- chordata_cyprinodontidae: 237
# Check if all classes now have the target count
all_balanced = all(count == target_count for count in final_class_counts.values())
print(f"\033[1mAll classes reached target count ({target_count})?\033[0m {'Yes' if all_balanced else 'No'}")
if not all_balanced:
     print("Some classes might not have reached the target count due to errors or empty original directories.")
All classes reached target count (237)? Yes
# Zip the OnlyAnimal_train_DataAugmentationSMOTE dataset
# !cd data
# !zip -r OnlyAnimal_train_DataAugmentationSMOTE.zip OnlyAnimal_train_DataAugmentationSMOTE
🔗 Bibliography/References
[1] OpenAI. (2021). openai/clip-vit-base-patch16 · Hugging Face. Huggingface.co. https://huggingface.co/openai/clip-vit-base-patch16

======================================================================================== 10_InnovativeApproachesModels_DLProject_Group37.ipynb

No description has been provided for this image
DL Project | Predicting Rare Species from Images using Deep Learning
Spring Semester | 2024 - 2025
Master in Data Science and Advanced Analytics
André Silvestre, 20240502
Diogo Duarte, 20240525
Filipa Pereira, 20240509
Maria Cruz, 20230760
Umeima Mahomed, 20240543
Group 37
📚 Libraries Import
# System imports
import os
import sys
import time
import datetime
from tqdm import tqdm
from typing_extensions import Self, Any      # For Python 3.10
# from typing import Self, Any               # For Python >3.11

from pathlib import Path

# Data manipulation imports
import numpy as np
import pandas as pd  
import warnings
warnings.filterwarnings("ignore")

# Data visualization imports
import matplotlib.pyplot as plt
import seaborn as sns

# Deep learning imports
import tensorflow as tf
from keras.ops import add
from keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model, Sequential, Input
from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D
from tensorflow.keras import regularizers                                                                           # For L2 regularization
# import visualkeras

# Evaluation imports
from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall

# Other imports
from itertools import product

# Set the style of the visualization
pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame
pd.set_option("display.max_columns", None)            # display all columns

# Disable warnings (FutureWarning)
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Set random seed for reproducibility
np.random.seed(2025)
# Creates a SSL context that does not verify the server’s certificate - Needed for downloading pretrained models
# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html
import ssl
ssl._create_default_https_context = ssl._create_unverified_context
print("TensorFlow Version:", tf.__version__)
print("Is TensorFlow built with CUDA?", tf.test.is_built_with_cuda())
print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("GPU Device Name:", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. 
                                                                                    # Runtime > Change runtime type > Hardware Accelerator)
# Get build information from TensorFlow
build_info = tf.sysconfig.get_build_info()

print("TensorFlow version:", tf.__version__)
print("Python version:", sys.version)
print("CUDA version:", build_info.get("cuda_version", "Not available"))
print("cuDNN version:", build_info.get("cudnn_version", "Not available"))
# Import custom module for importing data, visualization, and utilities
import utilities
🧮 Import Databases
🟨 Google Collab
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# # Run in Google Collab to download the dataset already splitted
# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez
# # Download the file from Google Drive using wget
# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \
#   "https://drive.usercontent.google.com/download?id=1IJILAN1uppcLl_Z4B_Qki3x-oFV13CX6&export=download" -O- | \
#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p' > /tmp/confirm.txt

# # Read the confirmation token from the temporary file
# with open('/tmp/confirm.txt', 'r') as f:
#     confirm_token = f.read().strip()

# # Download the file using the confirmation token and cookies
# !wget --load-cookies /tmp/cookies.txt \
#   "https://drive.usercontent.google.com/download?id=1IJILAN1uppcLl_Z4B_Qki3x-oFV13CX6&export=download&confirm={confirm_token}" \
#   -O /content/RareSpecies_Split.zip

# # Clean up temporary files
# !rm /tmp/cookies.txt /tmp/confirm.txt

# # List files in the /content directory to verify the download
# !ls -lh /content/

# # Unzip the downloaded file
# !unzip /content/RareSpecies_Split.zip -d /content/

# # List the unzipped files to verify
# !ls -lh /content/
# Define the path to the data
train_dir = Path("data/RareSpecies_Split/train")
val_dir = Path("data/RareSpecies_Split/val")
test_dir = Path("data/RareSpecies_Split/test")
train_SMOTE_OnlyAnimal_dir = Path("data/OnlyAnimal_train_DataAugmentationSMOTE")

# For Google Collab
# train_dir = Path("/content/RareSpecies_Split/train")
# val_dir = Path("/content/RareSpecies_Split/val")
# test_dir = Path("/content/RareSpecies_Split/test")
# train_SMOTE_OnlyAnimal_dir = Path("/content/OnlyAnimal_train_DataAugmentationSMOTE")
# Image Generators 
n_classes = 202                                     # Number of classes (we already know this based on previous notebook)
image_size = (224, 224)                             # Image size (224x224)
img_height, img_width = image_size                  # Image dimensions
batch_size = 64                                     # Batch size
input_shape = (img_height, img_width, 3)            # Input shape of the model
value_range = (0.0, 1.0)                            # Range of pixel values
# Get class names from directory
class_names = sorted(os.listdir(train_SMOTE_OnlyAnimal_dir))
class_indices = {name: i for i, name in enumerate(class_names)}

# Import the image dataset from the directory
from utilities import load_images_from_directory
train_datagen, val_datagen, test_datagen = load_images_from_directory(train_SMOTE_OnlyAnimal_dir, val_dir, test_dir,
                                                                      labels='inferred', label_mode='categorical',
                                                                      class_names=class_names, color_mode='rgb',
                                                                      batch_size=batch_size, image_size=image_size, seed=2025, 
                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)

print(f"\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), "
        f"Val ({val_datagen.cardinality().numpy() * batch_size}), "
        f"Test ({test_datagen.cardinality().numpy() * batch_size})")
# Check the shape of the data (batch_size, img_width, img_height, 3)
for x, y in train_datagen.take(1):
    print("Train (SMOTE with OnlyAnimal) batch shape:", x.shape, y.shape)
for x, y in val_datagen.take(1):
    print("Val batch shape:", x.shape, y.shape)
for x, y in test_datagen.take(1):
    print("Test batch shape:", x.shape, y.shape)

10 | Innovative Approaches - Models with only Animals



🐢🌿 Import Classification Results
# Load the DataFrame with classification results
df_train_results = pd.read_csv("./AnimalClassification/TrainAnimalClassification_Joined.csv")
df_val_results = pd.read_csv("./AnimalClassification/ValAnimalClassification_Joined.csv")
df_test_results = pd.read_csv("./AnimalClassification/TestAnimalClassification_Joined.csv")
🥇 Best Model
# Create directories for saving model checkpoints and evaluation logs
os.makedirs("./ModelCallbacks/10_ConvNeXtBaseFinalModels", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist
os.makedirs("./ModelsEvaluation/10_ConvNeXtBaseFinalModels", exist_ok=True)
FALTA POR OS PARÂMETROS DO MELHOR MODELO + Mudar nome dos modelos na Evaluation

from tensorflow.keras.applications import ConvNeXtBase
from tensorflow.keras.applications.convnext import preprocess_input

class RareSpeciesCNN_ConvNeXtBase_FinalModel(Model):
    """Custom CNN for rare species classification using ConvNeXtBase.
    
    Architecture: ConvNeXtBase 
    """
    def __init__(self, n_classes=202):
        super().__init__()                          # Call the parent class constructor

        # ----------------------------------------------------------------------------------------------------------------------------------------
        # Preprocess image for ConvNeXtBase
        # ResNet152V2 requires images to be preprocessed in a specific way (Source: https://keras.io/api/applications/convnext/#convnextbase-function)
        self.convnextbase_preprocess = Lambda(lambda x: preprocess_input(x), name='ConvNeXtBase_Preprocess')
        
        # Load ConvNeXtBase Pretrained Model
        self.convnextbase = ConvNeXtBase(include_top=False,         # Do not include the top classification layer (we will add our own because we have a different number of classes)
                                         classes=n_classes,         # Number of classes
                                         weights='imagenet')        # Use ImageNet weights
        self.convnextbase.trainable = False                         # Freeze the convolutional layers in the base model (transfer learning)
        # -----------------------------------------------------------------------------------------------------------------------------------------

        # --- Classification Head ---
        self.global_avg_pool = GlobalAveragePooling2D(name="Global_Average_Pooling")      # Global Average Pooling layer
        self.dense1 = Dense(128, name="Dense_Layer1")                                     # Smaller intermediate dense layer
        self.dropout = Dropout(0.5, name="Dropout_Layer")                                 # Dropout layer for regularization
        self.dense_output = Dense(n_classes, activation='softmax', name="Output_Layer")   # Output layer with softmax activation (for multi-class classification)

    def call(self, inputs, training=False):
        x = inputs                                     # Input layer

        # Preprocess image for ConvNeXtBase
        x = self.convnextbase_preprocess(x)              # Preprocess the image for ConvNeXtBase
        
        # Pass through ConvNeXtBase model
        x = self.convnextbase(x, training=training)      # Pass through the model (ConvNeXtBase)

        # Classification Head
        x = self.global_avg_pool(x)                      # Global Average Pooling
        x = self.dense1(x)                               # Dense layer
        x = self.dropout(x, training=training)           # Dropout layer
        outputs = self.dense_output(x)                   # Output layer
        return outputs

# Example Instantiation and Summary
model = RareSpeciesCNN_ConvNeXtBase_FinalModel(n_classes=n_classes)

# Build the model by providing an input shape
inputs = Input(shape=(img_width, img_height, 3))        # Input shape
_ = model.call(inputs)                                  # Call the model to build it
model.summary()                                         # Print the model summary
🐢🌿 Train/Validation/Test Only Animals
# Compile model
optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name="Optimizer")                                        # Adam for faster convergence
loss = CategoricalCrossentropy(name="Loss")                                                                       # Suitable for multi-class one-hot labels
metrics = [CategoricalAccuracy(name="accuracy"), Precision(name="precision"), Recall(name="recall"), F1Score(average="macro", name="f1_score"), AUC(name="auc")]
model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
# Callbacks
model_name = f"RareSpeciesCNN_ConvNeXtBase_FinalModels_{datetime.datetime.now().strftime('%Y%m%d')}" # Model name
callbacks = [ModelCheckpoint(f"./ModelCallbacks/10_ConvNeXtBaseFinalModels/checkpoint_{model_name}.keras", monitor="val_loss", save_best_only=True, verbose=0), 
             CSVLogger(f"./ModelCallbacks/10_ConvNeXtBaseFinalModels/metrics_{model_name}.csv"), 
             LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1),
             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)] # Reduce learning rate if no improvement in validation loss
# Train model
start_time = time.time()
history = model.fit(train_datagen, batch_size = batch_size, epochs=100, validation_data=val_datagen, callbacks=callbacks, verbose=1)
train_time = round(time.time() - start_time, 2)
print(f"\nTraining completed in \033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\033[0m).")
🧪 Model Selection & 📏 Model Evaluation
# Evaluate model
from utilities import plot_metrics
plot_metrics(history, file_path=f"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TrainingValidationMetrics_{model_name}.png")
# Evaluate on validation and test sets
train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}
val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)
test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)
# Display results
from utilities import display_side_by_side, create_evaluation_dataframe
results_df = create_evaluation_dataframe(
    model_name="ConvNeXtBase - Final Model", variation="SMOTE (OnlyAnimals)",
    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,
    csv_save_path= f"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TrainingValidationMetrics_{model_name}.csv"
)
display_side_by_side(results_df, super_title="Model Evaluation Results")
# Plot n right and n wrong predictions
from utilities import plot_predictions
plot_predictions(
    model=model,
    class_names=class_names,
    train_dir=train_dir,
    test_data=test_datagen,
    num_images=10,
    file_path=f"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TestPredictions_{model_name}.png",
)
🟨 Google Collab
# # Save Google Collab Workspace
# # Source: https://stackoverflow.com/questions/48774285/how-to-download-file-created-in-colaboratory-workspace

# # Zip the directories
# !zip -r /content/ModelCallbacks.zip /content/ModelCallbacks
# !zip -r /content/ModelsEvaluation.zip /content/ModelsEvaluation

# # Download the zip files
# from google.colab import files
# files.download("/content/ModelCallbacks.zip")
# files.download("/content/ModelsEvaluation.zip")
🔗 Bibliography/References
[1] OpenAI. (2021). openai/clip-vit-base-patch16 · Hugging Face. Huggingface.co. https://huggingface.co/openai/clip-vit-base-patch16

[2] Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., Xie, S., Facebook, A., & Research. (2022). A ConvNet for the 2020s. https://arxiv.org/pdf/2201.03545

============================================

MELHORA TUDO O Q ME DESTE COM O QUE ACHAS QUE FALTA E CONFIRMA TUDO. DIZ-ME SÓ O QUE ALTERASTE!
DEPOIS FAZ OS ANNEXS QUE FALTA. SE ACHARES MELHOR POR NUMA TABELA OS DIFERENTES MODELOS PRETREINADOS OU ESCREVE UM TEXTO CORRIDO COM A SUA DESCRIÇÃO BEM ESCRITA CUMPRINDO SEMPRE AS COISAS QUE TE DISSE ANTERIROREMENTE.

EXPLICA O QUE AINDA FALTAR E DIMINUI O TEXTO QUE CONSEGUIRES.