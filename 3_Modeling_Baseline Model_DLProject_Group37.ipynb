{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1834cc3d7b582eb2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; flex-wrap: wrap;\">\n",
    "    <div style=\"flex: 1; max-width: 400px; display: flex; justify-content: center;\">\n",
    "        <img src=\"https://i.ibb.co/JBPWVYR/Logo-Nova-IMS-Black.png\" style=\"max-width: 50%; height: auto; margin-top: 50px; margin-bottom: 50px;margin-left: 6rem;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 2; text-align: center; margin-top: 20px;margin-left: 8rem;\">\n",
    "        <div style=\"font-size: 28px; font-weight: bold; line-height: 1.2;\">\n",
    "            <span style=\"color: #22c1c3;\">DL Project |</span> <span style=\"color: #08529C;\">Predicting Rare Species from Images using Deep Learning</span>\n",
    "        </div>\n",
    "        <div style=\"font-size: 17px; font-weight: bold; margin-top: 10px;\">\n",
    "            Spring Semester | 2024 - 2025\n",
    "        </div>\n",
    "        <div style=\"font-size: 17px; font-weight: bold;\">\n",
    "            Master in Data Science and Advanced Analytics\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px;\">\n",
    "            <div>Andr√© Silvestre, 20240502</div>\n",
    "            <div>Diogo Duarte, 20240525</div>\n",
    "            <div>Filipa Pereira, 20240509</div>\n",
    "            <div>Maria Cruz, 20230760</div>\n",
    "            <div>Umeima Mahomed, 20240543</div>\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px; font-weight: bold;\">\n",
    "            Group 37\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c9197",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 1px; color: white; border-radius: 500px; text-align: center;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f603cf1cb0fd531",
   "metadata": {},
   "source": [
    "## **üìö Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self, Any      # For Python 3.10\n",
    "# from typing import Self, Any               # For Python >3.11\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from keras.ops import add\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda\n",
    "import visualkeras\n",
    "\n",
    "# Evaluation imports\n",
    "from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall\n",
    "\n",
    "# Other imports\n",
    "from itertools import product\n",
    "\n",
    "# Set the style of the visualization\n",
    "pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)            # display all columns\n",
    "\n",
    "# Disable warnings (FutureWarning)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Is TensorFlow built with CUDA?\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"GPU Device Name:\", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. \n",
    "                                                                                    # Runtime > Change runtime type > Hardware Accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d96104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra: https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
    "# If you‚Äôre using a GPU, TensorFlow might pre-allocate GPU memory, leaving less for CPU operations. \n",
    "# Enabling memory growth lets the GPU allocate only what‚Äôs needed.\n",
    "if tf.test.is_built_with_cuda():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom module for importing data, visualization, and utilities\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb366e4",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 1px; color: white; border-radius: 500px; text-align: center;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d91a720daf9a2",
   "metadata": {},
   "source": [
    "## **üßÆ Import Databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run in Google Collab to download the dataset already splitted\n",
    "# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez\n",
    "# # Download the file from Google Drive using wget\n",
    "# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=11vkRJLP-re8E-8DWaoKeSuG66u64ez0J&export=download\" -O- | \\\n",
    "#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p' > /tmp/confirm.txt\n",
    "\n",
    "# # Read the confirmation token from the temporary file\n",
    "# with open('/tmp/confirm.txt', 'r') as f:\n",
    "#     confirm_token = f.read().strip()\n",
    "\n",
    "# # Download the file using the confirmation token and cookies\n",
    "# !wget --load-cookies /tmp/cookies.txt \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=11vkRJLP-re8E-8DWaoKeSuG66u64ez0J&export=download&confirm={confirm_token}\" \\\n",
    "#   -O /content/RareSpecies_Split.zip\n",
    "\n",
    "# # Clean up temporary files\n",
    "# !rm /tmp/cookies.txt /tmp/confirm.txt\n",
    "\n",
    "# # List files in the /content directory to verify the download\n",
    "# !ls -lh /content/\n",
    "\n",
    "# # Unzip the downloaded file\n",
    "# !unzip /content/RareSpecies_Split.zip -d /content/\n",
    "\n",
    "# # List the unzipped files to verify\n",
    "# !ls -lh /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87da922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data\n",
    "train_dir = Path(\"data/RareSpecies_Split/train\")\n",
    "val_dir = Path(\"data/RareSpecies_Split/val\")\n",
    "test_dir = Path(\"data/RareSpecies_Split/test\")\n",
    "\n",
    "# For Google Collab\n",
    "# train_dir = Path(\"/content/RareSpecies_Split/train\")\n",
    "# val_dir = Path(\"/content/RareSpecies_Split/val\")\n",
    "# test_dir = Path(\"/content/RareSpecies_Split/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Generators \n",
    "n_classes = 202                                     # Number of classes (we already know this based on previous notebook)\n",
    "# image_size = (224, 224)                             # Image size (224x224)\n",
    "image_size = (128, 128)                             # Image size (128x128)\n",
    "img_height, img_width = image_size                  # Image dimensions\n",
    "batch_size = 8                                     # Batch size\n",
    "input_shape = (img_height, img_width, 3)            # Input shape of the model\n",
    "value_range = (0.0, 1.0)                            # Range of pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6686885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names from directory\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Import the image dataset from the directory\n",
    "from utilities import load_images_from_directory\n",
    "train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,\n",
    "                                                                      labels='inferred', label_mode='categorical', \n",
    "                                                                      class_names=class_names, color_mode='rgb',\n",
    "                                                                      batch_size=batch_size, image_size=image_size, seed=2025, \n",
    "                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)\n",
    "\n",
    "print(f\"\\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), \"\n",
    "        f\"Val ({val_datagen.cardinality().numpy() * batch_size}), \"\n",
    "        f\"Test ({test_datagen.cardinality().numpy() * batch_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data (batch_size, img_width, img_height, 3)\n",
    "for x, y in train_datagen.take(1):\n",
    "    print(\"Train batch shape:\", x.shape, y.shape)\n",
    "for x, y in val_datagen.take(1):\n",
    "    print(\"Val batch shape:\", x.shape, y.shape)\n",
    "for x, y in test_datagen.take(1):\n",
    "    print(\"Test batch shape:\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f158b792ebd291",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='3'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 140px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Avenir Next LT Pro', sans-serif;\">\n",
    "        <b>3 | Modeling - Baseline Model</b></h1></center>\n",
    "</div>\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26312ae5b0368022",
   "metadata": {},
   "source": [
    "# **üí° Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3556121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "class RareSpeciesCNN(Model):\n",
    "    \"\"\"Custom CNN for rare species classification.\n",
    "    \n",
    "    Architecture: Simple CNN \n",
    "    Why: Small model to establish baseline, avoiding overfitting on 202 classes.\n",
    "    Alternatives: Deeper CNNs (e.g., ResNet) or transfer learning (e.g., EfficientNet).\n",
    "    Allows selection of preprocessing steps like grayscale, contrast, and saturation adjustment.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=202, \n",
    "                 apply_grayscale=False, \n",
    "                 apply_contrast=False, contrast_factor=1.5,\n",
    "                 apply_saturation=False, saturation_factor=1.5):\n",
    "        \"\"\"Initializes the model.\n",
    "        \n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            apply_grayscale (bool): If True, convert images to grayscale.\n",
    "            apply_contrast (bool): If True, adjust image contrast.\n",
    "            contrast_factor (float): Factor to adjust contrast by (if apply_contrast is True).\n",
    "            apply_saturation (bool): If True, adjust image saturation.\n",
    "            saturation_factor (float): Factor to adjust saturation by (if apply_saturation is True).\n",
    "        \"\"\"\n",
    "        super().__init__() # Call the parent class constructor\n",
    "        \n",
    "        # Store preprocessing flags and factors\n",
    "        self.apply_grayscale = apply_grayscale\n",
    "        self.apply_contrast = apply_contrast\n",
    "        self.apply_saturation = apply_saturation\n",
    "        \n",
    "        # --- Preprocessing Layers ---        \n",
    "        # Rescaling layer (always applied)\n",
    "        self.rescale_layer = Rescaling(scale= 1 / 255.0, name=\"Rescale_Layer\")    # Rescales pixel values to [0, 1]\n",
    "        \n",
    "        # Conditionally define Lambda layer for contrast adjustment\n",
    "        if self.apply_contrast:\n",
    "            # Define Lambda layer for contrast adjustment\n",
    "            # Source: https://keras.io/api/layers/core_layers/lambda/\n",
    "            #         https://www.tensorflow.org/api_docs/python/tf/image/adjust_contrast\n",
    "            #         contrast_factor > 1 increases contrast, < 1 decreases contrast\n",
    "            self.contrast_layer = Lambda(\n",
    "                lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor),\n",
    "                name='Adjust_Contrast'\n",
    "            )\n",
    "        \n",
    "        # Conditionally define Lambda layer for saturation adjustment\n",
    "        if self.apply_saturation:\n",
    "            # Define Lambda layer for saturation adjustment\n",
    "            # Source: https://www.tensorflow.org/api_docs/python/tf/image/adjust_saturation\n",
    "            #         saturation_factor > 1 increases saturation, < 1 decreases saturation\n",
    "            self.saturation_layer = Lambda(\n",
    "                lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor),\n",
    "                name='Adjust_Saturation'\n",
    "            )\n",
    "            \n",
    "        # Conditionally define Lambda layer for grayscale conversion\n",
    "        if self.apply_grayscale:\n",
    "            # Define Lambda layer for grayscale conversion\n",
    "            # Source: https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_grayscale\n",
    "            self.grayscale_layer = Lambda(\n",
    "                lambda x: tf.image.rgb_to_grayscale(x), \n",
    "                name='RGB_to_Grayscale'\n",
    "            )\n",
    "            # IMPORTANT: Add a Conv2D layer immediately after grayscale to ensure \n",
    "            # the number of channels is compatible with subsequent layers \n",
    "            # if they expect 3 channels. Here, we'll keep it 1 channel and adjust conv1.\n",
    "            # Alternatively, convert grayscale back to 3 identical channels:\n",
    "            # self.grayscale_to_rgb_layer = Lambda(\n",
    "            #     lambda x: tf.image.grayscale_to_rgb(x),\n",
    "            #     name='Grayscale_to_RGB'\n",
    "            # )\n",
    "            \n",
    "            \n",
    "        # --- Convolutional Layers ---\n",
    "        # Adjust the first Conv layer's input channels if grayscale is applied and not converted back to RGB\n",
    "        # If grayscale IS applied, the input to conv1 will have 1 channel.\n",
    "        # If grayscale IS NOT applied, the input will have 3 channels (after rescaling).\n",
    "        # We will handle this by checking the shape dynamically or assuming subsequent layers can handle 1 channel if needed.\n",
    "        # For simplicity here, let's assume conv1 works with either 1 or 3 channels.\n",
    "        # If grayscale is applied, the input depth is 1, otherwise 3.\n",
    "        # A more robust way might involve explicitly setting input_shape or checking channels.\n",
    "        # Let's define conv1 to work even if input is grayscale (1 channel)\n",
    "        \n",
    "        # Source: https://stackoverflow.com/questions/60157742/convolutional-neural-network-cnn-input-shape/61075207#61075207 (Explain Conv2D)\n",
    "        self.conv1 = Conv2D(filters=3*8, kernel_size=(3, 3), activation='relu', name=\"Conv_Layer1\", padding=\"same\")    # 24 filters\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2), name=\"Max_Pool_Layer1\")                                            # Reduces spatial dimensions by half\n",
    "        \n",
    "        # Subsequent layers\n",
    "        self.conv2l = Conv2D(filters=3*16, kernel_size=(3, 3), activation='relu', name=\"Conv_Layer2l\", padding=\"same\") # 48 filters\n",
    "        self.conv2r = Conv2D(filters=3*16, kernel_size=(3, 3), activation='relu', name=\"Conv_Layer2r\", padding=\"same\") # 48 filters (parallel path example)\n",
    "        # Need to combine conv2l and conv2r, e.g., by concatenation or addition before pooling\n",
    "        # For simplicity, let's just use one path for now:\n",
    "        self.conv2 = Conv2D(filters=3*16, kernel_size=(3, 3), activation='relu', name=\"Conv_Layer2\", padding=\"same\") # 48 filters\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2, 2), name=\"MaxPool_Layer2\")                                            # Further reduces spatial dimensions\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.flatten = Flatten(name=\"Flatten_Layer\")                                  # Flattens the output for the dense layer\n",
    "        self.dropout = Dropout(0.5, name=\"Dropout_Layer\")                             # Applies dropout regularization\n",
    "        self.dense = Dense(n_classes, activation='softmax', name=\"Output_Layer\")      # Outputs probabilities for n_classes\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Defines the forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch of images).\n",
    "            training (bool): Indicates if the model is in training mode (for Dropout).\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (probabilities for each class).\n",
    "        \"\"\"\n",
    "        # Apply mandatory rescaling\n",
    "        x = self.rescale_layer(inputs)\n",
    "        \n",
    "        # Apply conditional preprocessing layers\n",
    "        if self.apply_contrast:\n",
    "            x = self.contrast_layer(x)\n",
    "        if self.apply_saturation:\n",
    "            x = self.saturation_layer(x)\n",
    "        if self.apply_grayscale:\n",
    "            x = self.grayscale_layer(x)\n",
    "            # If subsequent layers strictly require 3 channels, uncomment this:\n",
    "            # x = self.grayscale_to_rgb_layer(x) \n",
    "            # Note: If grayscale is applied, conv1 will process a 1-channel input unless converted back.\n",
    "\n",
    "        # Pass through convolutional blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Using the simplified single path for conv2\n",
    "        x = self.conv2(x) \n",
    "        \n",
    "        # If using parallel conv2l and conv2r, you would need to combine them here, e.g.:\n",
    "        left_path = self.conv2l(x)\n",
    "        right_path = self.conv2r(x)\n",
    "        x = tf.keras.layers.concatenate([left_path, right_path], axis=-1) # Combine features\n",
    "        \n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Flatten and pass through the classification head\n",
    "        x = self.flatten(x)\n",
    "        # Apply dropout only during training\n",
    "        x = self.dropout(x, training=training) \n",
    "        outputs = self.dense(x)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Example Instantiation and Summary\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=True, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "# Source: https://www.kaggle.com/code/devsubhash/visualize-deep-learning-models-using-visualkeras\n",
    "visualkeras.layered_view(model, \n",
    "                         legend=True, \n",
    "                         show_dimension=True,\n",
    "                         scale_xy=1,                                        # Adjust the scale of the image\n",
    "                        #  scale_z=1,\n",
    "                         # to_file='./BaselineModel_Architecture.png',\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391168a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optimizer = SGD(learning_rate=0.1, name=\"Optimizer\")                                                       # SGD with decay for stability\n",
    "# optimizer = Adam(learning_rate=0.0001, name=\"Optimizer\")                                                   # Adam for faster convergence\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=0.0, amsgrad=False, name=\"Optimizer\")  # Adam \n",
    "\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                            # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), \n",
    "           Precision(name=\"precision\"),\n",
    "           Recall(name=\"recall\"), \n",
    "           F1Score(average=\"macro\", name=\"f1_score\"),\n",
    "           AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed55ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for saving the model and logs\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}\"                                                                             # Model name \n",
    "print(f\"\\n\\033[1mModel name:\\033[0m {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "# Create a directory for saving the model and logs\n",
    "os.makedirs(\"./ModelCallbacks\", exist_ok=True)      # Create directory if it doesn't exist\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}\"                                                                             # Model name \n",
    "callbacks = [\n",
    "    ModelCheckpoint(f\"./ModelCallbacks/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0),       # Save best model\n",
    "    CSVLogger(f\"./ModelCallbacks/metrics_{model_name}.csv\"),                                                                      # Log training metrics\n",
    "    LearningRateScheduler(lambda epoch, lr: lr * 0.95),                                                                           # Exponential decay for learning rate\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1)                                                                      # Stop training when the validation loss stops improving\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f48a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d0768",
   "metadata": {},
   "source": [
    "### **Original Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ddcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\033[1mBatch size:\\033[0m {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ae37e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "**Alterei o Batch Size para 16, pois o modelo estava a dar erro de mem√≥ria.**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=5, validation_data=val_datagen, callbacks=callbacks, verbose=2)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204995d589f8431f",
   "metadata": {},
   "source": [
    "#### **üß™ Model Selection & üìè Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "\n",
    "os.makedirs(\"./ModelsEvaluation\", exist_ok=True)      # Create directory if it doesn't exist\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/2_Training_Validation_Metrics_{datetime.datetime.now().strftime('%Y%m%d')}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=0)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\",\n",
    "    variation=\"Default\",\n",
    "    train_metrics=train_results,\n",
    "    val_metrics=val_results,\n",
    "    test_metrics=test_results,\n",
    "    train_time=train_time\n",
    ")\n",
    "\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77537d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea3f13",
   "metadata": {},
   "source": [
    "## **üìä Best Model - Predictions Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import plot_confusion_matrix\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "plot_confusion_matrix(\n",
    "    y_true=test_datagen.classes,\n",
    "    y_pred=model.predict(test_datagen, batch_size=batch_size),\n",
    "    title=\"Confusion Matrix | Best Baseline Model\",\n",
    "    # file_path=\"./ModelsEvaluation/3_Test_Confusion_Matrix.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5 right and 5 wrong predictions\n",
    "from utilities import plot_predictions\n",
    "plot_predictions(\n",
    "    model=model,\n",
    "    data=test_datagen,\n",
    "    n_samples=5,\n",
    "    file_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b849b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save to CSV\n",
    "# results_df.set_index('Models', inplace=True)\n",
    "# results_df.to_csv(\"ModelsEvaluation/BaselineModelEvaluation_1_29.03.2025.csv\", index=False)                ### Change the name of the file to save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b90a90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61c8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a3689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dff3bdf66c1f233",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5816b",
   "metadata": {},
   "source": [
    "# **üîó Bibliography/References**\n",
    "\n",
    "**[[1]](https://)** AAAAAAAAAA\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf218",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
