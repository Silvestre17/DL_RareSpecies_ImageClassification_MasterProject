{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1834cc3d7b582eb2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; flex-wrap: wrap;\">\n",
    "    <div style=\"flex: 1; max-width: 400px; display: flex; justify-content: center;\">\n",
    "        <img src=\"https://i.ibb.co/JBPWVYR/Logo-Nova-IMS-Black.png\" style=\"max-width: 50%; height: auto; margin-top: 50px; margin-bottom: 50px;margin-left: 6rem;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 2; text-align: center; margin-top: 20px;margin-left: 8rem;\">\n",
    "        <div style=\"font-size: 28px; font-weight: bold; line-height: 1.2;\">\n",
    "            <span style=\"color: #22c1c3;\">DL Project |</span> <span style=\"color: #08529C;\">Predicting Rare Species from Images using Deep Learning</span>\n",
    "        </div>\n",
    "        <div style=\"font-size: 17px; font-weight: bold; margin-top: 10px;\">\n",
    "            Spring Semester | 2024 - 2025\n",
    "        </div>\n",
    "        <div style=\"font-size: 17px; font-weight: bold;\">\n",
    "            Master in Data Science and Advanced Analytics\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px;\">\n",
    "            <div>André Silvestre, 20240502</div>\n",
    "            <div>Diogo Duarte, 20240525</div>\n",
    "            <div>Filipa Pereira, 20240509</div>\n",
    "            <div>Maria Cruz, 20230760</div>\n",
    "            <div>Umeima Mahomed, 20240543</div>\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px; font-weight: bold;\">\n",
    "            Group 37\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c9197",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 1px; color: white; border-radius: 500px; text-align: center;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f603cf1cb0fd531",
   "metadata": {},
   "source": [
    "## **📚 Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self, Any      # For Python 3.10\n",
    "# from typing import Self, Any               # For Python >3.11\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from keras.ops import add\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers                                                                           # For L2 regularization\n",
    "import visualkeras\n",
    "\n",
    "# Evaluation imports\n",
    "from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall\n",
    "\n",
    "# Other imports\n",
    "from itertools import product\n",
    "\n",
    "# Set the style of the visualization\n",
    "pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)            # display all columns\n",
    "\n",
    "# Disable warnings (FutureWarning)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0d56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Is TensorFlow built with CUDA?\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"GPU Device Name:\", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. \n",
    "                                                                                    # Runtime > Change runtime type > Hardware Accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get build information from TensorFlow\n",
    "build_info = tf.sysconfig.get_build_info()\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"CUDA version:\", build_info.get(\"cuda_version\", \"Not available\"))\n",
    "print(\"cuDNN version:\", build_info.get(\"cudnn_version\", \"Not available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d96104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra: https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
    "# If you’re using a GPU, TensorFlow might pre-allocate GPU memory, leaving less for CPU operations. \n",
    "# Enabling memory growth lets the GPU allocate only what’s needed.\n",
    "if tf.test.is_built_with_cuda():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom module for importing data, visualization, and utilities\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb366e4",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 1px; color: white; border-radius: 500px; text-align: center;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d91a720daf9a2",
   "metadata": {},
   "source": [
    "## **🧮 Import Databases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run in Google Collab to download the dataset already splitted\n",
    "# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez\n",
    "# # Download the file from Google Drive using wget\n",
    "# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download\" -O- | \\\n",
    "#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p' > /tmp/confirm.txt\n",
    "\n",
    "# # Read the confirmation token from the temporary file\n",
    "# with open('/tmp/confirm.txt', 'r') as f:\n",
    "#     confirm_token = f.read().strip()\n",
    "\n",
    "# # Download the file using the confirmation token and cookies\n",
    "# !wget --load-cookies /tmp/cookies.txt \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}\" \\\n",
    "#   -O /content/RareSpecies_Split.zip\n",
    "\n",
    "# # Clean up temporary files\n",
    "# !rm /tmp/cookies.txt /tmp/confirm.txt\n",
    "\n",
    "# # List files in the /content directory to verify the download\n",
    "# !ls -lh /content/\n",
    "\n",
    "# # Unzip the downloaded file\n",
    "# !unzip /content/RareSpecies_Split.zip -d /content/\n",
    "\n",
    "# # List the unzipped files to verify\n",
    "# !ls -lh /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87da922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data\n",
    "train_dir = Path(\"data/RareSpecies_Split/train\")\n",
    "val_dir = Path(\"data/RareSpecies_Split/val\")\n",
    "test_dir = Path(\"data/RareSpecies_Split/test\")\n",
    "\n",
    "# For Google Collab\n",
    "# train_dir = Path(\"/content/RareSpecies_Split/train\")\n",
    "# val_dir = Path(\"/content/RareSpecies_Split/val\")\n",
    "# test_dir = Path(\"/content/RareSpecies_Split/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Generators \n",
    "n_classes = 202                                     # Number of classes (we already know this based on previous notebook)\n",
    "image_size = (224, 224)                             # Image size (224x224)\n",
    "img_height, img_width = image_size                  # Image dimensions\n",
    "batch_size = 64                                     # Batch size\n",
    "input_shape = (img_height, img_width, 3)            # Input shape of the model\n",
    "value_range = (0.0, 1.0)                            # Range of pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6686885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names from directory\n",
    "class_names = sorted(os.listdir(train_dir))\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Import the image dataset from the directory\n",
    "from utilities import load_images_from_directory\n",
    "train_datagen, val_datagen, test_datagen = load_images_from_directory(train_dir, val_dir, test_dir,\n",
    "                                                                      labels='inferred', label_mode='categorical',\n",
    "                                                                      class_names=class_names, color_mode='rgb',\n",
    "                                                                      batch_size=batch_size, image_size=image_size, seed=2025, \n",
    "                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)\n",
    "\n",
    "print(f\"\\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), \"\n",
    "        f\"Val ({val_datagen.cardinality().numpy() * batch_size}), \"\n",
    "        f\"Test ({test_datagen.cardinality().numpy() * batch_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data (batch_size, img_width, img_height, 3)\n",
    "for x, y in train_datagen.take(1):\n",
    "    print(\"Train batch shape:\", x.shape, y.shape)\n",
    "for x, y in val_datagen.take(1):\n",
    "    print(\"Val batch shape:\", x.shape, y.shape)\n",
    "for x, y in test_datagen.take(1):\n",
    "    print(\"Test batch shape:\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f158b792ebd291",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='3'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 140px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Avenir Next LT Pro', sans-serif;\">\n",
    "        <b>3 | Modeling - Baseline Model</b></h1></center>\n",
    "</div>\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26312ae5b0368022",
   "metadata": {},
   "source": [
    "# **💡 Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving model checkpoints and evaluation logs\n",
    "os.makedirs(\"./ModelCallbacks/3_BaselineModel_10epochs\", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist\n",
    "os.makedirs(\"./ModelsEvaluation/3_BaselineModel_10epochs\", exist_ok=True)\n",
    "\n",
    "os.makedirs(\"./ModelCallbacks/3_BaselineModel\", exist_ok=True)\n",
    "os.makedirs(\"./ModelsEvaluation/3_BaselineModel\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3556121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model\n",
    "class RareSpeciesCNN(Model):\n",
    "    \"\"\"Custom CNN for rare species classification.\n",
    "    \n",
    "    Architecture: Simple CNN \n",
    "    Why: Small model to establish baseline, avoiding overfitting on 202 classes.\n",
    "    Alternatives: Deeper CNNs (e.g., ResNet) or transfer learning (e.g., EfficientNet).\n",
    "    Allows selection of preprocessing steps like grayscale, contrast, and saturation adjustment.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=202,                                   # Number of output classes\n",
    "                 apply_grayscale=False,                                 # If True, convert images to grayscale\n",
    "                 apply_contrast=False, contrast_factor=1.5,             # 1.5 = increase contrast\n",
    "                 apply_saturation=False, saturation_factor=1.5):        # 1.5 = increase saturation\n",
    "        \"\"\"Initializes the model.\n",
    "        \n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            apply_grayscale (bool): If True, convert images to grayscale first.\n",
    "            apply_contrast (bool): If True, adjust image contrast.\n",
    "            contrast_factor (float): Factor for contrast adjustment.\n",
    "            apply_saturation (bool): If True, adjust image saturation.\n",
    "            saturation_factor (float): Factor for saturation adjustment.\n",
    "        \"\"\"\n",
    "        super().__init__() # Call the parent class constructor\n",
    "        \n",
    "        # Store preprocessing flags and factors\n",
    "        self.apply_grayscale = apply_grayscale\n",
    "        self.apply_contrast = apply_contrast\n",
    "        self.apply_saturation = apply_saturation\n",
    "        \n",
    "        # --- Preprocessing Layers ---        \n",
    "        # Rescaling layer (always applied)\n",
    "        self.rescale_layer = Rescaling(scale= 1 / 255.0, name=\"Rescale_Layer\")    # Rescales pixel values to [0, 1]\n",
    "        \n",
    "        # Conditionally define Lambda layer for contrast adjustment\n",
    "        if self.apply_contrast:\n",
    "            # Define Lambda layer for contrast adjustment\n",
    "            # Source: https://keras.io/api/layers/core_layers/lambda/\n",
    "            #         https://www.tensorflow.org/api_docs/python/tf/image/adjust_contrast\n",
    "            #         contrast_factor > 1 increases contrast, < 1 decreases contrast\n",
    "            self.contrast_layer = Lambda(\n",
    "                lambda x: tf.image.adjust_contrast(x, contrast_factor=contrast_factor),\n",
    "                name='Adjust_Contrast'\n",
    "            )\n",
    "        \n",
    "        # Conditionally define Lambda layer for saturation adjustment\n",
    "        if self.apply_saturation:\n",
    "            # Define Lambda layer for saturation adjustment\n",
    "            # Source: https://www.tensorflow.org/api_docs/python/tf/image/adjust_saturation\n",
    "            #         saturation_factor > 1 increases saturation, < 1 decreases saturation\n",
    "            self.saturation_layer = Lambda(\n",
    "                lambda x: tf.image.adjust_saturation(x, saturation_factor=saturation_factor),\n",
    "                name='Adjust_Saturation'\n",
    "            )\n",
    "            \n",
    "        # Conditionally define Lambda layer for grayscale conversion\n",
    "        if self.apply_grayscale:\n",
    "            # Define Lambda layer for grayscale conversion\n",
    "            # Source: https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_grayscale\n",
    "            self.grayscale_layer = Lambda(\n",
    "                lambda x: tf.image.rgb_to_grayscale(x), \n",
    "                name='RGB_to_Grayscale'\n",
    "            )\n",
    "            # IMPORTANT: Add a Conv2D layer immediately after grayscale to ensure \n",
    "            #            the number of channels is compatible with subsequent layers \n",
    "            #            if they expect 3 channels. Here, we'll keep it 1 channel and adjust conv1.\n",
    "            # Alternatively, convert grayscale back to 3 identical channels:\n",
    "            # self.grayscale_to_rgb_layer = Lambda(\n",
    "            #     lambda x: tf.image.grayscale_to_rgb(x),\n",
    "            #     name='Grayscale_to_RGB'\n",
    "            # )\n",
    "            \n",
    "            \n",
    "        # --- Convolutional Layers ---\n",
    "        # Adjust the first Conv layer's input channels if grayscale is applied and not converted back to RGB\n",
    "        # If grayscale IS applied, the input to conv1 will have 1 channel.\n",
    "        # If grayscale IS NOT applied, the input will have 3 channels (after rescaling).\n",
    "        # We will handle this by checking the shape dynamically or assuming subsequent layers can handle 1 channel if needed.\n",
    "        # For simplicity here, let's assume conv1 works with either 1 or 3 channels.\n",
    "        # If grayscale is applied, the input depth is 1, otherwise 3.\n",
    "        # A more robust way might involve explicitly setting input_shape or checking channels.\n",
    "        # Let's define conv1 to work even if input is grayscale (1 channel)\n",
    "\n",
    "        # Block 1\n",
    "        # Source: https://stackoverflow.com/questions/60157742/convolutional-neural-network-cnn-input-shape/61075207#61075207 (Explain Conv2D)\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=(3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001), name=\"Conv_Block1_Conv\")\n",
    "        self.bn1 = BatchNormalization(name=\"Conv_Block1_BN\")\n",
    "        self.act1 = Activation('relu', name=\"Conv_Block1_Act\")\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2, 2), name=\"Conv_Block1_Pool\") # Output: 112x112x32\n",
    "\n",
    "        # Block 2\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=(3, 3), padding='same',kernel_regularizer=regularizers.l2(0.001), name=\"Conv_Block2_Conv\")\n",
    "        self.bn2 = BatchNormalization(name=\"Conv_Block2_BN\")\n",
    "        self.act2 = Activation('relu', name=\"Conv_Block2_Act\")\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2, 2), name=\"Conv_Block2_Pool\") # Output: 56x56x64\n",
    "\n",
    "        # Block 3\n",
    "        self.conv3 = Conv2D(filters=128, kernel_size=(3, 3), padding='same',kernel_regularizer=regularizers.l2(0.001), name=\"Conv_Block3_Conv\")\n",
    "        self.bn3 = BatchNormalization(name=\"Conv_Block3_BN\")\n",
    "        self.act3 = Activation('relu', name=\"Conv_Block3_Act\")\n",
    "        self.pool3 = MaxPooling2D(pool_size=(2, 2), name=\"Conv_Block3_Pool\") # Output: 28x28x128\n",
    "\n",
    "        ### Block 4\n",
    "        self.conv4 = Conv2D(filters=256, kernel_size=(3, 3), padding='same',kernel_regularizer=regularizers.l2(0.001), name=\"Conv_Block4_Conv\")\n",
    "        self.bn4 = BatchNormalization(name=\"Conv_Block4_BN\")\n",
    "        self.act4 = Activation('relu', name=\"Conv_Block4_Act\")\n",
    "        self.pool4 = MaxPooling2D(pool_size=(2, 2), name=\"Conv_Block4_Pool\") # Output: 14x14x256\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.global_avg_pool = GlobalAveragePooling2D(name=\"Global_Average_Pooling\")\n",
    "        self.dense1 = Dense(128, name=\"Dense_Layer1\")                                     # Smaller intermediate dense layer\n",
    "        self.bn_dense1 = BatchNormalization(name=\"Dense_Layer1_BN\")\n",
    "        self.act_dense1 = Activation('relu', name=\"Dense_Layer1_Act\")\n",
    "        self.dropout = Dropout(0.5, name=\"Dropout_Layer\")\n",
    "        self.dense_output = Dense(n_classes, activation='softmax', name=\"Output_Layer\")\n",
    "\n",
    "\n",
    "    def call(self: Self, inputs: Any, training:bool=False) -> Any:\n",
    "        \"\"\"Defines the forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input tensor (batch of images).\n",
    "            training (bool): Indicates if the model is in training mode (for Dropout).\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor (probabilities for each class).\n",
    "        \"\"\"\n",
    "        # Apply mandatory rescaling\n",
    "        x = self.rescale_layer(inputs)\n",
    "\n",
    "        # Apply conditional preprocessing layers\n",
    "        if self.apply_contrast:\n",
    "            x = self.contrast_layer(x)\n",
    "        if self.apply_saturation:\n",
    "            x = self.saturation_layer(x)\n",
    "        if self.apply_grayscale:\n",
    "            x = self.grayscale_layer(x)\n",
    "            # If subsequent layers strictly require 3 channels, uncomment this:\n",
    "            # x = self.grayscale_to_rgb_layer(x)\n",
    "            # Note: If grayscale is applied, conv1 will process a 1-channel input unless converted back.\n",
    "\n",
    "        # Conv Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Conv Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.act2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Conv Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        x = self.act3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Conv Block 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x, training=training)\n",
    "        x = self.act4(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Classification Head\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn_dense1(x, training=training)\n",
    "        x = self.act_dense1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        outputs = self.dense_output(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Example Instantiation and Summary\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False,\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "# Source: https://www.kaggle.com/code/devsubhash/visualize-deep-learning-models-using-visualkeras\n",
    "visualkeras.layered_view(model,\n",
    "                         legend=True,\n",
    "                         show_dimension=True,\n",
    "                         scale_xy=1,                                        # Adjust the scale of the image\n",
    "                         # scale_z=1,\n",
    "                         # to_file='./BaselineModel_Architecture.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391168a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optimizer = SGD(learning_rate=0.1, weight_decay=0.01, name=\"Optimizer\")                                         # SGD with decay for stability\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), \n",
    "           Precision(name=\"precision\"),\n",
    "           Recall(name=\"recall\"), \n",
    "           F1Score(average=\"macro\", name=\"f1_score\"),\n",
    "           AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed55ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for saving the model and logs\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Original\"                                             # Model name \n",
    "print(f\"\\n\\033[1mModel name:\\033[0m {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "# Create a directory for saving the model and logs\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Original\"                                              # Model name \n",
    "callbacks = [\n",
    "    ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0),       # Save best model\n",
    "    CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"),                                                                      # Log training metrics\n",
    "    LearningRateScheduler(lambda epoch, lr: lr * 0.95),                                                                           # Exponential decay for learning rate\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1)                                                                      # Stop training when the validation loss stops improving\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f48a4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d0768",
   "metadata": {},
   "source": [
    "### **Original Dataset | Grayscale=F | Contrast=F | Saturation=F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204995d589f8431f",
   "metadata": {},
   "source": [
    "##### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\",\n",
    "    variation=\"Original | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001\",           # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"      # Save the results to a CSV file\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414b175",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba7ca3",
   "metadata": {},
   "source": [
    "## **Original Dataset | Grayscale=T | Contrast=F | Saturation=F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Original Dataset | Grayscale=T | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=True, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optimizer = SGD(learning_rate=0.1, weight_decay=0.01, name=\"Optimizer\")                                         # SGD with decay for stability\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_OriginalGrayscale\" # Model name \n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad9d878",
   "metadata": {},
   "source": [
    "##### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Original | Grayscale=T | Contrast=F | Saturation=F | Adam=0.001\",   # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5fd275",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5f6df",
   "metadata": {},
   "source": [
    "## **Original Dataset | Grayscale=F | Contrast=T | Saturation=F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Original Dataset | Grayscale=F | Contrast=T | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=True,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optimizer = SGD(learning_rate=0.1, weight_decay=0.01, name=\"Optimizer\")                                         # SGD with decay for stability\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e70195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_OriginalContrast\" # Model name\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f44b1",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Original | Grayscale=F | Contrast=T | Saturation=F | Adam=0.001\",   # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea3ba86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd48fe8",
   "metadata": {},
   "source": [
    "## **Original Dataset | Grayscale=F | Contrast=F | Saturation=T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Original Dataset | Grayscale=F | Contrast=F | Saturation=T\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=True\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optimizer = SGD(learning_rate=0.1, weight_decay=0.01, name=\"Optimizer\")                                         # SGD with decay for stability\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_OriginalSaturation\" # Model name\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb6878",
   "metadata": {},
   "source": [
    "### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Original | Grayscale=F | Contrast=F | Saturation=T | Adam=0.001\",   # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eab287",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa058b52",
   "metadata": {},
   "source": [
    "# **🖌️ SMOTE (Dataset Augmentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceaca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data with Data Augmentation (SMOTE)\n",
    "from utilities import load_images_from_directory\n",
    "\n",
    "train_DataAugmentationSMOTE_dir = Path(\"data/RareSpecies_Split/train_DataAugmentationSMOTE\")\n",
    "val_dir = Path(\"data/RareSpecies_Split/val\")\n",
    "test_dir = Path(\"data/RareSpecies_Split/test\")\n",
    "\n",
    "# train_DataAugmentationSMOTE_dir = Path(\"/content/RareSpecies_Split/train_DataAugmentationSMOTE\")\n",
    "# val_dir = Path(\"/content/RareSpecies_Split/val\")\n",
    "# test_dir = Path(\"/content/RareSpecies_Split/test\")\n",
    "\n",
    "class_names = sorted(os.listdir(train_DataAugmentationSMOTE_dir))\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Import the image dataset from the directory\n",
    "train_DataAugmentationSMOTE_datagen, val_datagen, test_datagen = load_images_from_directory(train_DataAugmentationSMOTE_dir, val_dir, test_dir,\n",
    "                                                                      labels='inferred', label_mode='categorical',\n",
    "                                                                      class_names=class_names, color_mode='rgb',\n",
    "                                                                      batch_size=batch_size, image_size=image_size, seed=2025, \n",
    "                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)\n",
    "\n",
    "# Check the shape of the data (batch_size, img_width, img_height, 3)\n",
    "for x, y in train_DataAugmentationSMOTE_datagen.take(1):\n",
    "    print(\"Train batch shape:\", x.shape, y.shape)\n",
    "for x, y in val_datagen.take(1):\n",
    "    print(\"Val batch shape:\", x.shape, y.shape)\n",
    "for x, y in test_datagen.take(1):\n",
    "    print(\"Test batch shape:\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "train_class_counts = {class_name: len(list((train_DataAugmentationSMOTE_dir / class_name).glob('*')))\n",
    "                      for class_name in list(class_names)}\n",
    "train_class_distribution = pd.DataFrame({\n",
    "    'n': train_class_counts.values(),\n",
    "    '%': [count / sum(train_class_counts.values()) * 100 for count in train_class_counts.values()],\n",
    "})\n",
    "train_class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e99728",
   "metadata": {},
   "source": [
    "## **SMOTE Data Augmentation | Grayscale=F | Contrast=F | Saturation=F**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fe8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - SMOTE Data Augmentation | Grayscale=F | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_SMOTE\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_DataAugmentationSMOTE_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a1230",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53615bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"SMOTE | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time, \n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704891c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855128e0",
   "metadata": {},
   "source": [
    "## **SMOTE Data Augmentation | Grayscale=T | Contrast=F | Saturation=F**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb87bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - SMOTE Data Augmentation | Grayscale=T | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=True, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d648f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_SMOTEGrayscale\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_DataAugmentationSMOTE_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839340f7",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5dc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"SMOTE | Grayscale=T | Contrast=F | Saturation=F | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45f1c1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548200e1",
   "metadata": {},
   "source": [
    "## **SMOTE Data Augmentation | Grayscale=F | Contrast=T | Saturation=F**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b70988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - SMOTE Data Augmentation | Grayscale=F | Contrast=T | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=True,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a961ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_SMOTEContrast\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_DataAugmentationSMOTE_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69750aaf",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af33e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"SMOTE | Grayscale=F | Contrast=T | Saturation=F | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d1e50",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd6e88",
   "metadata": {},
   "source": [
    "## **SMOTE Data Augmentation | Grayscale=F | Contrast=F | Saturation=T**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27586cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - SMOTE Data Augmentation | Grayscale=F | Contrast=F | Saturation=T\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=True\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125280a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_SMOTESaturation\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_DataAugmentationSMOTE_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178a011",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a54c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"SMOTE | Grayscale=F | Contrast=F | Saturation=T | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f0433",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f8581f",
   "metadata": {},
   "source": [
    "# **🏋️ Weights**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Samples in Each Class in the Original Training Set\n",
    "original_class_counts = {}\n",
    "total_samples = 0\n",
    "for class_name in class_names:\n",
    "    class_path = train_dir / class_name\n",
    "    if class_path.is_dir():\n",
    "        count = len(list(class_path.glob('*'))) # Count image files\n",
    "        original_class_counts[class_name] = count\n",
    "        total_samples += count\n",
    "    else:\n",
    "        print(f\"Warning: Expected directory not found: {class_path}\")\n",
    "\n",
    "# Calculate Class Weights\n",
    "# Formula: weight_for_class_i = (total_samples / (num_classes * samples_in_class_i))  -> This gives higher weight to smaller classes.\n",
    "class_weights = {}\n",
    "for class_name, count in original_class_counts.items():\n",
    "    if count == 0:\n",
    "        print(f\"Warning: Class '{class_name}' has 0 samples. Assigning weight 0.\")\n",
    "        weight = 0.0\n",
    "    else:\n",
    "        weight = (total_samples) / (len(class_names) * count)\n",
    "\n",
    "    # Get the index for this class name\n",
    "    class_index = class_indices[class_name]\n",
    "    class_weights[class_index] = weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc530feb",
   "metadata": {},
   "source": [
    "## **Weights | Grayscale=F | Contrast=F | Saturation=F**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a4119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Weights | Grayscale=F | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff47fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e12cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Weights\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1, \n",
    "                    class_weight=class_weights) # Pass class weights to the fit method\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcaf33",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Weights | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be14de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18957c21",
   "metadata": {},
   "source": [
    "## **Weights | Grayscale=T | Contrast=F | Saturation=F**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Weights | Grayscale=T | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=True, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f63302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_WeightsGrayscale\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1, class_weight=class_weights)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4cc12",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Weights | Grayscale=T | Contrast=F | Saturation=F | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086b392",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5575d0",
   "metadata": {},
   "source": [
    "## **Weights | Grayscale=F | Contrast=T | Saturation=F**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Weights | Grayscale=F | Contrast=T | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=True,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cae109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_WeightsContrast\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1, \n",
    "                    class_weight=class_weights)    # Pass class weights to the fit method\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271cfe4",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff70b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Weights | Grayscale=F | Contrast=T | Saturation=F | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1512275",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1af494",
   "metadata": {},
   "source": [
    "## **Weights | Grayscale=F | Contrast=F | Saturation=T**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bce934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model - Weights | Grayscale=F | Contrast=F | Saturation=T\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=True\n",
    ")\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ad3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e15308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_WeightsSaturation\"\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel_10epochs/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size=batch_size, epochs=10, validation_data=val_datagen, callbacks=callbacks, verbose=1, \n",
    "                    class_weight=class_weights)     # Pass class weights to the fit method\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851e605",
   "metadata": {},
   "source": [
    "#### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba643099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "\n",
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model\", variation=\"Weights | Grayscale=F | Contrast=F | Saturation=T | Adam=0.001\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel_10epochs/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77537d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea3f13",
   "metadata": {},
   "source": [
    "# **🥇 Best Combinations Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008b7de",
   "metadata": {},
   "source": [
    "#### **Original | Grayscale=F | Contrast=F | Saturation=F**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d9df1",
   "metadata": {},
   "source": [
    "##### **100 Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aecf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model with máx 100 epochs and EarlyStopping\n",
    "# Model - Original Dataset | Grayscale=F | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aad8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Original_maxEpochs100\" # Model name\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f9246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=100, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c7040",
   "metadata": {},
   "source": [
    "###### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c25f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6278c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faa949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model | 100 Epochs\", variation=\"Original | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001\",   # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79097109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import plot_confusion_matrix\n",
    "\n",
    "# Extract y_true from the dataset\n",
    "y_true = []\n",
    "for batch in test_datagen:\n",
    "    # If label_mode is 'categorical', labels are one-hot encoded; convert to integers:\n",
    "    images, labels = batch\n",
    "    if labels.ndim > 1:\n",
    "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    else:\n",
    "        y_true.extend(labels.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Get predictions from the model\n",
    "y_pred = model.predict(test_datagen, batch_size=batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "plot_confusion_matrix(\n",
    "    y_true=y_true, y_pred=y_pred,\n",
    "    title=\"Confusion Matrix | Baseline Model | 100 Epochs\",\n",
    "    file_path=\"./ModelsEvaluation/3_BaselineModel/3_TestConfusionMatrix_BaselineModel_100epochs.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n right and n wrong predictions\n",
    "from utilities import plot_predictions\n",
    "plot_predictions(\n",
    "    model=model,\n",
    "    class_names=class_names,\n",
    "    train_dir=train_dir,\n",
    "    test_data=test_datagen,\n",
    "    num_images=10,\n",
    "    file_path=\"./ModelsEvaluation/3_BaselineModel/3_TestPredictions_BestBaselineModel_100epochs.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba956f1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e3ab",
   "metadata": {},
   "source": [
    "##### **200 Epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model with máx 200 epochs and EarlyStopping\n",
    "# Model - Original Dataset | Grayscale=F | Contrast=F | Saturation=F\n",
    "model = RareSpeciesCNN(\n",
    "    n_classes=n_classes, \n",
    "    apply_grayscale=False, \n",
    "    apply_contrast=False,                         \n",
    "    apply_saturation=False\n",
    ")\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_{datetime.datetime.now().strftime('%Y%m%d')}_Original_maxEpochs200\" # Model name\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/3_BaselineModel/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), CSVLogger(f\"./ModelCallbacks/3_BaselineModel/metrics_{model_name}.csv\"), LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f23212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=200, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33d3af",
   "metadata": {},
   "source": [
    "###### **🧪 Model Selection & 📏 Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af01ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/3_BaselineModel/3_BaselineModel_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67588743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef5e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"Baseline Model | 200 Epochs\", variation=\"Original | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001\",   # Dataset | Grayscale | Contrast | Saturation | Optimizer=Learning Rate\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/3_BaselineModel/3_BaselineModel_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import plot_confusion_matrix\n",
    "\n",
    "# Extract y_true from the dataset\n",
    "y_true = []\n",
    "for batch in test_datagen:\n",
    "    # If label_mode is 'categorical', labels are one-hot encoded; convert to integers:\n",
    "    images, labels = batch\n",
    "    if labels.ndim > 1:\n",
    "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    else:\n",
    "        y_true.extend(labels.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Get predictions from the model\n",
    "y_pred = model.predict(test_datagen, batch_size=batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "plot_confusion_matrix(\n",
    "    y_true=y_true, y_pred=y_pred,\n",
    "    title=\"Confusion Matrix | Baseline Model | 200 Epochs\",\n",
    "    file_path=\"./ModelsEvaluation/3_BaselineModel/3_TestConfusionMatrix_BaselineModel_200epochs.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n right and n wrong predictions\n",
    "from utilities import plot_predictions\n",
    "plot_predictions(\n",
    "    model=model,\n",
    "    class_names=class_names,\n",
    "    train_dir=train_dir,\n",
    "    test_data=test_datagen,\n",
    "    num_images=10,\n",
    "    file_path=\"./ModelsEvaluation/3_BaselineModel/3_TestPredictions_BestBaselineModel_200epochs.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3e982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fccf55e",
   "metadata": {},
   "source": [
    "#### **Original | Grayscale=F | Contrast=F | Saturation=T**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83e0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86877bae",
   "metadata": {},
   "source": [
    "#### **SMOTE | Grayscale=F | Contrast=F | Saturation=F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d84000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import SMOTE training data\n",
    "# train_DataAugmentationSMOTE_dir = Path(\"data/RareSpecies_Split/train_DataAugmentationSMOTE\")\n",
    "# val_dir = Path(\"data/RareSpecies_Split/val\")\n",
    "# test_dir = Path(\"data/RareSpecies_Split/test\")\n",
    "\n",
    "# # train_DataAugmentationSMOTE_dir = Path(\"/content/RareSpecies_Split/train_DataAugmentationSMOTE\")\n",
    "# # val_dir = Path(\"/content/RareSpecies_Split/val\")\n",
    "# # test_dir = Path(\"/content/RareSpecies_Split/test\")\n",
    "\n",
    "# # Import the image dataset from the directory\n",
    "# train_DataAugmentationSMOTE_datagen, val_datagen, test_datagen = load_images_from_directory(train_DataAugmentationSMOTE_dir, val_dir, test_dir,\n",
    "#                                                                       labels='inferred', label_mode='categorical',\n",
    "#                                                                       class_names=class_names, color_mode='rgb',\n",
    "#                                                                       batch_size=batch_size, image_size=image_size, seed=2025, \n",
    "#                                                                       interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)\n",
    "# # Check the shape of the data (batch_size, img_width, img_height, 3)\n",
    "# for x, y in train_DataAugmentationSMOTE_datagen.take(1):\n",
    "#     print(\"Train batch shape:\", x.shape, y.shape)\n",
    "# for x, y in val_datagen.take(1):\n",
    "#     print(\"Val batch shape:\", x.shape, y.shape)\n",
    "# for x, y in test_datagen.take(1):\n",
    "#     print(\"Test batch shape:\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a0b24",
   "metadata": {},
   "source": [
    "#### **SMOTE | Grayscale=F | Contrast=T | Saturation=F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f147c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1515a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display results\n",
    "# from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "# results_df = create_evaluation_dataframe(\n",
    "#     model_name=\"Best Model\", variation=\"SMOTE | Grayscale=F | Contrast=F | Saturation=F | Adam=0.001\",\n",
    "#     train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=None,\n",
    "# )\n",
    "# display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the metrics\n",
    "# from utilities import plot_metrics\n",
    "\n",
    "# # Load the history from the CSV file\n",
    "# history_csv_path = f\"./ModelCallbacks/3_BaselineModel_10epochs/metrics_RareSpeciesCNN_20250413_SMOTE.csv\"\n",
    "# history = pd.read_csv(history_csv_path)\n",
    "# history.set_index('epoch', inplace=True)\n",
    "# plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utilities import plot_confusion_matrix\n",
    "\n",
    "# # Extract y_true from the dataset\n",
    "# y_true = []\n",
    "# for batch in test_datagen:\n",
    "#     # If label_mode is 'categorical', labels are one-hot encoded; convert to integers:\n",
    "#     images, labels = batch\n",
    "#     if labels.ndim > 1:\n",
    "#         y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "#     else:\n",
    "#         y_true.extend(labels.numpy())\n",
    "# y_true = np.array(y_true)\n",
    "\n",
    "# # Get predictions from the model\n",
    "# y_pred = model.predict(test_datagen, batch_size=batch_size)\n",
    "# y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # Plot confusion matrix for test set\n",
    "# plot_confusion_matrix(\n",
    "#     y_true=y_true, y_pred=y_pred,\n",
    "#     title=\"Confusion Matrix | Best Baseline Model\",\n",
    "#     file_path=\"./ModelsEvaluation/3_BaselineModel_10epochs/3_TestConfusionMatrix_BestBaselineModel.png\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot 5 right and 5 wrong predictions\n",
    "# from utilities import plot_predictions\n",
    "# plot_predictions(\n",
    "#     model=model,\n",
    "#     class_names=class_names,\n",
    "#     train_dir=train_dir,\n",
    "#     test_data=test_datagen,\n",
    "#     num_images=5,\n",
    "#     file_path=\"./ModelsEvaluation/3_BaselineModel/3_TestPredictions_BestBaselineModel.png\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b849b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc8664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84b90a90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba221ed",
   "metadata": {},
   "source": [
    "#### **🟨 Google Collab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Google Collab Workspace\n",
    "# Source: https://stackoverflow.com/questions/48774285/how-to-download-file-created-in-colaboratory-workspace\n",
    "# !zip -r /content/ModelCallbacks.zip /content/ModelCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r /content/ModelsEvaluation.zip /content/ModelsEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download(\"/content/ModelCallbacks.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files.download(\"/content/ModelsEvaluation.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff3bdf66c1f233",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf218",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
