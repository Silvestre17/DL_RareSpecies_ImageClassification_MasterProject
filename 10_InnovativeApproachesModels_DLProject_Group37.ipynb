{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1834cc3d7b582eb2",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: center; flex-wrap: wrap;\">\n",
    "    <div style=\"flex: 1; max-width: 400px; display: flex; justify-content: center;\">\n",
    "        <img src=\"https://i.ibb.co/JBPWVYR/Logo-Nova-IMS-Black.png\" style=\"max-width: 50%; height: auto; margin-top: 50px; margin-bottom: 50px;margin-left: 6rem;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 2; text-align: center; margin-top: 20px;margin-left: 8rem;\">\n",
    "        <div style=\"font-size: 28px; font-weight: bold; line-height: 1.2;\">\n",
    "            <span style=\"color: #22c1c3;\">DL Project |</span> <span style=\"color: #08529C;\">Predicting Rare Species from Images using Deep Learning</span>\n",
    "        </div>\n",
    "        <div style=\"font-size: 17px; font-weight: bold; margin-top: 10px;\">\n",
    "            Spring Semester | 2024 - 2025\n",
    "        </div>\n",
    "        <div style=\"font-size: 17px; font-weight: bold;\">\n",
    "            Master in Data Science and Advanced Analytics\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px;\">\n",
    "            <div>Andr√© Silvestre, 20240502</div>\n",
    "            <div>Diogo Duarte, 20240525</div>\n",
    "            <div>Filipa Pereira, 20240509</div>\n",
    "            <div>Maria Cruz, 20230760</div>\n",
    "            <div>Umeima Mahomed, 20240543</div>\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px; font-weight: bold;\">\n",
    "            Group 37\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c9197",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 1px; color: white; border-radius: 500px; text-align: center;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea0a04",
   "metadata": {},
   "source": [
    "## **üìö Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623882dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 11:20:26.451683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745490026.661430   16088 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745490026.704613   16088 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745490027.136181   16088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745490027.136240   16088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745490027.136243   16088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745490027.136244   16088 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 11:20:27.176335: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Self, Any      # For Python 3.10\n",
    "# from typing import Self, Any               # For Python >3.11\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from keras.ops import add\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling, Lambda, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers                                                                           # For L2 regularization\n",
    "# import visualkeras\n",
    "\n",
    "# Evaluation imports\n",
    "from keras.metrics import CategoricalAccuracy, AUC, F1Score, Precision, Recall\n",
    "\n",
    "# Other imports\n",
    "from itertools import product\n",
    "\n",
    "# Set the style of the visualization\n",
    "pd.set_option('future.no_silent_downcasting', True)   # use int instead of float in DataFrame\n",
    "pd.set_option(\"display.max_columns\", None)            # display all columns\n",
    "\n",
    "# Disable warnings (FutureWarning)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a56e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a SSL context that does not verify the server‚Äôs certificate - Needed for downloading pretrained models\n",
    "# Source: https://precli.readthedocs.io/0.3.4/rules/python/stdlib/ssl_create_unverified_context.html\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a7b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "Is TensorFlow built with CUDA? True\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU Device Name: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745490032.284722   16088 gpu_device.cc:2019] Created device /device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Is TensorFlow built with CUDA?\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"GPU Device Name:\", tf.test.gpu_device_name())                                # (if error in Google Colab: Make sure your Hardware accelerator is set to GPU. \n",
    "                                                                                    # Runtime > Change runtime type > Hardware Accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e59dfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Python version: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]\n",
      "CUDA version: 12.5.1\n",
      "cuDNN version: 9\n"
     ]
    }
   ],
   "source": [
    "# Get build information from TensorFlow\n",
    "build_info = tf.sysconfig.get_build_info()\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"CUDA version:\", build_info.get(\"cuda_version\", \"Not available\"))\n",
    "print(\"cuDNN version:\", build_info.get(\"cudnn_version\", \"Not available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2b7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom module for importing data, visualization, and utilities\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d2b1a",
   "metadata": {},
   "source": [
    "## **üßÆ Import Databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a05d66",
   "metadata": {},
   "source": [
    "#### **üü® Google Collab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run in Google Collab to download the dataset already splitted\n",
    "# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez\n",
    "# # Download the file from Google Drive using wget\n",
    "# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download\" -O- | \\\n",
    "#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p' > /tmp/confirm.txt\n",
    "\n",
    "# # Read the confirmation token from the temporary file\n",
    "# with open('/tmp/confirm.txt', 'r') as f:\n",
    "#     confirm_token = f.read().strip()\n",
    "\n",
    "# # Download the file using the confirmation token and cookies\n",
    "# !wget --load-cookies /tmp/cookies.txt \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=1dmr2cGxgM-kp1aXlmd9cQzVCkcl4JTFo&export=download&confirm={confirm_token}\" \\\n",
    "#   -O /content/RareSpecies_Split.zip\n",
    "\n",
    "# # Clean up temporary files\n",
    "# !rm /tmp/cookies.txt /tmp/confirm.txt\n",
    "\n",
    "# # List files in the /content directory to verify the download\n",
    "# !ls -lh /content/\n",
    "\n",
    "# # Unzip the downloaded file\n",
    "# !unzip /content/RareSpecies_Split.zip -d /content/\n",
    "\n",
    "# # List the unzipped files to verify\n",
    "# !ls -lh /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run in Google Collab to download the dataset already splitted\n",
    "# # Source: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drivez\n",
    "# # Download the file from Google Drive using wget\n",
    "# !wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=1Gfzm6mdhQ75cAzLQAK_gf-NFKk1KDRXo&export=download\" -O- | \\\n",
    "#   sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p' > /tmp/confirm.txt\n",
    "\n",
    "# # Read the confirmation token from the temporary file\n",
    "# with open('/tmp/confirm.txt', 'r') as f:\n",
    "#     confirm_token = f.read().strip()\n",
    "\n",
    "# # Download the file using the confirmation token and cookies\n",
    "# !wget --load-cookies /tmp/cookies.txt \\\n",
    "#   \"https://drive.usercontent.google.com/download?id=1Gfzm6mdhQ75cAzLQAK_gf-NFKk1KDRXo&export=download&confirm={confirm_token}\" \\\n",
    "#   -O /content/InnovativeApproach.zip\n",
    "\n",
    "# # Clean up temporary files\n",
    "# !rm /tmp/cookies.txt /tmp/confirm.txt\n",
    "\n",
    "# # List files in the /content directory to verify the download\n",
    "# !ls -lh /content/\n",
    "\n",
    "# # Unzip the downloaded file\n",
    "# !unzip /content/InnovativeApproach.zip -d /content/\n",
    "\n",
    "# # List the unzipped files to verify\n",
    "# !ls -lh /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6d85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the data\n",
    "train_SMOTE_OnlyAnimal_dir = Path(\"data/InnovativeApproach/OnlyAnimal_train_DataAugmentationSMOTE\")\n",
    "val_OnlyAnimal_dir = Path(\"data/InnovativeApproach/OnlyAnimal_val\")\n",
    "test_OnlyAnimal_dir = Path(\"data/InnovativeApproach/OnlyAnimal_test\")\n",
    "\n",
    "# For Google Collab\n",
    "# train_SMOTE_OnlyAnimal_dir = Path(\"/content/InnovativeApproach/OnlyAnimal_train_DataAugmentationSMOTE\")\n",
    "# val_OnlyAnimal_dir = Path(\"/content/InnovativeApproach/OnlyAnimal_val\")\n",
    "# test_OnlyAnimal_dir = Path(\"/content/InnovativeApproach/OnlyAnimal_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d19030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Generators \n",
    "n_classes = 202                                     # Number of classes (we already know this based on previous notebook)\n",
    "image_size = (224, 224)                             # Image size (224x224)\n",
    "img_height, img_width = image_size                  # Image dimensions\n",
    "batch_size = 64                                     # Batch size\n",
    "input_shape = (img_height, img_width, 3)            # Input shape of the model\n",
    "value_range = (0.0, 1.0)                            # Range of pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "983a5550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47874 files belonging to 202 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745490106.108634   16088 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1020 files belonging to 202 classes.\n",
      "Found 1025 files belonging to 202 classes.\n",
      "\n",
      "Loaded: Train (47936), Val (1024), Test (1088)\n"
     ]
    }
   ],
   "source": [
    "# Get class names from directory\n",
    "class_names = sorted(os.listdir(train_SMOTE_OnlyAnimal_dir))\n",
    "class_indices = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "# Import the image dataset from the directory\n",
    "from utilities import load_images_from_directory\n",
    "train_datagen, val_datagen, test_datagen = load_images_from_directory(train_SMOTE_OnlyAnimal_dir, val_OnlyAnimal_dir, test_OnlyAnimal_dir,\n",
    "                                                                      labels='inferred', label_mode='categorical',\n",
    "                                                                      class_names=class_names, color_mode='rgb',\n",
    "                                                                      batch_size=batch_size, image_size=image_size, seed=2025, \n",
    "                                                                      interpolation='bilinear', crop_to_aspect_ratio=False, pad_to_aspect_ratio=False)\n",
    "\n",
    "print(f\"\\nLoaded: Train ({train_datagen.cardinality().numpy() * batch_size}), \"\n",
    "        f\"Val ({val_datagen.cardinality().numpy() * batch_size}), \"\n",
    "        f\"Test ({test_datagen.cardinality().numpy() * batch_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3691ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (SMOTE with OnlyAnimal) batch shape: (64, 224, 224, 3) (64, 202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 11:21:55.741781: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val batch shape: (64, 224, 224, 3) (64, 202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 11:21:59.902255: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch shape: (64, 224, 224, 3) (64, 202)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the data (batch_size, img_width, img_height, 3)\n",
    "for x, y in train_datagen.take(1):\n",
    "    print(\"Train (SMOTE with OnlyAnimal) batch shape:\", x.shape, y.shape)\n",
    "for x, y in val_datagen.take(1):\n",
    "    print(\"Val (OnlyAnimal) batch shape:\", x.shape, y.shape)\n",
    "for x, y in test_datagen.take(1):\n",
    "    print(\"Test (OnlyAnimal) batch shape:\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f158b792ebd291",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='3'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #22c1c3, #27b1dd, #2d9cfd, #090979); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 140px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Avenir Next LT Pro', sans-serif;\">\n",
    "        <b>10 | Innovative Approaches - Models with only Animals</b></h1></center>\n",
    "</div>\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b47fa0",
   "metadata": {},
   "source": [
    "## **üê¢üåø Import Classification Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6014f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame with classification results\n",
    "df_train_results = pd.read_csv(\"./AnimalClassification/TrainAnimalClassification_Joined.csv\")\n",
    "df_val_results = pd.read_csv(\"./AnimalClassification/ValAnimalClassification_Joined.csv\")\n",
    "df_test_results = pd.read_csv(\"./AnimalClassification/TestAnimalClassification_Joined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a435db",
   "metadata": {},
   "source": [
    "# **ü•á Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving model checkpoints and evaluation logs\n",
    "os.makedirs(\"./ModelCallbacks/10_ConvNeXtBaseFinalModels\", exist_ok=True)      # exist_ok=True | Create directory if it doesn't exist\n",
    "os.makedirs(\"./ModelsEvaluation/10_ConvNeXtBaseFinalModels\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a4b97",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "> **FALTA POR OS PAR√ÇMETROS DO MELHOR MODELO + Mudar nome dos modelos na Evaluation**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ConvNeXtBase\n",
    "from tensorflow.keras.applications.convnext import preprocess_input\n",
    "\n",
    "class RareSpeciesCNN_ConvNeXtBase_FinalModel(Model):\n",
    "    \"\"\"Custom CNN for rare species classification using ConvNeXtBase.\n",
    "    \n",
    "    Architecture: ConvNeXtBase \n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=202):\n",
    "        super().__init__()                          # Call the parent class constructor\n",
    "\n",
    "        # ----------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # Preprocess image for ConvNeXtBase\n",
    "        # ResNet152V2 requires images to be preprocessed in a specific way (Source: https://keras.io/api/applications/convnext/#convnextbase-function)\n",
    "        self.convnextbase_preprocess = Lambda(lambda x: preprocess_input(x), name='ConvNeXtBase_Preprocess')\n",
    "        \n",
    "        # Load ConvNeXtBase Pretrained Model\n",
    "        self.convnextbase = ConvNeXtBase(include_top=False,         # Do not include the top classification layer (we will add our own because we have a different number of classes)\n",
    "                                         classes=n_classes,         # Number of classes\n",
    "                                         weights='imagenet')        # Use ImageNet weights\n",
    "        self.convnextbase.trainable = False                         # Freeze the convolutional layers in the base model (transfer learning)\n",
    "        # -----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # --- Classification Head ---\n",
    "        self.global_avg_pool = GlobalAveragePooling2D(name=\"Global_Average_Pooling\")      # Global Average Pooling layer\n",
    "        self.dense1 = Dense(128, name=\"Dense_Layer1\")                                     # Smaller intermediate dense layer\n",
    "        self.dropout = Dropout(0.5, name=\"Dropout_Layer\")                                 # Dropout layer for regularization\n",
    "        self.dense_output = Dense(n_classes, activation='softmax', name=\"Output_Layer\")   # Output layer with softmax activation (for multi-class classification)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs                                     # Input layer\n",
    "\n",
    "        # Preprocess image for ConvNeXtBase\n",
    "        x = self.convnextbase_preprocess(x)              # Preprocess the image for ConvNeXtBase\n",
    "        \n",
    "        # Pass through ConvNeXtBase model\n",
    "        x = self.convnextbase(x, training=training)      # Pass through the model (ConvNeXtBase)\n",
    "\n",
    "        # Classification Head\n",
    "        x = self.global_avg_pool(x)                      # Global Average Pooling\n",
    "        x = self.dense1(x)                               # Dense layer\n",
    "        x = self.dropout(x, training=training)           # Dropout layer\n",
    "        outputs = self.dense_output(x)                   # Output layer\n",
    "        return outputs\n",
    "\n",
    "# Example Instantiation and Summary\n",
    "model = RareSpeciesCNN_ConvNeXtBase_FinalModel(n_classes=n_classes)\n",
    "\n",
    "# Build the model by providing an input shape\n",
    "inputs = Input(shape=(img_width, img_height, 3))        # Input shape\n",
    "_ = model.call(inputs)                                  # Call the model to build it\n",
    "model.summary()                                         # Print the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4fb35",
   "metadata": {},
   "source": [
    "### üê¢üåø **Train/Validation/Test `Only Animals`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e80503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "optimizer = Adam(learning_rate=0.001, weight_decay=0.01, name=\"Optimizer\")                                        # Adam for faster convergence\n",
    "loss = CategoricalCrossentropy(name=\"Loss\")                                                                       # Suitable for multi-class one-hot labels\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), Precision(name=\"precision\"), Recall(name=\"recall\"), F1Score(average=\"macro\", name=\"f1_score\"), AUC(name=\"auc\")]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_name = f\"RareSpeciesCNN_ConvNeXtBase_FinalModels_{datetime.datetime.now().strftime('%Y%m%d')}\" # Model name\n",
    "callbacks = [ModelCheckpoint(f\"./ModelCallbacks/10_ConvNeXtBaseFinalModels/checkpoint_{model_name}.keras\", monitor=\"val_loss\", save_best_only=True, verbose=0), \n",
    "             CSVLogger(f\"./ModelCallbacks/10_ConvNeXtBaseFinalModels/metrics_{model_name}.csv\"), \n",
    "             LearningRateScheduler(lambda epoch, lr: lr * 0.95), EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)] # Reduce learning rate if no improvement in validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(train_datagen, batch_size = batch_size, epochs=100, validation_data=val_datagen, callbacks=callbacks, verbose=1)\n",
    "train_time = round(time.time() - start_time, 2)\n",
    "print(f\"\\nTraining completed in \\033[1m{train_time} seconds ({str(datetime.timedelta(seconds=train_time))} h)\\033[0m).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2793c",
   "metadata": {},
   "source": [
    "#### **üß™ Model Selection & üìè Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from utilities import plot_metrics\n",
    "plot_metrics(history, file_path=f\"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TrainingValidationMetrics_{model_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a9d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation and test sets\n",
    "train_results = {'accuracy': history.history['accuracy'][-1], 'precision': history.history['precision'][-1], 'recall': history.history['recall'][-1], 'f1_score': history.history['f1_score'][-1], 'auc': history.history['auc'][-1]}\n",
    "val_results = model.evaluate(val_datagen, batch_size=batch_size, return_dict=True, verbose=1)\n",
    "test_results = model.evaluate(test_datagen, batch_size=batch_size, return_dict=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ad6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from utilities import display_side_by_side, create_evaluation_dataframe\n",
    "results_df = create_evaluation_dataframe(\n",
    "    model_name=\"ConvNeXtBase - Final Model\", variation=\"SMOTE (OnlyAnimals)\",\n",
    "    train_metrics=train_results, val_metrics=val_results, test_metrics=test_results, train_time=train_time,\n",
    "    csv_save_path= f\"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TrainingValidationMetrics_{model_name}.csv\"\n",
    ")\n",
    "display_side_by_side(results_df, super_title=\"Model Evaluation Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5820b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n right and n wrong predictions\n",
    "from utilities import plot_predictions\n",
    "plot_predictions(\n",
    "    model=model,\n",
    "    class_names=class_names,\n",
    "    train_dir=train_SMOTE_OnlyAnimal_dir,\n",
    "    test_data=test_datagen,\n",
    "    num_images=10,\n",
    "    file_path=f\"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TestPredictions_{model_name}.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3312ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from utilities import plot_confusion_matrix\n",
    "\n",
    "# Extract y_true from the dataset\n",
    "y_true = []\n",
    "for batch in test_datagen:\n",
    "    # If label_mode is 'categorical', labels are one-hot encoded; convert to integers:\n",
    "    images, labels = batch\n",
    "    if labels.ndim > 1:\n",
    "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    else:\n",
    "        y_true.extend(labels.numpy())\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Get predictions from the model\n",
    "y_pred = model.predict(test_datagen, batch_size=batch_size)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "plot_confusion_matrix(\n",
    "    y_true=y_true, y_pred=y_pred,\n",
    "    title=\"Confusion Matrix | Best Model\",\n",
    "    file_path=\"./ModelsEvaluation/10_ConvNeXtBaseFinalModels/10_TestConfusionMatrix.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e9788",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b08241",
   "metadata": {},
   "source": [
    "#### **üü® Google Collab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Google Collab Workspace\n",
    "# # Source: https://stackoverflow.com/questions/48774285/how-to-download-file-created-in-colaboratory-workspace\n",
    "\n",
    "# # Zip the directories\n",
    "# !zip -r /content/ModelCallbacks.zip /content/ModelCallbacks\n",
    "# !zip -r /content/ModelsEvaluation.zip /content/ModelsEvaluation\n",
    "\n",
    "# # Download the zip files\n",
    "# from google.colab import files\n",
    "# files.download(\"/content/ModelCallbacks.zip\")\n",
    "# files.download(\"/content/ModelsEvaluation.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff3bdf66c1f233",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5816b",
   "metadata": {},
   "source": [
    "# **üîó Bibliography/References**\n",
    "\n",
    "**[[1]](https://huggingface.co/openai/clip-vit-base-patch16)** OpenAI. (2021). openai/clip-vit-base-patch16 ¬∑ Hugging Face. Huggingface.co. https://huggingface.co/openai/clip-vit-base-patch16\n",
    "\n",
    "**[[2]](https://arxiv.org/pdf/2201.03545)** Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., Xie, S., Facebook, A., & Research. (2022). A ConvNet for the 2020s. https://arxiv.org/pdf/2201.03545"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf218",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
